[
  {
    "index": 0,
    "source_corpus_id": 252715594,
    "ref_id": "b57",
    "citation_corpus_id": 238582653,
    "start": 11564,
    "end": 11568,
    "title": "VECTOR-QUANTIZED IMAGE MODELING WITH IM- PROVED VQGAN",
    "abstract": "Pretraining language models with next-token prediction on massive text corpora has delivered phenomenal zero-shot, few-shot, transfer learning and multi-tasking capabilities on both generative and discriminative language tasks. Motivated by this success, we explore a Vector-quantized Image Modeling (VIM) approach that involves pretraining a Transformer to predict rasterized image tokens autoregressively. The discrete image tokens are encoded from a learned Vision-Transformerbased VQGAN (ViT-VQGAN). We first propose multiple improvements over vanilla VQGAN from architecture to codebook learning, yielding better efficiency and reconstruction fidelity. The improved ViT-VQGAN further improves vectorquantized image modeling tasks, including unconditional, class-conditioned image generation and unsupervised representation learning. When trained on Im-ageNet at 256 × 256 resolution, we achieve Inception Score (IS) of 175.1 and Fréchet Inception Distance (FID) of 4.17, a dramatic improvement over the vanilla VQGAN, which obtains 70.6 and 17.04 for IS and FID, respectively. Based on ViT-VQGAN and unsupervised pretraining, we further evaluate the pretrained Transformer by averaging intermediate features, similar to Image GPT (iGPT). This ImageNet-pretrained VIM-L significantly beats iGPT-L on linear-probe accuracy from 60.3% to 73.2% for a similar model size. VIM-L also outperforms iGPT-XL which is trained with extra web image data and larger model size.arXiv:2110.04627v3 [cs.CV] 5 Jun 2022Published as a conference paper at ICLR 2022 Figure 1: Overview of ViT-VQGAN (left) and Vector-quantized Image Modeling (right) for both image generation and image understanding.Remarkable image generation results have been achieved by pre-quantizing images into discrete latent variables and modeling them autoregressively, including VQVAE (Oord et al., 2017), DALL-E (Ramesh et al., 2021)  and VQGAN (Esser et al., 2021). In these approaches, a convolution neural network (CNN) is learned to auto-encode an image and a second stage CNN or Transformer is learned to model the density of encoded latent variables. These have been proved effective for image generation, but few studies have evaluated the learned representation in discriminative tasks (Ramesh et al., 2021; Esser et al., 2021).We explore an approach we refer to as Vector-quantized Image Modeling (VIM) and apply it to both image generation and image understanding tasks. VIM follows a two-stage approach:",
    "prev": "Quantization and Losses:\n\nTo learn a discrete latent space, we quantize our encoder outputs into the entries of a learned codebook via the vector quantization (VQ) objective in VQVAEs [45],\nL VQ = sg(z) − e 2 2 + β z − sg(e) 2 2 ,(1)\nwhere sg(x) ≡ x, and d dx sg(x) ≡ 0 is the stop-gradient operator, β is the commitment loss weight, and e is a codebook vector from codebook E. The index to the codebook vector closest to z is found by i = argmin j z − E j 2 2 .",
    "curr": "In addition to the VQ objective, we adopt the factorized and 2normalized codes from ViT-VQGAN [58] to improve codebook usage and reconstruction quality.",
    "next": "To train our model, we use a combination of L 2 loss, image perceptual loss L IP [20,61], video perceptual loss L VP by using the I3D network [6] as feature extractor, and adversarial loss L Adv with StyleGAN architecture [21].",
    "query": "Are there any research papers on vector-quantized image modeling with improved VQGAN techniques, using ViT-VQGAN and Transformer-based autoregressive approaches for image generation and representation learning?"
  },
  {
    "index": 6,
    "source_corpus_id": 223956716,
    "ref_id": "b46",
    "citation_corpus_id": 6212000,
    "start": 1947,
    "end": 1966,
    "title": "UNDERSTANDING DEEP LEARNING REQUIRES RE- THINKING GENERALIZATION",
    "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on the generalization of deep neural networks, particularly those examining the role of model expressivity and the ability of large networks to fit random training data labels?"
  },
  {
    "index": 8,
    "source_corpus_id": 212996548,
    "ref_id": "b9",
    "citation_corpus_id": 52967399,
    "start": 2418,
    "end": 2439,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "INTRODUCTION\n\nTransformer (Vaswani et al., 2017) is widely used in natural language processing due to its high training efficiency and superior capability in capturing long-distance dependencies.",
    "curr": "Building on top of them, modern state-of-the-art models, such as BERT (Devlin et al., 2019), are able to learn powerful language representations from unlabeled text and even surpass the human performance on the challenging question answering task.",
    "next": "However, the good performance comes at a high computational cost.",
    "query": "Are there any research papers on pre-training of bidirectional Transformer models for language understanding and their applications in question answering tasks?"
  },
  {
    "index": 9,
    "source_corpus_id": 202719276,
    "ref_id": "b8",
    "citation_corpus_id": 6706414,
    "start": 2268,
    "end": 2292,
    "title": "Published as a conference paper at ICLR 2015 EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES",
    "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
    "prev": "INTRODUCTION\n\nDeep learning has achieved a remarkable performance breakthrough on various challenging benchmarks in machine learning fields, such as image classification (Krizhevsky et al., 2012) and speech recognition .",
    "curr": "However, recent studies (Szegedy et al., 2014;Goodfellow et al., 2015) have revealed that deep neural network models are strikingly susceptible to adversarial examples, in which small perturbations around the input are sufficient to mislead the predictions of the target model.",
    "next": "Moreover, such perturbations are almost imperceptible to humans and often transfer across diverse models to achieve black-box attacks (Papernot et al., 2017;Liu et al., 2017).",
    "query": "Are there any research papers on the explanation of adversarial examples in deep learning models and methods for adversarial training to improve robustness?"
  },
  {
    "index": 10,
    "source_corpus_id": 220665539,
    "ref_id": "b22",
    "citation_corpus_id": 5834589,
    "start": 3440,
    "end": 3473,
    "title": "ON LARGE-BATCH TRAINING FOR DEEP LEARNING: GENERALIZATION GAP AND SHARP MINIMA",
    "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say 32-512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions-and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap. Published as a conference paper at ICLR 2017 These methods minimize the objective function f by iteratively taking steps of the form: J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. . Weak sharp minima and penalty functions in mathematical programming. PhD thesis, University of Cambridge, 1988.Michael P Friedlander and Mark Schmidt. Hybrid deterministic-stochastic methods for data fitting.",
    "prev": "This question is motivated by the surprising realization over the past decade that deep neural network training can be performed almost entirely with first-order stochastic optimization.",
    "curr": "In fact, empirical evidence supports the hypothesis that the regularizing effect of gradient noise assists model generalization (Keskar et al., 2017;Smith and Le, 2018;Hochreiter and Schmidhuber, 1997).",
    "next": "Stochastic gradient descent variants such as AdaGrad (Duchi et al., 2011) and Adam (Kingma and Ba, 2015) form the core of almost all successful optimization techniques for these models, using small subsets of the data to form the noisy gradient estimates.",
    "query": "Are there any studies on the relationship between gradient noise, sharp minima, and the generalization performance of deep learning models trained with large-batch versus small-batch optimization methods?"
  },
  {
    "index": 12,
    "source_corpus_id": 264802502,
    "ref_id": "b11",
    "citation_corpus_id": 28202810,
    "start": 2358,
    "end": 2380,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": " offline RL algorithms conditioned on observation histories suffer from poor sample complexity, in accordance with the above intuition.We then identify sufficient conditions under which offline RL can still be efficient -intuitively, it needs to learn a compact representation of history comprising only features relevant for action selection.We introduce a bisimulation loss that captures the extent to which this happens, and propose that offline RL can explicitly optimize this loss to aid worst-case sample complexity.Empirically, we show that across a variety of tasks either our proposed loss improves performance, or the value of this loss is already minimized as a consequence of standard offline RL, indicating that it correlates well with good performance.",
    "curr": "INTRODUCTION\n\nDeep reinforcement learning (RL) has achieved impressive performance in games (Mnih et al., 2013;Silver et al., 2017;AlphaStar, 2019), robotic locomotion (Schulman et al., 2015;2017), and control (Todorov et al., 2012;Haarnoja et al., 2018).A key challenge in the widespread adoption of RL algorithms is the need for deploying a suboptimal policy in the environment to collect online interactions, which can be detrimental in many applications such as recommender systems (Afsar et al., 2021), healthcare (Shortreed et al., 2011;Wang et al., 2018), and robotics (Kalashnikov et al., 2018).Offline RL aims to learn effective policies entirely from an offline dataset of previously collected demonstrations (Levine et al., 2020), which makes it a promising approach for applications where exploring online from scratch is unsafe or costly.A major reason for the success of offline RL algorithms is their ability to combine components of suboptimal trajectories in the offline dataset using common states, a phenomenon called \"trajectory stitching\" (Fu et al., 2019a;2020).",
    "next": "Most offline RL methods are formulated in a Markov decision process (MDP) where the state is fully observed (Sutton and Barto, 2018).However, in many real-world tasks,",
    "query": "Are there any research papers on off-policy maximum entropy deep reinforcement learning algorithms with a stochastic actor for continuous control tasks?"
  },
  {
    "index": 15,
    "source_corpus_id": 253523474,
    "ref_id": "b10",
    "citation_corpus_id": 245906072,
    "start": 3522,
    "end": 3546,
    "title": "Implicit Bias of MSE Gradient Optimization in Underparameterized Neural Networks",
    "abstract": "We study the dynamics of a neural network in function space when optimizing the mean squared error via gradient flow. We show that in the underparameterized regime the network learns eigenfunctions of an integral operator TK∞ determined by the Neural Tangent Kernel (NTK) at rates corresponding to their eigenvalues. For example, for uniformly distributed data on the sphere S d−1 and rotation invariant weight distributions, the eigenfunctions of TK∞ are the spherical harmonics. Our results can be understood as describing a spectral bias in the underparameterized regime. The proofs use the concept of \"Damped Deviations\", where deviations of the NTK matter less for eigendirections with large eigenvalues due to the occurence of a damping factor. Aside from the underparameterized regime, the damped deviations point-of-view can be used to track the dynamics of the empirical risk in the overparameterized setting, allowing us to extend certain results in the literature. We conclude that damped deviations offers a simple and unifying perspective of the dynamics when optimizing the squared error.",
    "prev": "In particular, bounding the smallest eigenvalue of the NTK Gram matrix is a staple technique for establishing convergence guarantees for the optimization (Du et al., 2019a,b;Oymak & Soltanolkotabi, 2020).",
    "curr": "Furthermore, the full spectrum of the NTK Gram matrix governs the dynamics of the empirical risk (Arora et al., 2019b), and the eigenvalues of the associated integral operator characterize the dynamics of the generalization error outside the training set (Bowman & Montufar, 2022;Bowman & Montúfar, 2022).",
    "next": "Moreover, the decay rate of the generalization error for Gaussian process regression using the NTK can be characterized by the decay rate of the spectrum (Caponnetto & De Vito, 2007;Cui et al., 2021;Jin et al., 2022).",
    "query": "Are there any research papers on the analysis of Neural Tangent Kernel (NTK) spectra and their influence on empirical risk dynamics, generalization error, and eigenfunction learning in underparameterized neural networks?"
  },
  {
    "index": 16,
    "source_corpus_id": 162184036,
    "ref_id": "b15",
    "citation_corpus_id": 1957433,
    "start": 17859,
    "end": 17884,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on global logbilinear models for word representation that combine matrix factorization and local context window methods?"
  },
  {
    "index": 17,
    "source_corpus_id": 264555396,
    "ref_id": "b17",
    "citation_corpus_id": 28202810,
    "start": 6218,
    "end": 6241,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "to the RL algorithm.Furthermore, it is robust in many challenging environment specifications, including large 3D navigation maps, procedurally generated environments, sparse reward tasks, pixel-based observations and continuous action spaces.Videos of the trained agents and summarized findings can be found on our supplementary webpage1 .",
    "curr": "RELATED WORK\n\nExploration in RL Exploration is a central challenge in RL.Classical exploration strategies explore in an aleatoric fashion.ϵ-greedy (Sutton & Barto, 2018) forces the agent to sample random actions during training for the sake of exploration.Adding random structured noise in the action space (Lillicrap et al., 2015;Fujimoto et al., 2018) can enable exploration in continuous spaces.Adding random noise in the parameter space has also been proposed to improve exploration (Fortunato et al., 2017).Maximum entropy RL provides a framework to find optimal policies that are as diverse as possible, and hence better explore the space of solutions (Haarnoja et al., 2018;Levine et al., 2020).For hard-exploration tasks, structured exploration has been studied through the lens of hierarchical RL (Gehring et al., 2021;Eysenbach et al., 2018).In MDPs with sparse reward distributions, exploration bonuses (i.e.intrinsic rewards) provide proxy objectives to the agents that can induce state-covering behaviors, hence allowing agents to find the sparse rewards.Count-based methods (Auer, 2002) derive an exploration bonus from state visitation frequencies.Importantly, the inverse counts of a given state measure its novelty and hence provide a suitable objective to train exploratory agents.Furthermore, count-based rewards implicitly define an efficient annealing exploratory schedule, since novelty globally decays as the agent visits all the possible states in the MDP.These properties make count-based exploration a very appealing technique to enable efficient exploration.However, they don't scale well to high-dimensional state spaces (Bellemare et al., 2016).Pseudo-",
    "next": null,
    "query": "Are there any research papers on off-policy maximum entropy deep reinforcement learning algorithms with stochastic actors for continuous control tasks?"
  },
  {
    "index": 18,
    "source_corpus_id": 251341969,
    "ref_id": "b15",
    "citation_corpus_id": 208857488,
    "start": 7437,
    "end": 7455,
    "title": "Observational Overfitting in Reinforcement Learning",
    "abstract": "A major component of overfitting in model-free reinforcement learning (RL) involves the case where the agent may mistakenly correlate reward with certain spurious features from the observations generated by the Markov Decision Process (MDP). We provide a general framework for analyzing this scenario, which we use to design multiple synthetic benchmarks from only modifying the observation space of an MDP. When an agent overfits to different observation spaces even if the underlying MDP dynamics is fixed, we term this observational overfitting. Our experiments expose intriguing properties especially with regards to implicit regularization, and also corroborate results from previous works in RL generalization and supervised learning (SL). * Work partially performed as an OpenAI Fellow. † Work performed during the Google AI Residency Program. http://g.co/airesidency 1",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any studies that investigate observational overfitting in model-free reinforcement learning, particularly focusing on spurious feature correlations in observations and synthetic benchmark design for analyzing generalization?"
  },
  {
    "index": 23,
    "source_corpus_id": 231632937,
    "ref_id": "b37",
    "citation_corpus_id": 52911937,
    "start": 2092,
    "end": 2114,
    "title": "DIRECTED-INFO GAIL: LEARNING HIERARCHICAL POLICIES FROM UNSEGMENTED DEMONSTRATIONS USING DIRECTED INFORMATION",
    "abstract": "The use of imitation learning to learn a single policy for a complex task that has multiple modes or hierarchical structure can be challenging. In fact, previous work has shown that when the modes are known, learning separate policies for each mode or sub-task can greatly improve the performance of imitation learning. In this work, we discover the interaction between sub-tasks from their resulting stateaction trajectory sequences using a directed graphical model. We propose a new algorithm based on the generative adversarial imitation learning framework which automatically learns sub-task policies from unsegmented demonstrations. Our approach maximizes the directed information flow in the graphical model between sub-task latent variables and their generated trajectories. We also show how our approach connects with the existing Options framework, which is commonly used to learn hierarchical policies. * Denotes equal contribution",
    "prev": "In this paper we ask, how do we learn to decompose the task automatically and utilize the decomposition to solve sparse reward problems?",
    "curr": "Deep RL has made great strides solving a variety of tasks recently, with hierarchical RL (hRL) demonstrating promise in solving such sparse reward tasks (Sharma et al., 2019b;Le et al., 2018;Merel et al., 2019;Ranchod et al., 2015).",
    "next": "In hRL, the task is decomposed into a hierarchy of subtasks, where policies at the top of the hierarchy call upon policies below to perform actions to solve their respective subtasks.",
    "query": "Are there any studies that explore methods for learning hierarchical policies from unsegmented demonstrations using directed information and imitation learning?"
  },
  {
    "index": 24,
    "source_corpus_id": 246904522,
    "ref_id": "b38",
    "citation_corpus_id": 5034059,
    "start": 1944,
    "end": 1964,
    "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    "abstract": "For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusively tailored to a specific task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating and analyzing the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all our tasks yields better results than training a separate model for each task. However, the low absolute performance of our best model indicates the need for improved general NLU systems. son. 2013. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint 1312.3005.",
    "prev": "* Equal contribution.",
    "curr": "INTRODUCTION\n\nOver the past few years, Transformer (Vaswani et al., 2017) has been widely used in various natural language processing (NLP) tasks, including text classification (Wang et al., 2018a), text translation (Ott et al., 2018), question answering (Rajpurkar et al., 2016; and text generation (Brown et al., 2020).",
    "next": "The recent application of Transformer in computer vision (CV) field also demonstrate the potential capacity of Transformer architecture.",
    "query": "Are there any studies that explore the application of Transformer models in natural language processing and computer vision tasks?"
  },
  {
    "index": 25,
    "source_corpus_id": 252846609,
    "ref_id": "b18",
    "citation_corpus_id": 52920808,
    "start": 9411,
    "end": 9427,
    "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks",
    "abstract": "One of the mystery in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an m hidden node shallow neural network with ReLU activation and n training data, we show as long as m is large enough and the data is non-degenerate, randomly initialized gradient descent converges a globally optimal solution with a linear convergence rate for the quadratic loss function.Our analysis is based on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods. * Equal Contribution.",
    "prev": "We call the NTK of a finite network with θ chosen at some point during training the network's empirical NTK.",
    "curr": "Although the empirical NTK cannot exactly model the full training dynamics of finite networks, (Du et al., 2018(Du et al., , 2019a give some non-asymptotic guarantees.",
    "next": "Bi-level optimization with NTK: Let (X d , y d ) and (X p , y p ) denote the clean and poison training examples, respectively, (X t , y t ) denote clean test examples, and (X a , y a ) denote test data with the trigger applied and the target label.",
    "query": "Are there any research papers on the theoretical analysis of gradient descent in over-parameterized neural networks, focusing on convergence guarantees and the role of over-parameterization and random initialization?"
  },
  {
    "index": 26,
    "source_corpus_id": 257834209,
    "ref_id": "b41",
    "citation_corpus_id": 3626819,
    "start": 1592,
    "end": 1612,
    "title": "Deep contextualized word representations",
    "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
    "prev": "In our experiments, SPIN reduces memory requirements, improves accuracy across a range of metalearning tasks, and improves state-of-the-art performance on an important practical problem, genotype imputation.",
    "curr": "INTRODUCTION\n\nRecent advances in deep learning have been driven by large-scale parametric models (Krizhevsky et al., 2012;Peters et al., 2018;Devlin et al., 2019;Ramesh et al., 2022).",
    "next": "Modern parametric models rely on large numbers of weights to capture the signal contained in the training set and to facilitate generalization (Frankle & Carbin, 2018;; as a result, they require non-trivial computational resources (Hoffmann et al., 2022), have limited interpretability (Belinkov, 2022), and impose a significant carbon footprint (Bender et al., 2021).",
    "query": "Are there any research papers on deep contextualized word representations and their applications in natural language processing tasks using bidirectional language models?"
  },
  {
    "index": 29,
    "source_corpus_id": 263831863,
    "ref_id": "b21",
    "citation_corpus_id": 49411844,
    "start": 2458,
    "end": 2476,
    "title": "DARTS: Differentiable Architecture Search",
    "abstract": "This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques.",
    "prev": "ation is that the model obtained by the proposed inner optimization can mimic the self-supervised target model.To achieve this, we also introduce the MSE between representations of the inner model and the self-supervised target model on the original full dataset for outer optimization.Lastly, assuming that a feature extractor is fixed, we only optimize a linear head on top of the feature extractor, which allows us to reduce the computational cost and obtain a closedform solution of the head with kernel ridge regression.We empirically validate the effectiveness of our method on various applications involving transfer learning.",
    "curr": "INTRODUCTION\n\nAs a consequence of collecting large-scale datasets and recent advances in parallel data processing, deep models have achieved remarkable success in various machine learning problems.However, some applications such as hyperparameter optimization (Franceschi et al., 2017), continual learning (Lopez-Paz & Ranzato, 2017), or neural architecture search (Liu et al., 2019) require repetitive training processes.In such scenarios, it is prohibitively costly to use all the examples from the huge dataset, which motivates the need to compress the full dataset into a small representative set of examples.Recently, many dataset distillation (or condensation) methods (Wang et al., 2018;Zhao et al., 2021;Zhao & Bilen, 2021;Nguyen et al., 2021a;b;Cazenavette et al., 2022;Zhou et al., 2022;Loo et al., 2022;Zhao & Bilen, 2023) have successfully learned a small number of examples on which we can train a model to achieve performance comparable to the one trained on the full dataset.",
    "next": "Despite the recent success of dataset distillation methods, they are not designed to produce a distilled dataset that can be effectively transferred to downstream tasks (Figure 1-(a)).In other words, we may not achieve meaningful performance improvements when pre-training a model on the distilled dataset and fine-tuning it on the target dataset.However, condensing general-purpose datasets",
    "query": "Are there any studies that explore methods for dataset distillation enabling effective transfer learning to downstream tasks?"
  },
  {
    "index": 30,
    "source_corpus_id": 5763832,
    "ref_id": "b28",
    "citation_corpus_id": 6628106,
    "start": 20967,
    "end": 20988,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "We use gradient descent with a batch size of 1 robot for optimization, as the problem is not stochastic in nature.",
    "curr": "The parameters are optimized with Adam's rule (Kingma and Ba, 2014) with a learning rate of 0.001.",
    "next": "Every update step with this method takes about 5 s on CPU.",
    "query": "Are there any research papers on the Adam optimization algorithm for stochastic optimization problems?"
  },
  {
    "index": 32,
    "source_corpus_id": 21850704,
    "ref_id": "b5",
    "citation_corpus_id": 1729177,
    "start": 1836,
    "end": 1854,
    "title": "Hedge Trimmer: A Parse-and-Trim Approach to Headline Generation",
    "abstract": "This paper presents Hedge Trimmer, a HEaDline GEneration system that creates a headline for a newspaper story using linguistically-motivated heuristics to guide the choice of a potential headline. We present feasibility tests used to establish the validity of an approach that constructs a headline by selecting words in order from a story. In addition, we describe experimental results that demonstrate the effectiveness of our linguistically-motivated approach over a HMM-based model, using both human evaluation and automatic metrics for comparing the two approaches.",
    "prev": "There are two prominent types of summarization algorithms.",
    "curr": "First, extractive summarization systems form summaries by copying parts of the input (Neto et al., 2002;Dorr et al., 2003;Nallapati et al., 2017).",
    "next": "Second, abstractive summarization systems generate new phrases, possibly rephrasing or using words that were not in the original text (Chopra et al., 2016;Zeng et al., 2016).",
    "query": "Are there any research papers on headline generation using linguistically-motivated heuristics or parse-and-trim approaches?"
  },
  {
    "index": 33,
    "source_corpus_id": 239009555,
    "ref_id": "b26",
    "citation_corpus_id": 3536221,
    "start": 3973,
    "end": 3995,
    "title": "MODEL-ENSEMBLE TRUST-REGION POLICY OPTI- MIZATION",
    "abstract": "Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning. However, they tend to suffer from high sample complexity which hinders their use in real-world domains. Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and, to date, it has succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and we show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning than backpropagation through time. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks 1 2 .Published as a conference paper at ICLR 2018 assumption in this approach, henceforth termed vanilla model-based RL, is that with enough data, the learned model will be accurate enough, such that a policy optimized on it will also perform well in the real environment.Although vanilla model-based RL can work well on low-dimensional tasks with relatively simple dynamics, we find that on more challenging continuous control tasks, performance was highly unstable. The reason is that the policy optimization tends to exploit regions where insufficient data is available to train the model, leading to catastrophic failures. Previous work has pointed out this issue as model biasWhile this issue can be regarded as a form of overfitting, we emphasize that standard countermeasures from the supervised learning literature, such as regularization or cross validation, are not sufficient here -supervised learning can guarantee generalization to states from the same distribution as the data, but the policy optimization stage steers the optimization exactly towards areas where data is scarce and the model is inaccurate. This problem is severely aggravated when expressive models such as deep neural networks are employed.",
    "prev": "Model-based RL (MBRL) reduces the amount of data required for policy optimization by approximating the environment with a learned model, which we can use to generate simulated state transitions (Sutton, 1990;Racanière et al., 2017;Moerland et al., 2020).",
    "curr": "While early approaches on lowdimensional tasks by Schneider (1997); Deisenroth & Rasmussen (2011) used probabilistic models with closed-form posteriors, recent methods rely on neural networks to scale to complex tasks on discrete (Kaiser et al., 2020) and continuous (Chua et al., 2018;Kurutach et al., 2018) action spaces.",
    "next": "However, the learned representation of the true environment always remains imperfect, which introduces approximation errors to the RL problem (Atkeson & Santamaria, 1997;Abbeel et al., 2006).",
    "query": "Are there any research papers on model-based reinforcement learning methods that use model ensembles and trust-region optimization to address approximation errors and improve stability in complex environments?"
  },
  {
    "index": 36,
    "source_corpus_id": 261697392,
    "ref_id": "b16",
    "citation_corpus_id": 227209335,
    "start": 2939,
    "end": 2942,
    "title": "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024ˆ1024 images for the first time from a score-based generative model.",
    "prev": "However, despite their impressive generation quality, these models often suffer from excessive inference time and computational consumption [5,7,8,9,10].",
    "curr": "This can be attributed to the fact that most of these models are either auto-regressive [13,14,15] or diffusion models [16,17].",
    "next": "For instance, Stable Diffusion, even when using a state-of-the-art sampler [18,19,20], typically requires more than 20 steps to generate acceptable images.",
    "query": "Are there any studies that explore methods to reduce computational consumption in diffusion or auto-regressive models through score-based generative modeling and stochastic differential equations?"
  },
  {
    "index": 38,
    "source_corpus_id": 220302524,
    "ref_id": "b1",
    "citation_corpus_id": 3618568,
    "start": 1983,
    "end": 1986,
    "title": "Reading Wikipedia to Answer Open-Domain Questions",
    "abstract": "This paper proposes to tackle opendomain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.",
    "prev": "Introduction\n\nMany language systems rely on text retrieval as their first step to find relevant information.",
    "curr": "For example, search ranking [1], open domain question answering [2], and fact verification [3,4] all first retrieve relevant documents as the input to their later stage reranking, machine reading, and reasoning models.",
    "next": "All these later-stage models enjoy the advancements of deep learning techniques [5,6], while, in contrast, the first stage retrieval still mainly relies on matching discrete bag-of-words [1,2,3,7].",
    "query": "Are there any research papers on methods that combine document retrieval with machine comprehension for open-domain question answering?"
  },
  {
    "index": 39,
    "source_corpus_id": 252596001,
    "ref_id": "b20",
    "citation_corpus_id": 209439843,
    "start": 6589,
    "end": 6611,
    "title": "MEASURING COMPOSITIONAL GENERALIZATION: A COMPREHENSIVE METHOD ON REALISTIC DATA",
    "abstract": "State-of-the-art machine learning methods exhibit limited compositional generalization. At the same time, there is a lack of realistic benchmarks that comprehensively measure this ability, which makes it challenging to find and evaluate improvements. We introduce a novel method to systematically construct such benchmarks by maximizing compound divergence while guaranteeing a small atom divergence between train and test sets, and we quantitatively compare this method to other approaches for creating compositional generalization benchmarks. We present a large and realistic natural language question answering dataset that is constructed according to this method, and we use it to analyze the compositional generalization ability of three machine learning architectures. We find that they fail to generalize compositionally and that there is a surprisingly strong negative correlation between compound divergence and accuracy. We also demonstrate how our method can be used to create new compositionality benchmarks on top of the existing SCAN dataset, which confirms these findings. Radev. Improving text-to-SQL evaluation methodology. In ACL, 2018. URL http://aclweb.org/anthology/P18-1033.Jerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2):3-71, 1988. URL https://pdfs.semanticscholar.org/d806/ 76034bfabfea59f35698af0f715a555fcf50.pdf.",
    "prev": "d and wrote N1 A: that (edited and wrote) (N1) Q: whose N1 was employed by and founded M1 A: whose (N1) (was employed by and founded) (M1) Q: whose N1 married M2 and married N2 A: whose (N1) ((married M2) and (married N2)) Q: that played M2 , played M3 , and played M4 A: that ((played M2) , (played M3) , and (played M4)) Q: that wrote , edited , executive produced , and directed N1 A: that (wrote , edited , executive produced , and directed) (N1) Q: that N3 were written by and art directed by A: that (N3) (were written by and art directed by) Q: Was N1 N2\n\nINTRODUCTION\n\nCompositionality is a key part of human intelligence as it allows us to understand and produce a potentially infinite number of novel combinations of known components (Chomsky, 1957;Montague, 1970;Lake et al., 2017).",
    "curr": "In contrast, standard neural sequence models, transformers and recurrent neural networks, often fail to capture the compositional structure of the problem domain and thus fail to generalize compositionally (Keysers et al., 2020;.",
    "next": "Prior efforts to improve compositional generalization primarily rely on specialized architectures or training procedures (Lake, 2019;Nye et al., 2020;Andreas, 2020;Conklin et al., 2021;Liu et al., 2021).",
    "query": "Are there any research papers on methods for measuring compositional generalization in machine learning using realistic benchmarks?"
  },
  {
    "index": 40,
    "source_corpus_id": 259095643,
    "ref_id": "b15",
    "citation_corpus_id": 10494183,
    "start": 3887,
    "end": 3906,
    "title": "Generating Steganographic Text with LSTMs",
    "abstract": "Motivated by concerns for user privacy, we design a steganographic system (\"stegosystem\") that enables two users to exchange encrypted messages without an adversary detecting that such an exchange is taking place. We propose a new linguistic stegosystem based on a Long Short-Term Memory (LSTM) neural network. We demonstrate our approach on the Twitter and Enron email datasets and show that it yields high-quality steganographic text while significantly improving capacity (encrypted bits per word) relative to the state-of-the-art.",
    "prev": "This can lead to low accuracy or impractical false positive rates that especially impact vulnerable subgroups, such as non-native speakers [Liang et al., 2023].",
    "curr": "One way to enable accurate detection of machine-generated text is through watermarking, where generated text is marked imperceptibly so that its origin can be determined [Atallah et al., 2001, Fang et al., 2017, Kirchenbauer et al., 2023.",
    "next": "Because watermarks rely on subtle patterns in text that are statistically unlikely to be replicated by a human, watermarking enables detectors that achieve high levels of accuracy on relatively short fragments of text.",
    "query": "Are there any research papers on methods for generating steganographic text using LSTM networks to enable secure and undetectable communication?"
  },
  {
    "index": 41,
    "source_corpus_id": 247595088,
    "ref_id": "b23",
    "citation_corpus_id": 6628106,
    "start": 20588,
    "end": 20607,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "The goal of our first experiments is to give a broad comparison of the proposed HIGs with commonly used optimizers.",
    "curr": "This includes stochastic gradient descent (SGD), Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), RMSprop , Adam (Kingma & Ba, 2015), and Gauss-Newton (GN) applied to mini batches.",
    "next": "The results are shown in figure 2b where all curves show the best runs for each optimizer with suitable hyperparameters independently selected, as explained in the appendix.",
    "query": "Are there any research papers on the Adam optimizer for stochastic optimization and its performance compared to other gradient-based methods like SGD, Adagrad, and RMSprop?"
  },
  {
    "index": 42,
    "source_corpus_id": 259342096,
    "ref_id": "b43",
    "citation_corpus_id": 237416585,
    "start": 7349,
    "end": 7352,
    "title": "Published as a conference paper at ICLR 2022 FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS",
    "abstract": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning-finetuning language models on a collection of datasets described via instructions-substantially improves zeroshot performance on unseen tasks.",
    "prev": "Among these models, transformer-based language models [49] have emerged as the de facto standard for a wide range of NLP tasks, owing to their unparalleled capabilities in capturing complex linguistic patterns and generalizing across diverse contexts.",
    "curr": "One particularly successful paradigm for training such models is instruction-tuning [44,52,4,28,34,38], which enhances their performance on specific tasks by adapting their pre-trained representations to follow natural language instructions.",
    "next": "While the benefits of Large Language Models (LLMs) are indisputable, their rapidly growing size and computational requirements pose significant challenges in terms of training efficiency, memory footprint, and deployment costs.",
    "query": "Are there any studies that explore methods for improving zero-shot learning capabilities in language models through instruction tuning?"
  },
  {
    "index": 43,
    "source_corpus_id": 21196492,
    "ref_id": "b20",
    "citation_corpus_id": 11816014,
    "start": 4490,
    "end": 4514,
    "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
    "abstract": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com.",
    "prev": "(2016) show that skip layer connections facilitate signal propagation and alleviate gradient degradation.",
    "curr": "The combination of the deep residual coattention encoder and the mixed objective leads to higher performance across question types, question lengths, and answer lengths on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) compared to our DCN baseline.",
    "next": "The improvement is especially apparent on long questions, which require the model to capture long-range dependencies between the document and the question.",
    "query": "Are there any research papers on methods for improving machine comprehension models using deep residual coattention encoders and mixed objectives to handle long-range dependencies in question answering tasks?"
  },
  {
    "index": 45,
    "source_corpus_id": 254926490,
    "ref_id": "b48",
    "citation_corpus_id": 237491751,
    "start": 5512,
    "end": 5531,
    "title": "SITUATEDQA: Incorporating Extra-Linguistic Contexts into QA",
    "abstract": "Answers to the same question may change depending on the extra-linguistic contexts (when and where the question was asked). To study this challenge, we introduce SITUATEDQA, an open-retrieval QA dataset where systems must produce the correct answer to a question given the temporal or geographical context. To construct SITUATEDQA, we first identify such questions in existing QA datasets. We find that a significant proportion of information seeking questions have context-dependent answers (e.g. roughly 16.5% of NQ-Open). For such context-dependent questions, we then crowdsource alternative contexts and their corresponding answers. Our study shows that existing models struggle with producing answers that are frequently updated or from uncommon locations. We further quantify how existing models, which are trained on data collected in the past, fail to generalize to answering questions asked in the present, even when provided with an updated evidence corpus (a roughly 15 point drop in accuracy). Our analysis suggests that open-retrieval QA benchmarks should incorporate extra-linguistic context to stay relevant globally and in the future. Our data, code, and datasheet are available at https: //situatedqa.github.io/.",
    "prev": "Evaluate humans and models on a new benchmark of ambiguously-specified tasks, demonstrating that while pure language models fail to disambiguate the intended task well, sufficiently-large models trained with human feedback data are able to approach or even exceed the performance of our human participants to resolve the ambiguity between tasks 3.",
    "curr": "Show how finetuning on ambiguous in-context prompts and examples can enable traditional language models to surpass the performance of HFD models when evaluated on unseen tasks, providing a promising route towards models that capably manage task ambiguity 2 RELATED WORK\n\n\nAMBIGUITY IN NATURAL LANGUAGE PROCESSING\n\nAmbiguity is a well-studied topic in NLP, with work spanning topics as diverse as search queries (Cronen-Townsend & Croft, 2002;Wang & Agichtein, 2010), question answering (Min et al., 2020;Zhang & Choi, 2021), named entities (Bunescu & Pasca, 2006;Cucerzan, 2007;Dredze et al., 2010), coreference resolution (Webster et al., 2018), machine translation (Stanovsky et al., 2019), and information-seeking dialogues (Aliannejadi et al., 2019;Guo et al., 2021;Aliannejadi et al., 2021;Sun et al., 2022;.",
    "next": "Our work differs from these prior streams of work by studying task ambiguity (Finn et al., 2018;Tamkin et al., 2022c), where the task the agent is being asked to perform is ambiguous, rather than an ambiguous input for a clear task.",
    "query": "Are there any studies that explore methods for handling task ambiguity in question answering by incorporating temporal or geographical context?"
  },
  {
    "index": 46,
    "source_corpus_id": 52912260,
    "ref_id": "b22",
    "citation_corpus_id": 14124313,
    "start": 1763,
    "end": 1792,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "ckle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task.Yet, even with such meta-learning, the low-data problem in the novel classification task still remains.In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data.TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results.",
    "curr": "INTRODUCTION\n\nRecent breakthroughs in deep learning (Krizhevsky et al., 2012;Simonyan and Zisserman, 2015;He et al., 2016) highly rely on the availability of large amounts of labeled data.However, this reliance on large data increases the burden of data collection, which hinders its potential applications to the low-data regime where the labeled data is rare and difficult to gather.On the contrary, humans have the ability to recognize new objects after observing only one or few instances (Lake et al., 2011).For example, children can generalize the concept of \"apple\" after given a single instance of it.This significant gap between human and deep learning has reawakened the research interest on few-shot learning (Vinyals et al., 2016;Snell et al., 2017;Finn et al., 2017;Ravi and Larochelle, 2017;Lee and Choi, 2018;Xu et al., 2017;Wang et al., 2018).",
    "next": "Few-shot learning aims to learn a classifier that generalizes well with a few examples of each of these classes.Traditional techniques such as fine-tuning (Jia et al., 2014) that work well with deep learning models would severely overfit on this",
    "query": "Are there any studies that explore few-shot learning approaches using transductive inference and graph-based label propagation for low-data classification tasks?"
  },
  {
    "index": 47,
    "source_corpus_id": 3535369,
    "ref_id": "b8",
    "citation_corpus_id": 6628106,
    "start": 34220,
    "end": 34239,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "The number of times we train the value network is fixed to 25, while for the control variate, it was chosen to be a hyperparameter.",
    "curr": "All models were trained using ADAM (Kingma & Ba, 2015), with β 1 = 0.9, β 2 = 0.999, and = 1e − 08.",
    "next": "The baseline A2C case has 2 hyperparameters to tune: the learning rate for the optimizer for the policy and value network.",
    "query": "Are there any research papers on the ADAM optimizer's application in machine learning training, particularly focusing on hyperparameter tuning and stochastic optimization techniques?"
  },
  {
    "index": 49,
    "source_corpus_id": 247595243,
    "ref_id": "b62",
    "citation_corpus_id": 220363897,
    "start": 9302,
    "end": 9320,
    "title": "Meta-Learning Symmetries by Reparameterization",
    "abstract": "Many successful deep learning architectures are equivariant to certain transformations in order to conserve parameters and improve generalization: most famously, convolution layers are equivariant to shifts of the input. This approach only works when practitioners know a priori symmetries of the task and can manually construct an architecture with the corresponding equivariances. Our goal is a general approach for learning equivariances from data, without needing prior knowledge of a task's symmetries or custom task-specific architectures. We present a method for learning and encoding equivariances into networks by learning corresponding parameter sharing patterns from data. Our method can provably encode equivarianceinducing parameter sharing for any finite group of symmetry transformations, and we find experimentally that it can automatically learn a variety of equivariances from symmetries in data. We provide our experiment code and pre-trained models at https://github.com/AllanYangZhou/metalearning-symmetries.",
    "prev": "In our imbalanced setting, generative invariance transfer aims to learn transformations from the head classes that apply to the tail classes of the same dataset, and does not assume the data is paired or grouped.",
    "curr": "There is a general interest in obtaining invariances for machine learning models (Benton et al., 2020;Zhou et al., 2021).",
    "next": "Data augmentation (Beymer & Poggio, 1995;Niyogi et al., 1998) can be used to train classifiers to be invariant to certain hand-picked transformations, but requires the practitioner to know and implement those transformations in advance.",
    "query": "Are there any research papers on methods for learning equivariances in neural networks through meta-learning and reparameterization without prior knowledge of symmetries?"
  },
  {
    "index": 50,
    "source_corpus_id": 252683543,
    "ref_id": "b6",
    "citation_corpus_id": 44134226,
    "start": 4073,
    "end": 4091,
    "title": "Hierarchical Neural Story Generation",
    "abstract": "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",
    "prev": "To avoid this inconsistency, they proposed a self-terminating (ST) language model that uses new parametrization for its classifier rather than usual softmax parametrization.",
    "curr": "They proved that the ST language model is consistent with respect to greedy search, beam search, top-k sampling (Fan et al., 2018) as well as nucleus sampling (Holtzman et al., 2020).",
    "next": "The ST language model increases the termination probability of each sequence monotonically to 1, but this parametrization is not appropriate for learning our natural language.",
    "query": "Are there any research papers on hierarchical neural story generation with model fusion and gated multi-scale self-attention mechanisms?"
  },
  {
    "index": 51,
    "source_corpus_id": 246240237,
    "ref_id": "b0",
    "citation_corpus_id": 53022741,
    "start": 13236,
    "end": 13253,
    "title": "Subgradient Descent Learns Orthogonal Dictionaries",
    "abstract": "This paper concerns dictionary learning, i.e., sparse coding, a fundamental representation learning problem. We show that a subgradient descent algorithm, with random initialization, can provably recover orthogonal dictionaries on a natural nonsmooth, nonconvex 1 minimization formulation of the problem, under mild statistical assumptions on the data. This is in contrast to previous provable methods that require either expensive computation or delicate initialization schemes. Our analysis develops several tools for characterizing landscapes of nonsmooth functions, which might be of independent interest for provable training of deep networks with nonsmooth activations (e.g., ReLU), among numerous other applications. Preliminary experiments corroborate our analysis and show that our algorithm works well empirically in recovering orthogonal dictionaries.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on subgradient descent methods for provably learning orthogonal dictionaries in sparse coding, and their applications to deep networks with nonsmooth activations?"
  },
  {
    "index": 53,
    "source_corpus_id": 202660778,
    "ref_id": "b10",
    "citation_corpus_id": 52920808,
    "start": 8237,
    "end": 8255,
    "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks",
    "abstract": "One of the mystery in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an m hidden node shallow neural network with ReLU activation and n training data, we show as long as m is large enough and the data is non-degenerate, randomly initialized gradient descent converges a globally optimal solution with a linear convergence rate for the quadratic loss function.Our analysis is based on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods. * Equal Contribution.",
    "prev": "After the ICLR conference submission deadline, Yang & Zhang (2019) revised their paper by adding a new variance reduction algorithm that achieves O(H 2 / 3/2 ) sample complexity, which is also worse than our result by a factor of O(H 2 ).",
    "curr": "Apart from the convergence analysis of the general nonconcave performance functions, there has emerged a line of work Liu et al., 2019; that studies the global convergence of (proximal/trust-region) policy optimization with neural network function approximation, which applies the theory of overparameterized neural networks (Du et al., 2019b;a;Allen-Zhu et al., 2019;Zou et al., 2019;Cao & Gu, 2019) to reinforcement learning.",
    "next": "Notation v 2 denotes the Euclidean norm of a vector v ∈ R d and A 2 denotes the spectral norm of a matrix A ∈ R d×d .",
    "query": "Are there any research papers on gradient descent's convergence in over-parameterized neural networks, particularly focusing on provable optimization and the role of overparameterization in non-convex settings?"
  },
  {
    "index": 54,
    "source_corpus_id": 235293695,
    "ref_id": "b7",
    "citation_corpus_id": 54443381,
    "start": 2354,
    "end": 2377,
    "title": "EFFICIENT LIFELONG LEARNING WITH A-GEM",
    "abstract": "In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC  and other regularizationbased methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency. 1",
    "prev": "Building a system resembling human learning abilities is a deep-rooted desire since sustainable learning over a long-term period is essential for general artificial intelligence.",
    "curr": "In light of this need, continual learning (CL) (Thrun, 1995), or lifelong learning, tackles a learning scenario where a model continuously learns over a sequence of tasks (Kumar & Daume III, 2012;Li & Hoiem, 2016) within a broad research area, such as classification (Kirkpatrick et al., 2017;Chaudhry et al., 2019a), image generation (Zhai et al., 2019), language learning (Li et al., 2019b;Biesialska et al., 2020), clinical application (Lee & Lee, 2020;Lenga et al., 2020), speech recognition (Sadhu & Hermansky, 2020), and federated learning (Yoon et al., 2021).",
    "next": "A well-known challenge for continual learning is catastrophic forgetting (McCloskey & Cohen, 1989), where the continual learner loses the fidelity for past tasks after adapting the previously learned knowledge to future tasks.",
    "query": "Are there any research papers on efficient lifelong learning approaches that address catastrophic forgetting and evaluate computational and memory efficiency in continual learning scenarios?"
  },
  {
    "index": 56,
    "source_corpus_id": 238582772,
    "ref_id": "b22",
    "citation_corpus_id": 108300573,
    "start": 4309,
    "end": 4331,
    "title": "INTERPOLATION-PREDICTION NETWORKS FOR IRREGULARLY SAMPLED TIME SERIES",
    "abstract": "In this paper, we present a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series. The architecture is based on the use of a semi-parametric interpolation network followed by the application of a prediction network. The interpolation network allows for information to be shared across multiple dimensions of a multivariate time series during the interpolation stage, while any standard deep learning model can be used for the prediction network. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. We investigate the performance of this architecture on both classification and regression tasks, showing that our approach outperforms a range of baseline and recently proposed models. 1 1 Our implementation is available at : https://github.com/mlds-lab/interp-net 1 arXiv:1909.07782v1 [cs.LG] 13 Sep 2019Published as a conference paper at ICLR 2019 trends. Similar to the work ofLipton et al. (2016)andChe et al. (2018a), our architecture also explicitly leverages a separate information channel related to patterns of observation times. However, our representation uses a semi-parametric intensity function representation of this information that is more closely related to the work of Lasko (2014) on modeling medical event point processes.Our architecture thus produces three output time series for each input time series: a smooth interpolation modeling broad trends in the input, a short time-scale interpolation modeling transients, and an intensity function modeling local observation frequencies.This work is motivated by problems in the analysis of electronic health records (EHRs)(Marlin et al., 2012;Lipton et al., 2016;Futoma et al., 2017;Che et al., 2018a). It remains rare for hospital systems to capture dense physiological data streams. Instead, it is common for the physiological time series data in electronic health records to be both sparse and irregularly sampled. The additional issue of the lack of alignment in the observation times across physiological variables is also very common.We evaluate the proposed architecture on two datasets for both classification and regression tasks. Our approach outperforms a variety of simple baseline models as well as the basic and advanced GRU models introduced by Che et al. (2018a) across several metrics. We also compare our model with to the Gaussian process adapter (Li & Marlin, 2016) and multi-task Gaussian process RNN classifier(Futoma et al., 2017). Further, we perform full ablation testing of the information channels our architecture can produce to assess their impact on classification and regression performance.",
    "prev": "This decoupled approach does not fully exploit informative missingness patterns or deal with irregular sampling, thus producing suboptimal Previous studies (Wu et al., 2021;Li et al., 2020a;Zhang et al., 2019) have noted that inter-sensor correlations bring rich information in modeling time series.",
    "curr": "However, only few studies consider relational structure of irregularly sampled time series, and those which do have limited ability in capturing inter-sensor connections (Wu et al., 2021;Shukla & Marlin, 2018).",
    "next": "In contrast, we integrate recent advances in graph neural networks to take advantage of relational structure among sensors.",
    "query": "Are there any research papers on methods for handling irregularly sampled time series by leveraging relational structures and graph neural networks?"
  },
  {
    "index": 59,
    "source_corpus_id": 220302148,
    "ref_id": "b29",
    "citation_corpus_id": 3300937,
    "start": 2398,
    "end": 2401,
    "title": "Learning From Noisy Singly-labeled Data",
    "abstract": "Supervised learning depends on annotated examples, which are taken to be the ground truth. But these labels often come from noisy crowdsourcing platforms, like Amazon Mechanical Turk. Practitioners typically collect multiple labels per example and aggregate the results to mitigate noise (the classic crowdsourcing problem). Given a fixed annotation budget and unlimited unlabeled data, redundant annotation comes at the expense of fewer labeled examples. This raises two fundamental questions: (1) How can we best learn from noisy workers? (2) How should we allocate our labeling budget to maximize the performance of a classifier? We propose a new algorithm for jointly modeling labels and worker quality from noisy crowd-sourced data. The alternating minimization proceeds in rounds, estimating worker quality from disagreement with the current model and then updating the model by optimizing a loss function that accounts for the current estimate of worker quality. Unlike previous approaches, even with only one annotation per example, our algorithm can estimate worker quality. We establish a generalization error bound for models learned with our algorithm and establish theoretically that it's better to label many examples once (vs less multiply) when worker quality is above a threshold. Experiments conducted on both ImageNet (with simulated noisy workers) and MS-COCO (using the real crowdsourced labels) confirm our algorithm's benefits. 1 1. This work was done when the authors were visiting Amazon",
    "prev": "(1)\nWhile ERM is widely used and offers nice statistical properties, it can also perform poorly in practical situations where average performance is not an appropriate surrogate for the objective of interest.",
    "curr": "Significant research has thus been devoted to developing alternatives to traditional ERM for diverse applications, such as learning in the presence of noisy/corrupted data or outliers [25,30], performing classification with imbalanced data [37,38], ensuring that subgroups within a population are treated fairly [36,42,56], or developing solutions with favorable out-of-sample performance [43].",
    "next": "In this paper, we suggest that deficiencies in ERM can be flexibly addressed via a unified framework, tilted empirical risk minimization (TERM).",
    "query": "Are there any research papers on methods for learning from noisy singly-labeled data and alternatives to empirical risk minimization (ERM) in the presence of label noise?"
  },
  {
    "index": 60,
    "source_corpus_id": 245828046,
    "ref_id": "b21",
    "citation_corpus_id": 6212000,
    "start": 24704,
    "end": 24723,
    "title": "UNDERSTANDING DEEP LEARNING REQUIRES RE- THINKING GENERALIZATION",
    "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on the generalization of deep learning models and the rethinking of traditional explanations for their performance?"
  },
  {
    "index": 61,
    "source_corpus_id": 211132990,
    "ref_id": "b32",
    "citation_corpus_id": 3536221,
    "start": 4217,
    "end": 4240,
    "title": "MODEL-ENSEMBLE TRUST-REGION POLICY OPTI- MIZATION",
    "abstract": "Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning. However, they tend to suffer from high sample complexity which hinders their use in real-world domains. Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and, to date, it has succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and we show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning than backpropagation through time. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks 1 2 .Published as a conference paper at ICLR 2018 assumption in this approach, henceforth termed vanilla model-based RL, is that with enough data, the learned model will be accurate enough, such that a policy optimized on it will also perform well in the real environment.Although vanilla model-based RL can work well on low-dimensional tasks with relatively simple dynamics, we find that on more challenging continuous control tasks, performance was highly unstable. The reason is that the policy optimization tends to exploit regions where insufficient data is available to train the model, leading to catastrophic failures. Previous work has pointed out this issue as model biasWhile this issue can be regarded as a form of overfitting, we emphasize that standard countermeasures from the supervised learning literature, such as regularization or cross validation, are not sufficient here -supervised learning can guarantee generalization to states from the same distribution as the data, but the policy optimization stage steers the optimization exactly towards areas where data is scarce and the model is inaccurate. This problem is severely aggravated when expressive models such as deep neural networks are employed.",
    "prev": "A further study confirms that deep ensembles generally achieves the best performance on out-of-distribution uncertainty benchmarks (Ovadia et al., 2019;Gustafsson et al., 2019), compared to other methods such as MC-dropout (Gal & Ghahramani, 2015).",
    "curr": "In other applications such as model-based reinforcement learning (Deisenroth & Rasmussen, 2011;Wang et al., 2019), ensembles of neural networks can be used to estimate model uncertainty, leading to better overall performance (Kurutach et al., 2018).",
    "next": "Despite their success on benchmarks, ensembles are limited in practice due to their expensive computational and memory costs, which increase linearly with the ensemble size in both training and testing.",
    "query": "Are there any research papers on the use of model ensembles in reinforcement learning for uncertainty estimation and performance improvement, while addressing computational and memory limitations?"
  },
  {
    "index": 62,
    "source_corpus_id": 213938729,
    "ref_id": "b10",
    "citation_corpus_id": 6706414,
    "start": 6413,
    "end": 6438,
    "title": "Published as a conference paper at ICLR 2015 EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES",
    "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any studies that explain the vulnerability of neural networks to adversarial examples and explore methods for adversarial training to enhance model robustness?"
  },
  {
    "index": 64,
    "source_corpus_id": 259375870,
    "ref_id": "b23",
    "citation_corpus_id": 243865663,
    "start": 12470,
    "end": 12488,
    "title": "Have You Seen That Number? Investigating Extrapolation in Question Answering Models",
    "abstract": "Numerical reasoning in machine reading comprehension (MRC) has shown drastic improvements over the past few years. While the previous models for numerical MRC are able to interpolate the learned numerical reasoning capabilities, it is not clear whether they can perform just as well on numbers unseen in the training dataset. Our work rigorously tests state-of-the-art models on DROP, a numerical MRC dataset, to see if they can handle passages that contain out-of-range numbers. One of the key findings is that the models fail to extrapolate to unseen numbers. Presenting numbers as digit-by-digit input to the model, we also propose the E-digit number form that alleviates the lack of extrapolation in models and reveals the need to treat numbers differently from regular words in the text. Our work provides a valuable insight into the numerical MRC models and the way to represent number forms in MRC. * Equal contribution.",
    "prev": "Our work focuses on decoder-only models since they are well-suited for text generation and are widely used in LLMs (Brown et al., 2020;Touvron et al., 2023;MosaicML, 2023).",
    "curr": "However, encoder-decoder models have also been extensively studied in the literature in the context of learning arithmetic (Kim et al., 2021;Wang et al., 2021).",
    "next": "Qian et al.",
    "query": "Are there any studies on the extrapolation capabilities of numerical reasoning models in machine reading comprehension, particularly regarding their performance on out-of-range numbers?"
  },
  {
    "index": 65,
    "source_corpus_id": 222141728,
    "ref_id": "b2",
    "citation_corpus_id": 52922363,
    "start": 4476,
    "end": 4496,
    "title": "A CONVERGENCE ANALYSIS OF GRADIENT DESCENT FOR DEEP LINEAR NEURAL NETWORKS",
    "abstract": "We analyze speed of convergence to global optimum for gradient descent training a deep linear neural network (parameterized as x → W N W N −1 · · · W 1 x) by minimizing the 2 loss over whitened data. Convergence at a linear rate is guaranteed when the following hold: (i) dimensions of hidden layers are at least the minimum of the input and output dimensions; (ii) weight matrices at initialization are approximately balanced; and (iii) the initial loss is smaller than the loss of any rank-deficient solution. The assumptions on initialization (conditions (ii) and (iii)) are necessary, in the sense that violating any one of them may lead to convergence failure. Moreover, in the important case of output dimension 1, i.e. scalar regression, they are met, and thus convergence to global optimum holds, with constant probability under a random initialization scheme. Our results significantly extend previous analyses, e.g., of deep linear residual networks(Bartlett et al., 2018).",
    "prev": "(2018); Oymak & Soltanolkotabi (2020), and many more).",
    "curr": "There are also convergence results that focus on linear networks, without nonlinear activations (Bartlett et al., 2018;Arora et al., 2019a;Wu et al., 2019;Du & Hu, 2019;Hu et al., 2020).",
    "next": "These results typically focus on the convergence of loss, hence do not address which of the many global minima is reached.",
    "query": "Are there any research papers on convergence analysis of gradient descent for deep linear neural networks and the conditions required for guaranteed convergence to global optima?"
  },
  {
    "index": 68,
    "source_corpus_id": 253080406,
    "ref_id": "b24",
    "citation_corpus_id": 3626819,
    "start": 7137,
    "end": 7158,
    "title": "Deep contextualized word representations",
    "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
    "prev": "Language models.",
    "curr": "Large language models, such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2018), and GPT-2 (Radford et al., 2019), are able to achieve state-of-the-art performance on many standard NLP benchmarks.",
    "next": "More recent works, such as GPT-3 (Brown et al., 2020), PALM (Chowdhery et al., 2022), and Chinchilla (Hoffmann et al., 2022) further enable few-shot learning from textual prompts.",
    "query": "Are there any research papers on deep contextualized word representations using bidirectional language models and their impact on NLP tasks?"
  },
  {
    "index": 69,
    "source_corpus_id": 259833441,
    "ref_id": "b16",
    "citation_corpus_id": 13046179,
    "start": 5144,
    "end": 5148,
    "title": "A BASELINE FOR DETECTING MISCLASSIFIED AND OUT-OF-DISTRIBUTION EXAMPLES IN NEURAL NETWORKS",
    "abstract": "We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.Published as a conference paper at ICLR 2017 one method which outperforms the baseline on some (but not all) tasks. This new method evaluates the quality of a neural network's input reconstruction to determine if an example is abnormal.",
    "prev": "However, in real-world classifier deployment, one may encounter out-of-distribution (OOD) test samples, i.e., samples drawn from some distinct distribution P out = P in (e.g., images of aeroplanes).",
    "curr": "Out-ofdistribution detection is the problem of accurately identifying such OOD samples, and has received considerable study of late [18,30,20,43,23,22,48,51,3,26,52,46,21].",
    "next": "An accurate OOD detector allows one to abstain from making a prediction on OOD samples, rather than making an egregiously incorrect prediction; this yields more reliable and trust-worthy classifiers.",
    "query": "Are there any research papers on methods for detecting out-of-distribution and misclassified examples in neural networks?"
  },
  {
    "index": 75,
    "source_corpus_id": 231918471,
    "ref_id": "b25",
    "citation_corpus_id": 108304275,
    "start": 8791,
    "end": 8815,
    "title": "DISCRIMINATOR-ACTOR-CRITIC: ADDRESSING SAMPLE INEFFICIENCY AND REWARD BIAS IN ADVERSARIAL IMITATION LEARNING",
    "abstract": "We identify two issues with the family of algorithms based on the Adversarial Imitation Learning framework. The first problem is implicit bias present in the reward functions used in these algorithms. While these biases might work well for some environments, they can also lead to sub-optimal behavior in others. Secondly, even though these algorithms can learn from few expert demonstrations, they require a prohibitively large number of interactions with the environment in order to imitate the expert for many real-world applications. In order to address these issues, we propose a new algorithm called Discriminator-Actor-Critic that uses off-policy Reinforcement Learning to reduce policy-environment interaction sample complexity by an average factor of 10. Furthermore, since our reward function is designed to be unbiased, we can apply our algorithm to many problems without making any task-specific adjustments.",
    "prev": "Moving within the maximum-entropy RL framework (Ziebart, 2010;Levine, 2018), Ho & Ermon (2016) noted that the full procedure (RL • IRL) can be interpreted equivalently as the minimisation of some divergence between occupancy measures of the imitator and demonstrator:\n\narg min\nπ {ψ * (ρ π − ρ π D ) − H(π)},(1)\nwith H(π) being the discounted causal entropy (Bloem & Bambos, 2014) of the policy and ψ * the Fenchel conjugate of a chosen regulariser on the form of the reward.",
    "curr": "These are typically optimised in an adversarial fashion (Goodfellow et al., 2014) and given the focus on evaluating ρ π this often requires extensive interaction with the environment, otherwise banking on approximations over a replay buffer (Kostrikov et al., 2018) or a reformulation of the divergence to allow for off-policy evaluation (Kostrikov et al., 2019).",
    "next": "Bear in mind that optimal policies within the maximum-entropy framework are parameterised by a Boltzmann distribution:\nπ(a|s) = exp(Q(s, a)) b∈A exp(Q(s, b)) ,(2)\nwith Q(s, a) the soft Q-function, defined recursively via the soft Bellman-equation:\nQ(s, a) R(s, a) + γE s ∼ρπ soft max a Q(s , a )) .",
    "query": "Are there any research papers on methods for addressing sample inefficiency and reward bias in adversarial imitation learning using off-policy reinforcement learning techniques?"
  },
  {
    "index": 76,
    "source_corpus_id": 7942973,
    "ref_id": "b1",
    "citation_corpus_id": 6015236,
    "start": 1542,
    "end": 1562,
    "title": "Leveraging Linguistic Structure For Open Domain Information Extraction",
    "abstract": "Relation triples produced by open domaininformation extraction (open IE) systems are useful for question answering, inference, and other IE tasks. Traditionally these are extracted using a large set of patterns; however, this approach is brittle on out-of-domain text and long-range dependencies, and gives no insight into the substructure of the arguments. We replace this large pattern set with a few patterns for canonically structured sentences, and shift the focus to a classifier which learns to extract self-contained clauses from longer sentences. We then run natural logic inference over these short clauses to determine the maximally specific arguments for each candidate triple. We show that our approach outperforms a state-of-the-art open IE system on the end-to-end TAC-KBP 2013 Slot Filling task.",
    "prev": "We also show which hyperparameter choices had a significant effect on parsing accuracy, allowing us to achieve large gains over other graph-based approaches.",
    "curr": "INTRODUCTION\n\nDependency parsers-which annotate sentences in a way designed to be easy for humans and computers alike to understand-have been found to be extremely useful for a sizable number of NLP tasks, especially those involving natural language understanding in some way (Bowman et al., 2016;Angeli et al., 2015;Levy & Goldberg, 2014;Toutanova et al., 2016;Parikh et al., 2015).",
    "next": "However, frequent incorrect parses can severely inhibit final performance, so improving the quality of dependency parsers is needed for the improvement and success of these downstream tasks.",
    "query": "Are there any research papers on improving dependency parsers for open domain information extraction using linguistic structure and natural logic inference?"
  },
  {
    "index": 78,
    "source_corpus_id": 250451381,
    "ref_id": "b37",
    "citation_corpus_id": 216562627,
    "start": 6213,
    "end": 6233,
    "title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels",
    "abstract": "We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC)[20], are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based[21,31,22]methods and recently proposed contrastive learning[42]. Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at https://sites.google.com/view/data-regularized-q.IntroductionSample-efficient deep reinforcement learning (RL) algorithms capable of directly training from image pixels would open up many real-world applications in control and robotics. However, simultaneously training a convolutional encoder alongside a policy network is challenging when given limited environment interaction, strong correlation between samples and a typically sparse reward signal. Naive attempts to use a large capacity encoder result in severe over-fitting (seeFigure 1a) and smaller encoders produce impoverished representations that limit task performance.Limited supervision is a common problem across AI and a number of approaches are adopted: (i) pretraining with self-supervised learning (SSL), followed by standard supervised learning; (ii) supervised learning with an additional auxiliary loss and (iii) supervised learning with data augmentation. SSL approaches are highly effective in the large data regime, e.g. in domains such as vision [7, 23] and NLP[12,13]where large (unlabeled) datasets are readily available. However, in RL this is not the case: an off-policy RL agent is trained on a replay buffer that grows as the agent interacts with the environment. But in a sample efficient regime the buffer may only hold 10 4 -10 5 transitions from a few hundred trajectories, limiting the effectiveness of SSL methods.A wide range of auxiliary loss functions have been proposed to augment supervised objectives, e.g. weight regularization, noise injection [25], or some form of online SSL objective. In RL, * Equal contribution. Author ordering determined by coin flip. Both authors are corresponding.Preprint. Under review.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on using image augmentation techniques to regularize deep reinforcement learning algorithms for training from pixel data without pre-training or auxiliary losses?"
  },
  {
    "index": 80,
    "source_corpus_id": 246996534,
    "ref_id": "b1",
    "citation_corpus_id": 52889459,
    "start": 2032,
    "end": 2051,
    "title": "LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS",
    "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by truncating the latent space. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128×128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.3 and Fréchet Inception Distance (FID) of 9.6, improving over the previous best IS of 52.52 and FID of 18.65.",
    "prev": "* Indicates equal contribution arXiv:2202.08937v2 [cs.LG]\n\nINTRODUCTION\n\nThese days, generative adversarial networks (GANs) (Goodfellow et al., 2014) can successfully approximate the high-dimensional distributions of real images.",
    "curr": "The exceptional quality of the state-ofthe-art GANs (Karras et al., 2020b;Brock et al., 2019) makes them a key ingredient in applications, including semantic editing (Isola et al., 2017;Zhu et al., 2018;Shen et al., 2020;Voynov & Babenko, 2020), image processing (Pan et al., 2020;Ledig et al., 2017;Menon et al., 2020), video generation (Wang et al., 2018a), producing high-quality synthetics (Zhang et al., 2021;Voynov et al., 2020).",
    "next": "To extend the success of GANs to the limited-data regime, it is common to use pretraining, i.e., to initialize the optimization process by the GAN checkpoint pretrained on some large dataset.",
    "query": "Are there any research papers on large-scale training of generative adversarial networks for high-fidelity natural image synthesis, including methods to improve sample fidelity and diversity through techniques like orthogonal regularization?"
  },
  {
    "index": 82,
    "source_corpus_id": 247411320,
    "ref_id": "b29",
    "citation_corpus_id": 6212000,
    "start": 1595,
    "end": 1614,
    "title": "UNDERSTANDING DEEP LEARNING REQUIRES RE- THINKING GENERALIZATION",
    "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.",
    "prev": "Our code is available at: https://github.com/MSU-MLSys-Lab/DeepAA.",
    "curr": "INTRODUCTION\n\n... Data augmentation (DA) is a powerful technique for machine learning since it effectively regularizes the model by increasing the number and the diversity of data points (Goodfellow et al., 2016;Zhang et al., 2017).",
    "next": "A large body of data augmentation transformations has been proposed (Inoue, 2018;Zhang et al., 2018;DeVries & Taylor, 2017;Yun et al., 2019;Hendrycks et al., 2020;Yan et al., 2020) to improve model performance.",
    "query": "Are there any research papers on the generalization of deep neural networks and the factors contributing to their performance despite overfitting, including theoretical analyses of their expressivity?"
  },
  {
    "index": 83,
    "source_corpus_id": 252682980,
    "ref_id": "b13",
    "citation_corpus_id": 28202810,
    "start": 17633,
    "end": 17656,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "Furthermore, plenty of practical algorithms leverage fitted-Q subroutines for updating the critic step (e.g.",
    "curr": "[Schulman et al., 2017, Haarnoja et al., 2018) with different differentiable function choices.",
    "next": "In addition, we also incorporate pessimism for the design.",
    "query": "Are there any research papers on off-policy maximum entropy deep reinforcement learning algorithms with a stochastic actor and techniques for improving stability and reducing sample complexity?"
  },
  {
    "index": 85,
    "source_corpus_id": 249210151,
    "ref_id": "b71",
    "citation_corpus_id": 238419650,
    "start": 2237,
    "end": 2256,
    "title": "GENERATIVE MODELING WITH OPTIMAL TRANSPORT MAPS",
    "abstract": "With the discovery of Wasserstein GANs, Optimal Transport (OT) has become a powerful tool for large-scale generative modeling tasks. In these tasks, OT cost is typically used as the loss for training GANs. In contrast to this approach, we show that the OT map itself can be used as a generative model, providing comparable performance. Previous analogous approaches consider OT maps as generative models only in the latent spaces due to their poor performance in the original high-dimensional ambient space. In contrast, we apply OT maps directly in the ambient space, e.g., a space of high-dimensional images. First, we derive a minmax optimization algorithm to efficiently compute OT maps for the quadratic cost (Wasserstein-2 distance). Next, we extend the approach to the case when the input and output distributions are located in the spaces of different dimensions and derive error bounds for the computed OT map. We evaluate the algorithm on image generation and unpaired image restoration tasks. In particular, we consider denoising, colorization, and inpainting, where the optimality of the restoration map is a desired attribute, since the output (restored) image is expected to be close to the input (degraded) one.Published as a conference paper at ICLR 2022 (a) OT cost as the loss for the generative model. (b) OT map as the generative model. DC Dowson and BV Landau. The fréchet distance between multivariate normal distributions. Journal of multivariate analysis, 12(3):450-455, 1982.Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models. arXiv preprint arXiv:1903.08689, 2019.",
    "prev": "Optimal transport (OT) is a powerful framework to solve mass-moving problems for data distributions which finds many applications in machine learning and computer vision (Bonneel & Digne, 2023).Most existing methods to compute OT plans are designed for discrete distributions (Flamary et al., 2021;Peyré et al., 2019;Cuturi, 2013).These methods have good flexibility: they allow to control the properties of the plan via choosing the cost function.However, discrete methods find an optimal matching between two given (train) sets which does not generalize to new (test) data points.This limits the applications of discrete OT plan methods to scenarios when one needs to generate new data, e.g., image-to-image transfer (Zhu et al., 2017).",
    "curr": "Recent works (Rout et al., 2022;Korotin et al., 2023b;2021b;Fan et al., 2021a;Daniels et al., 2021) propose continuous methods to compute OT plans.Thanks to employing neural networks to parameterize OT solutions, the learned transport plan can be used directly as the generative model in data synthesis (Rout et al., 2022) and unpaired learning (Korotin et al., 2023b;Rout et al., 2022;Daniels et al., 2021;Gazdieva et al., 2022).",
    "next": "Existing continuous OT methods mostly focus on classic cost functions such as ℓ 2 (Korotin et al., 2021b;2023b;Fan et al., 2021a;Gazdieva et al., 2022) which estimate the closeness of input and 1 BACKGROUND AND NOTATIONS\n\nIn this section, we provide key concepts of the optimal transport theory.Throughout the paper, we consider compact X = Y ⊂ R D and P, Q ∈ P(X ), P(Y).",
    "query": "Are there any research papers on generative modeling using optimal transport maps in high-dimensional ambient spaces, such as image synthesis and restoration, with methods that directly apply OT maps as generative models rather than using OT cost as a loss function?"
  },
  {
    "index": 86,
    "source_corpus_id": 252668614,
    "ref_id": "b4",
    "citation_corpus_id": 10550488,
    "start": 7484,
    "end": 7504,
    "title": "Findings of the 2015 Workshop on Statistical Machine Translation",
    "abstract": "This paper presents the results of the WMT15 shared tasks, which included a standard news translation task, a metrics task, a tuning task, a task for run-time estimation of machine translation quality, and an automatic post-editing task. This year, 68 machine translation systems from 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic postediting task had a total of 4 teams, submitting 7 entries. ID Institution AALTO Aalto University (Grönroos et al., 2015) ABUMATRAN Abu-MaTran (Rubino et al., 2015) AFRL-MIT-* Air Force Research Laboratory / MIT Lincoln Lab (Gwinnup et al., 2015) CHALMERS Chalmers University of Technology (Kolachina and Ranta, 2015) CIMS University of Stuttgart and Munich (Cap et al., 2015) CMU Carnegie Mellon University CU-CHIMERA Charles University (Bojar and Tamchyna, 2015) CU-TECTO Charles University (Dušek et al., 2015) DFKI Deutsches Forschungszentrum für Künstliche Intelligenz (Avramidis et al., 2015) ILLINOIS University of Illinois (Schwartz et al., 2015) IMS University of Stuttgart (Quernheim, 2015) KIT Karsruhe Institut of Technology (Cho et al., 2015) KIT-LIMSI Karsruhe Institut of Technology / LIMSI (Ha et al., 2015) LIMSI LIMSI (Marie et al., 2015) MACAU University of Macau MONTREAL University of Montreal (Jean et al., 2015) PROMT ProMT RWTH RWTH Aachen (Peter et al., 2015)",
    "prev": "Furthermore, perplexity assigns cnn dailymail even lower scores than the in-domain xsum.",
    "curr": "For translation, the model is trained on WMT15 dataset and evaluated on other WMT test splits (Bojar et al., 2015), OPUS100 (Aulamo & Tiedemann, 2019), and MTNT (Michel & Neubig, 2018).",
    "next": "The in-domain and OOD datasets perplexity densities overlap even more.",
    "query": "Are there any research papers on evaluating machine translation models using perplexity metrics across in-domain and out-of-domain datasets such as WMT15, OPUS100, and MTNT?"
  },
  {
    "index": 87,
    "source_corpus_id": 21946795,
    "ref_id": "b23",
    "citation_corpus_id": 9059612,
    "start": 3552,
    "end": 3575,
    "title": "ADVERSARIAL MACHINE LEARNING AT SCALE",
    "abstract": "Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet (Russakovsky  et al., 2014). Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than singlestep attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a \"label leaking\" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.",
    "prev": "showed that adversarially trained models can be made robust to white-box attacks (i.e., with knowledge of the model parameters) if the perturbations computed during training closely maximize the model's loss.",
    "curr": "However, prior attempts at scaling this approach to ImageNet-scale tasks (Deng et al., 2009) have proven unsuccessful (Kurakin et al., 2017b).",
    "next": "It is thus natural to ask whether it is possible, at scale, to achieve robustness against the class of black-box adversaries Towards this goal, Kurakin et al.",
    "query": "Are there any research papers on scaling adversarial training techniques to large-scale machine learning tasks like ImageNet and improving robustness against black-box adversaries?"
  },
  {
    "index": 88,
    "source_corpus_id": 219636462,
    "ref_id": "b14",
    "citation_corpus_id": 3693512,
    "start": 2190,
    "end": 2194,
    "title": "LIFELONG LEARNING WITH DYNAMICALLY EXPAND- ABLE NETWORKS",
    "abstract": "We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets in lifelong learning scenarios on multiple public datasets, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch model with substantially fewer number of parameters.",
    "prev": "At the heart of catastrophic forgetting is the stability-plasticity dilemma [2,3], where a model exhibits high stability on previously trained tasks, but suffers from low plasticity for the integration of new knowledge (and vice-versa).",
    "curr": "Attempts to overcome this challenge in neural network-based CL can be grouped into three main strategies: regularization methods [4][5][6][7][8][9], memory replay [10][11][12][13], and dynamic network architecture [14][15][16].",
    "next": "In particular, regularization methods that control model weights bear the longest history due to its simplicity and efficiency to control the trade-off for a fixed model capacity.",
    "query": "Are there any studies that explore dynamic network architectures for lifelong learning to mitigate catastrophic forgetting through expanding network capacity?"
  },
  {
    "index": 90,
    "source_corpus_id": 252907593,
    "ref_id": "b14",
    "citation_corpus_id": 52967399,
    "start": 2581,
    "end": 2602,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Such coordinated representations are particularly useful for tasks such as cross-modal retrieval.",
    "curr": "Another vetted commonly used self-supervised learning framework is Masked Data Modeling (MDM), which learns a meaningful representation with the pretext task of recovering the original inputs or features from the corrupted ones (Devlin et al., 2019).",
    "next": "Particularly, based on the Audio Spectrogram Transformer (Gong et al., 2021a) and Vision Transformer (Dosovitskiy et al., 2020) backbones, the single-modal Masked Auto-Encoder (MAE)  achieved state-of-the-art (SOTA) performance on images and audio tasks (Huang et al., 2022a) individually.",
    "query": "Are there any research papers on self-supervised learning frameworks for cross-modal retrieval using masked auto-encoders and transformer-based architectures in vision and audio tasks?"
  },
  {
    "index": 91,
    "source_corpus_id": 203593909,
    "ref_id": "b17",
    "citation_corpus_id": 10480989,
    "start": 2616,
    "end": 2638,
    "title": "Language as a Latent Variable: Discrete Generative Models for Sentence Compression",
    "abstract": "In this work we explore deep generative models of text in which the latent representation of a document is itself drawn from a discrete language model distribution. We formulate a variational auto-encoder for inference in this model and apply it to the task of compressing sentences. In this application the generative model first draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary. In our empirical evaluation we show that generative formulations of both abstractive and extractive compression yield state-of-the-art results when trained on a large amount of supervised data. Further, we explore semi-supervised compression scenarios where we show that it is possible to achieve performance competitive with previously proposed supervised models while training on a fraction of the supervised data.",
    "prev": "In the context of sequence generation problems, semi-supervised approaches have been shown to work well in some cases.",
    "curr": "For example, back-translation (Sennrich et al., 2015) makes use of the monolingual data on the target side to improve machine translation systems, latent variable models  are employed to incorporate unlabeled source data to facilitate sentence compression (Miao & Blunsom, 2016) or code generation (Yin et al., 2018).",
    "next": "In this work, we revisit a much older and simpler semi-supervised method, self-training (ST, Scudder (1965)), where a base model trained with labeled data acts as a \"teacher\" to label the unannotated data, which is then used to augment the original small training set.",
    "query": "Are there any research papers on semi-supervised generative models for sentence compression using latent variables or self-training methods?"
  },
  {
    "index": 92,
    "source_corpus_id": 219708742,
    "ref_id": "b2",
    "citation_corpus_id": 52967399,
    "start": 6440,
    "end": 6461,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": " activation functions other than ReLU have appeared in the literature.Johnson (2019) shows that if the activation function ρ is uniformly continuous and can be uniformly approximated by a sequence of one-to-one functions, a width-d x network cannot universally approximate C(K, R).Kidger and Lyons (2020) show that if ρ is continuous, nonpolynomial, and continuously differentiable at at least one point (say z) with ρ (z) = 0, then networks of width d x + d y + 1 with activation ρ are dense in C(K, R dy ).Furthermore, Kidger and Lyons (2020) prove that ReLU networks of width d x + d y + 1 are dense in L p (R dx , R dy ).",
    "curr": "Limitations of prior arts.Note that none of the existing works succeeds in closing the gap between the upper bound (at least d x + d y ) and the lower bound (at most d x + 1).This gap is significant especially for applications with high-dimensional codomains (i.e., large d y ) such as image generation (Kingma and Welling, 2013;Goodfellow et al., 2014), language modeling (Devlin et al., 2019;Liu et al., 2019), and molecule generation (Gómez-Bombarelli et al., 2018;Jin et al., 2018).In the prior arts, the main bottleneck for proving an upper bound below d x + d y is that they maintain all d x neurons to store the input and all d y neurons to construct the function output; this means every layer already requires at least d x + d y neurons.In addition, the proof techniques for the lower bounds only consider the input dimension d x regardless of the output dimension d y .",
    "next": "Summary of results\n\nWe mainly focus on characterizing the minimum width of ReLU networks for universal approximation.Nevertheless, our results are not restricted to ReLU networks; they can be generalized to networks with general activation functions.Our contributions can be summarized as follows.",
    "query": "Are there any studies on the minimum width requirements for ReLU networks in universal approximation, particularly addressing the gap between upper and lower bounds in high-dimensional applications like image generation and language modeling?"
  },
  {
    "index": 93,
    "source_corpus_id": 220768638,
    "ref_id": "b8",
    "citation_corpus_id": 57189211,
    "start": 5159,
    "end": 5162,
    "title": "REASONING ABOUT PHYSICAL INTERACTIONS WITH OBJECT-ORIENTED PREDICTION AND PLANNING",
    "abstract": "Object-based factorizations provide a useful level of abstraction for interacting with the world. Building explicit object representations, however, often requires supervisory signals that are difficult to obtain in practice. We present a paradigm for learning object-centric representations for physical scene understanding without direct supervision of object properties. Our model, Object-Oriented Prediction and Planning (O2P2), jointly learns a perception function to map from image observations to object representations, a pairwise physics interaction function to predict the time evolution of a collection of objects, and a rendering function to map objects back to pixels. For evaluation, we consider not only the accuracy of the physical predictions of the model, but also its utility for downstream tasks that require an actionable representation of intuitive physics. After training our model on an image prediction task, we can use its learned representations to build block towers more complicated than those observed during training.",
    "prev": "Developing a factorized scene representation has been a core research topic in computer vision for decades.",
    "curr": "Most learning-based prior works are supervised, requiring annotated specification such as segmentations [9], patches [5], or simulation engines [27,10].",
    "next": "These supervised approaches face two challenges.",
    "query": "Are there any studies on unsupervised or self-supervised methods for learning object-centric representations in physical scene understanding?"
  },
  {
    "index": 94,
    "source_corpus_id": 84186721,
    "ref_id": "b1",
    "citation_corpus_id": 5590763,
    "start": 33895,
    "end": 33913,
    "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "prev": "At the first time step these messages, m\n\nv , are initialized with the respective atom features shown in Table 4.",
    "curr": "GGNNs then update these messages in a recursive nature:\nm (s) v = GRU   m (s−1) v , i∈Ne1(v) f single m (s−1) i + j∈Ne2(v) f double m (s−1) j + k∈Ne3(v) f triple m (s−1) k  (7)\nWhere GRU is a Gated Recurrent Unit (Cho et al., 2014), the functions N e1 (v), N e2 (v), N e3 (v) index the nodes connected by single, double and triple bonds to node v respectively and f single (·), f double (·) and f triple (·) are linear transformations with learnable parameters.",
    "next": "This process continues for S steps (where we choose S = 4).",
    "query": "Are there any research papers on message-passing neural networks with recursive GRU updates for molecular or graph-based representation learning?"
  },
  {
    "index": 95,
    "source_corpus_id": 254535921,
    "ref_id": "b7",
    "citation_corpus_id": 231627730,
    "start": 8406,
    "end": 8424,
    "title": "Preprint, under review. RANDOMIZED ENSEMBLED DOUBLE Q-LEARNING: LEARNING FAST WITHOUT A MODEL",
    "abstract": "Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple modelfree algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art modelbased algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio 1; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio 1. * Equal contribution, in alphabetical order. † Correspondence to: Keith Ross <keithwross@nyu.edu>. 1 arXiv:2101.05982v1 [cs.LG] 15 Jan 2021 Preprint, under review.zero for most of training, even when the UTD is very high. Furthermore, by adjusting the number of randomly selected Q-functions for in-target minimization, REDQ can control the average Q-function bias. In comparison with standard ensemble averaging and with SAC with a higher UTD, REDQ has much lower std of Q-function bias while maintaining an average bias that is negative but close to zero throughout most of training, resulting in significantly better learning performance. We perform an ablation study, and show that REDQ is very robust to choices of hyperparameters, and can work well with a small ensemble and a small number of Q functions in the in-target minimization. We also provide a theoretical analysis, providing additional insights into REDQ. Finally, we consider combining the REDQ algorithm with an online feature extractor network (OFENet) (Ota et al.,  2020)  to further improve performance, particularly for the more challenging environments Ant and Humanoid. We achieve more than 7x the sample efficiency of SAC to reach a score of 5000 for both Ant and Humanoid. In Humanoid, REDQ-OFE also greatly outperforms MBPO, reaching a score of 5000 at 150K interactions, which is 3x MBPO's score at that point.To ensure our comparisons are fair, and to ensure our results are reproducible(Henderson et al., 2018;Islam et al., 2017;Duan et al., 2016), we provide open source code. For all algorithmic comparisons, we use the same codebase (except for MBPO, for which we use the authors' code).RANDOMIZED ENSEMBLED DOUBLE Q-LEARNING (REDQ)Janner et al.(2019)proposed Model-Based Policy Optimization (MBPO), which was shown to be much more sample efficient than popular model-free algorithms such as SAC and PPO for the MuJoCo environments. MBPO learns a model, and generates \"fake data\" from its model as well as \"real data\" through environment interactions. It then performs parameter updates using both the fake and the real data. One of the distinguishing features of MBPO is that it has a UTD ratio 1 for updating its Q functions, enabling MBPO to achieve high sample efficiency.",
    "prev": "icitly using regularization on the learned Q-values (Kumar et al., 2020).The limitation of existing offline RL approaches is that the derived policies can only act under a fixed degree of conservatism, which is determined by an opaque hyperparameter that scales the estimated epistemic uncertainty, and has to be chosen during offline training.This means the policies will be unable to correct their behavior online, even if it becomes evident from online observations that the estimated value function is too pessimistic or optimistic.",
    "curr": "Our algorithm learns confidence-conditioned Q-values that capture all possible degrees of pessimism by conditioning on the confidence level, modeling epistemic uncertainty as a function of confidence.By doing so, instead of committing to one degree of pessimism, we enable policies that adapt how conservative they should behave using the observations they sees during online evaluation.Our approach is related to ensemble (Agarwal et al., 2020;Lee et al., 2021;Chen et al., 2021;An et al., 2021) approaches in that they also predict multiple Q-values to model epistemic uncertainty.However, existing ensemble methods train individual Q-values on the same objective, and rely on different parameter initializations.In contrast, each of our Q-values captures a different confidence-level.In addition, standard ensemble approaches do not consider adaptive policies.Recently, APE-V proposes using ensembles to learn adaptive policies that condition on belief over which value function is most accurate (Ghosh et al., 2022).Our approach considers a similar strategy for adaptation, but explicitly parameterizes the value function by the confidence level, introducing a novel training objective for this purpose.In our experiments, we compare to a method that adapts APE-V to our discrete-action benchmark tasks.Jiang & Huang (2020); Dai et al.",
    "next": "(2020) propose confidence intervals for policy evaluation at specified confidence-levels.We aim to learn a value function across all confide",
    "query": "Are there any research papers on model-free reinforcement learning algorithms that use ensembled Q-values, in-target minimization, and high UTD ratios to improve sample efficiency in continuous-action environments?"
  },
  {
    "index": 97,
    "source_corpus_id": 1880070,
    "ref_id": "b33",
    "citation_corpus_id": 780171,
    "start": 6329,
    "end": 6350,
    "title": "Data-Driven Response Generation in Social Media",
    "abstract": "We present a data-driven approach to generating responses to Twitter status posts, based on phrase-based Statistical Machine Translation. We find that mapping conversational stimuli onto responses is more difficult than translating between languages, due to the wider range of possible responses, the larger fraction of unaligned words/phrases, and the presence of large phrase pairs whose alignment cannot be further decomposed. After addressing these challenges, we compare approaches based on SMT and Information Retrieval in a human evaluation. We show that SMT outperforms IR on this task, and its output is preferred over actual human responses in 15% of cases. As far as we are aware, this is the first work to investigate the use of phrase-based SMT to directly translate a linguistic stimulus into an appropriate response.",
    "prev": "There are many obvious cases where these metrics fail, as they are often incapable of considering the semantic similarity between responses (see Figure 1).",
    "curr": "Despite this, many researchers still use BLEU to evaluate their dialogue models (Ritter et al., 2011;Sordoni et al., 2015b;Li et al., 2015;Galley et al., 2015;Li et al., 2016a), as there are few alternatives available that correlate with human judgements.",
    "next": "While human evaluation should always be used to evaluate dialogue models, it is often too expensive and time-consuming to do this for every model specification (for example, for every combination of model hyperparameters).",
    "query": "Are there any studies that examine the limitations of BLEU in evaluating dialogue models and explore alternative metrics that better capture semantic similarity between responses?"
  },
  {
    "index": 98,
    "source_corpus_id": 263829563,
    "ref_id": "b2",
    "citation_corpus_id": 237561567,
    "start": 7856,
    "end": 7873,
    "title": "Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction",
    "abstract": "An essential operation in web corpus construction consists in retaining the desired content while discarding the rest. Another challenge finding one's way through websites. This article introduces a text discovery and extraction tool published under open-source license. Its installation and use is straightforward, notably from Python and on the command-line. The software allows for main text, comments and metadata extraction, while also providing building blocks for web crawling tasks. A comparative evaluation on real-world data also shows its interest as well as the performance of other available solutions.The contributions of this paper are threefold: it references the software, features a benchmark, and provides a meaningful baseline for similar tasks. The tool performs significantly better than other open-source solutions in this evaluation and in external benchmarks.",
    "prev": "he Pile (Gao et al., 2020), and GPT-3 (Brown et al., 2020) introduce various pipelines for extracting quality data from Common Crawl for the purposes of language model training.These pipelines typically consist of three primary steps: text extraction, filtering, and deduplication.",
    "curr": "Text extraction\n\nExtracting plain text from HTML files is a critical step in the creation of Common Crawl-based datasets.The easiest way to extract text from Common Crawl documents is to use the WET corresponding to each webpage, which contains pre-extracted plain text of the webpage.CCNet and C4 both use Common Crawl's WET files.However, the text extracted in WET files may contain too much boilerplate or miss out on important content such as L A T E X equations.It is also possible to extract text directly from the raw HTML found in Common Crawl WARC files.The Pile uses an open source library called jusText (Endrédy & Novák, 2013) to extract text from HTML while RefinedWeb uses a library called Trafilatura (Barbaresi, 2021).These text extraction approaches differ in terms of extraction speed, customization, and their precision and recall for removing boilerplate content.",
    "next": "Filtering The first layer of filtering often involves language identification (Wenzek et al., 2019).Language filtering is used because certain other parts of the pipeline only work for specific languages, and is often done with simple linear classifiers such as from fastText (Joulin et al., 2016).Quality filtering can be done with a combination of perplexity, classifier, and rule-based methods.CCNet uses a 5-gram Kneser-Ney language model implemented in the KenLM library (Heafield, 2011) trained on the target domain.The documents in the dataset are then sorted and filtered by their perplexity under this model.Other datasets such as the one used to train GPT-3 (Brown et al., 2020) use a classifier-based approach.This involves training a classifier on known-high-quality documents, such as those from Wikipedia, as positive examples and unf",
    "query": "Are there any studies that evaluate and compare tools or methods for text extraction from web data, particularly focusing on handling boilerplate content and improving extraction accuracy in Common Crawl-based datasets?"
  },
  {
    "index": 100,
    "source_corpus_id": 261696510,
    "ref_id": "b9",
    "citation_corpus_id": 215745286,
    "start": 3723,
    "end": 3744,
    "title": "Unsupervised Commonsense Question Answering with Self-Talk",
    "abstract": "Natural language understanding involves reading between the lines with implicit background knowledge. Current systems either rely on pretrained language models as the sole implicit source of world knowledge, or resort to external knowledge bases (KBs) to incorporate additional relevant knowledge. We propose an unsupervised framework based on self-talk as a novel alternative to multiple-choice commonsense tasks. Inspired by inquiry-based discovery learning (Bruner, 1961), our approach inquires language models with a number of information seeking questions such as \"what is the definition of ...\" to discover additional background knowledge. Empirical results demonstrate that the self-talk procedure substantially improves the performance of zero-shot language model baselines on four out of six commonsense benchmarks, and competes with models that obtain knowledge from external KBs. While our approach improves performance on several benchmarks, the self-talk induced knowledge even when leading to correct answers is not always seen as useful by human judges, raising interesting questions about the inner-workings of pre-trained language models for commonsense reasoning.",
    "prev": "Select Implement def transform_grid(grid): out_grid = np.zeros_like(grid) for col in range(grid.shape[1]): non_zeros = \\ grid[:, col][grid[:, col] != 0] if len(non_zeros) > 0: out_grid[-len(non_zeros):,col]= \\ on_zeros return out_grid def transform_grid(grid): return ...\n\nINTRODUCTION\n\nInductive reasoning -the ability to infer general principles from specific examples and apply them to novel situations -is a core aspect of human intelligence (Peirce, 1868).",
    "curr": "Recently, large-scale pre-trained language models have received significant interest for their performance across a diverse range of reasoning tasks such as commonsense, arithmetic and symbolic reasoning (Rajani et al., 2019;Shwartz et al., 2020;Nye et al., 2021;Wei et al., 2022;Marasović et al., 2021;Lampinen et al., 2022;Zelikman et al., 2022;Zhou et al., 2022).",
    "next": "There has been extensive discussion of language models' impressive \"in-context learning\" capabilities, a form of inductive reasoning.",
    "query": "Are there any studies that explore methods for improving commonsense reasoning in language models through self-talk or in-context learning strategies?"
  },
  {
    "index": 101,
    "source_corpus_id": 252367996,
    "ref_id": "b21",
    "citation_corpus_id": 3488815,
    "start": 2196,
    "end": 2216,
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. * Authors ordered alphabetically.",
    "prev": "In\n\nINTRODUCTION\n\nAs machine learning models are increasingly deployed in security or safety-critical settings, robustness becomes an essential property.",
    "curr": "Adversarial training (Madry et al., 2018) is the state-of-the-art method for improving the adversarial robustness of deep neural networks.",
    "next": "Recent work has made substantial progress in robustness by scaling adversarial training to very large datasets.",
    "query": "Are there any research papers on methods for improving adversarial robustness in deep learning models through adversarial training or robust optimization techniques?"
  },
  {
    "index": 103,
    "source_corpus_id": 222133031,
    "ref_id": "b9",
    "citation_corpus_id": 6628106,
    "start": 5267,
    "end": 5288,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "This class encompasses many known schemes such as gradient descent with momentum, or full-matrix AdaGrad.",
    "curr": "However, methods such as Adam [Kingma and Ba, 2015] or diagonal AdaGrad [Duchi et al., 2011] are not in this class, as the diagonal re-scaling creates iterates x t outside the span of previous gradients.",
    "next": "Although we will focus on the distance to the solution, the results can be extended to other convergence criteria such as F (x t ) 2 .",
    "query": "Are there any research papers on adaptive stochastic optimization algorithms like Adam, focusing on their convergence properties and differences from methods such as AdaGrad?"
  },
  {
    "index": 104,
    "source_corpus_id": 247451000,
    "ref_id": "b6",
    "citation_corpus_id": 231662264,
    "start": 27174,
    "end": 27194,
    "title": "Published as a conference paper at ICLR 2021 CHARACTERIZING SIGNAL PROPAGATION TO CLOSE THE PERFORMANCE GAP IN UNNORMALIZED RESNETS",
    "abstract": "Batch Normalization is a key component in almost all state-of-the-art image classifiers, but it also introduces practical challenges: it breaks the independence between training examples within a batch, can incur compute and memory overhead, and often results in unexpected bugs. Building on recent theoretical analyses of deep ResNets at initialization, we propose a simple set of analysis tools to characterize signal propagation on the forward pass, and leverage these tools to design highly performant ResNets without activation normalization layers. Crucial to our success is an adapted version of the recently proposed Weight Standardization. Our analysis tools show how this technique preserves the signal in networks with ReLU or Swish activation functions by ensuring that the per-channel activation means do not grow with depth. Across a range of FLOP budgets, our networks attain performance competitive with the state-of-the-art EfficientNets on ImageNet.Published as a conference paper at ICLR 2021 works. Leveraging these SPPs, we show how to design unnormalized ResNets which are constrained to have signal propagation properties similar to batch-normalized ResNets.• We identify a key failure mode in unnormalized ResNets with ReLU or Swish activations and Gaussian weights. Because the mean output of these non-linearities is positive, the squared mean of the hidden activations on each channel grows rapidly as the network depth increases. To resolve this, we propose Scaled Weight Standardization, a minor modification of the recently proposed Weight Standardization (Qiao et al., 2019; Huang et al., 2017b), which prevents the growth in the mean signal, leading to a substantial boost in performance.• We apply our normalization-free network structure in conjunction with Scaled Weight Standardization to ResNets on ImageNet, where we for the first time attain performance which is comparable or better than batch-normalized ResNets on networks as deep as 288 layers.• Finally, we apply our normalization-free approach to the RegNet architecture (Radosavovic  et al., 2020). By combining this architecture with the compound scaling strategy proposed byTan& Le (2019), we develop a class of models without normalization layers which are competitive with the current ImageNet state of the art across a range of FLOP budgets. . Shufflenet v2: Practical guidelines for efficient cnn architecture design. In . Spectral normalization for generative adversarial networks. In ICLR, 2018. Y. Nesterov. A method for unconstrained convex minimization problem with the rate of convergence O(1/k 2 ). Doklady AN USSR, pp. (269), 543-547, 1983. Art B Owen. A robust hybrid of lasso and ridge regression. 2007. . ImageNet large scale visual recognition challenge. IJCV, 115:211-252, 2015. Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in neural information processing systems, pp. 901-909, 2016.",
    "prev": "This suggests that one can compensate for the removal of BN layers, at least in terms of their effect on the behaviour of the network at initialization time, by down-scaling the residual branch of each residual block.",
    "curr": "Arguably, almost all recent work on training deep networks without normalization layers (Zhang et al., 2018;Shao et al., 2020;Bachlechner et al., 2020;Brock et al., 2021a;b) has adopted this idea by introducing multipliers on the residual branches (which may or may not be optimized during training).",
    "next": "In Table 2, we show that one can close most of the gap with standard ResNets by simply adopting the modification in equation 6 without using BN layers.",
    "query": "Are there any research papers on methods to improve signal propagation in unnormalized ResNets without batch normalization layers, using techniques like scaled weight standardization or residual branch modifications?"
  },
  {
    "index": 105,
    "source_corpus_id": 263909429,
    "ref_id": "b42",
    "citation_corpus_id": 257279944,
    "start": 2561,
    "end": 2581,
    "title": "Human Motion Diffusion as a Generative Prior",
    "abstract": "A Capoeira practice. One is kicking and the other is avoiding the kick.\"Recent work has demonstrated the significant potential of denoising diffusion models for generating human motion, including text-to-motion capabilities. However, these methods are restricted by the paucity of annotated motion data, a focus on single-person motions, and a lack of detailed control. In this paper, we introduce three forms of composition based on diffusion priors: sequential, parallel, and model composition. Using sequential composition, we tackle the challenge of long sequence generation. We introduce DoubleTake, an inference-time method with which we generate long animations consisting of sequences of prompted intervals and their transitions, using a prior trained only for short clips. Using parallel composition, we show promising steps toward two-person generation. Beginning with two fixed priors as well as a few two-person training examples, we learn a slim communication block, ComMDM, to coordinate interaction between the two resulting motions. Lastly, using model composition, we first train individual priors to complete motions that realize a prescribed motion for a given joint. We then introduce DiffusionBlending, an interpolation mechanism to effectively blend several such models to enable flexible and efficient fine-grained joint and trajectory-level control and editing. We evaluate the composition methods using an off-the-shelf motion diffusion model, and further compare the results to dedicated models trained for these specific tasks. https://priormdm.github.io/priorMDM-page/ 1",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on methods for human motion generation using diffusion models, with a focus on long sequence generation, multi-person interaction, and fine-grained control mechanisms?"
  },
  {
    "index": 106,
    "source_corpus_id": 210920362,
    "ref_id": "b43",
    "citation_corpus_id": 2187805,
    "start": 34075,
    "end": 34094,
    "title": "A NOTE ON THE EVALUATION OF GENERATIVE MODELS",
    "abstract": "Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria-average log-likelihood, Parzen window estimates, and visual fidelity of samples-are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any studies that examine the challenges and criteria for evaluating generative models, particularly the independence of metrics like average log-likelihood and visual fidelity in high-dimensional data?"
  },
  {
    "index": 107,
    "source_corpus_id": 264555202,
    "ref_id": "b28",
    "citation_corpus_id": 52115700,
    "start": 30655,
    "end": 30680,
    "title": "Evaluating Theory of Mind in Question Answering",
    "abstract": "We propose a new dataset for evaluating question answering models with respect to their capacity to reason about beliefs. Our tasks are inspired by theory-of-mind experiments that examine whether children are able to reason about the beliefs of others, in particular when those beliefs differ from reality. We evaluate a number of recent neural models with memory augmentation. We find that all fail on our tasks, which require keeping track of inconsistent states of the world; moreover, the models' accuracy decreases notably when random sentences are introduced to the tasks at test. 1",
    "prev": "All these works, however, focus on protecting training data, without considering context, and rely heavily on having a well-defined notion of a single record.While this is ideal for tabular data, it is extremely Preprint   hard to define for language, as drawing borders around a unit of language that needs protection is not always feasible (Brown et al., 2022) and different units might need different levels of protection, based on information type and context.Our work, however, differs from existing literature in two main aspects: (1) we focus on the impact that context has on privacy, and how reasoning about this context is crucial in making judgments when it comes to language, and (2) we shift attention away from training data and towards interactions with the model, as providing lengthy history for the model is becoming more and more relevant.",
    "curr": "Theory of Mind (ToM) and LLMs\n\nThe development of ToM abilities has been a long-standing goal in AI research (Nematzadeh et al., 2018;Le et al., 2019;Sap et al., 2019;Shapira et al., 2023b;Kim et al., 2023).Although qualitative assessments might imply a degree of ToM in LLMs (Whang, 2023), more comprehensive quantitative investigations reveal that LLMs still struggle to reason ToM robustly (Sap et al., 2022;Shapira et al., 2023a;Ullman, 2023;Kim et al., 2023).This might account for the poor performance of LLMs on our benchmark.",
    "next": "Ethics and morality for LLMs Revealing secrets often involves making moral decisions in the real world.Many previous works focus on inferring the morality of the behavior based on textual descriptions Preprint of scenarios (Jiang et al., 2021;Zhou et al., 2023a;Forbes et al., 2020), while more works start to integrate social contexts into the machine morality discourse (Kim et al., 2022b;Pyatkin et al., 2023;Jin et al., 2022).",
    "query": "Are there any research papers on evaluating theory of mind in large language models for question answering tasks and their challenges in reasoning about beliefs?"
  },
  {
    "index": 108,
    "source_corpus_id": 245906072,
    "ref_id": "b17",
    "citation_corpus_id": 6212000,
    "start": 2118,
    "end": 2138,
    "title": "UNDERSTANDING DEEP LEARNING REQUIRES RE- THINKING GENERALIZATION",
    "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.",
    "prev": "This is surprising from an optimization point-of-view because the ERM problem induced by neural networks is nonconvex (Sontag & Sussmann, 1989, 1991 and can even be NP-Complete in certain cases (Blum & Rivest, 1993).",
    "curr": "Perhaps even more surprising is that the discovered solution can generalize even when the network is able to fit arbitrary labels (Zhang et al., 2017), rendering traditional complexity measures such as Rademacher complexity inadequate.",
    "next": "How does deep learning succeed in the face of pathological behavior by the standards of classical optimization and statistical learning theory?",
    "query": "Are there any research papers on the generalization properties of deep learning models in the face of non-convex optimization challenges and the limitations of traditional statistical learning theory?"
  },
  {
    "index": 110,
    "source_corpus_id": 263835059,
    "ref_id": "b22",
    "citation_corpus_id": 231934149,
    "start": 5608,
    "end": 5612,
    "title": "TOPOLOGICAL GRAPH NEURAL NETWORKS",
    "abstract": "Graph neural networks (GNNs) are a powerful architecture for tackling graph learning tasks, yet have been shown to be oblivious to eminent substructures such as cycles. We present TOGL, a novel layer that incorporates global topological information of a graph using persistent homology. TOGL can be easily integrated into any type of GNN and is strictly more expressive (in terms the Weisfeiler-Lehman graph isomorphism test) than message-passing GNNs. Augmenting GNNs with TOGL leads to improved predictive performance for graph and node classification tasks, both on synthetic data sets, which can be classified by humans using their topology but not by ordinary GNNs, and on real-world data.",
    "prev": "ts variants [24,26] still being used as static feature descriptors that require domain-specific hyperparameter choices.By contrast, our approach makes the ECT end-to-end trainable, resulting in an efficient and effective shape descriptor that can be integrated into deep learning models.Subsequently, we demonstrate such integrations both on the level of loss terms as well as on the level of novel computational layers.",
    "curr": "In a machine learning context, the choice of model is typically dictated by the type of data.For point clouds, a recent survey [14] outlines a plethora of models for point cloud analysis tasks like classification, many of them being based on learning equivariant functions [41].When additional structure is being present in the form of graphs or meshes, graph neural networks (GNNs) are typically employed for classification tasks [42], with some methods being capable to either learn explicitly on such higher-order domains [3,4,10,15,16] or harness their topological features [23,32].",
    "next": "Mathematical Background\n\nPrior to discussing our method and its implementation, we provide a self-contained description to the Euler Characteristic Transform (ECT).The ECT is often relying on simplicial complexes, the central building blocks in algebraic topology, which are extensively used for calculating homology groups and proving a variety of properties of topological spaces.While numerous variants of simplicial complexes exist, we will focus on those that are embedded in R n .Generally, simplicial complexes are obtained from on a set of points, to which higher-order elements-simplices-such as Figure 1: We construct a simplicial from an image of the MNIST data set (using a Delaunay complex construction on the non-zero pixels).For each choice of direction on S 1 , we obtain a Euler Characteristic Curve.The collection of all these curves constitutes the Euler Characteristic Transform.Existing work typically concatenates all these curves to obtain a static feature vector, where",
    "query": "Are there any studies that explore integrating topological methods, such as persistent homology, into graph neural networks to enhance their ability to capture structural features for classification tasks?"
  },
  {
    "index": 111,
    "source_corpus_id": 256808748,
    "ref_id": "b26",
    "citation_corpus_id": 210064473,
    "start": 2313,
    "end": 2330,
    "title": "SPACE: UNSUPERVISED OBJECT-ORIENTED SCENE REPRESENTATION VIA SPATIAL ATTENTION AND DECOMPOSITION",
    "abstract": "The ability to decompose complex multi-object scenes into meaningful abstractions like objects is fundamental to achieve higher-level cognition. Previous approaches for unsupervised object-oriented scene representation learning are either based on spatial-attention or scene-mixture approaches and limited in scalability which is a main obstacle towards modeling real-world scenes. In this paper, we propose a generative latent variable model, called SPACE, that provides a unified probabilistic modeling framework that combines the best of spatial-attention and scene-mixture approaches. SPACE can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. Previous models are good at either of these, but not both. SPACE also resolves the scalability problems of previous methods by incorporating parallel spatial-attention and thus is applicable to scenes with a large number of objects without performance degradations. We show through experiments on Atari and 3D-Rooms that SPACE achieves the above properties consistently in comparison to SPAIR, IODINE, and GENESIS. Results of our experiments can be found on our project website: https://sites.google.com/view/space-project-page * Visiting Student at Rutgers University. Authors named inside {} equally contributed. Correspondance to",
    "prev": "INTRODUCTION\n\nObjects, and their interactions, are the foundations of human cognition (Spelke & Kinzler, 2007).",
    "curr": "The endowment on making abstractions from perception and organizing them systematically empowers humans the ability to accomplish and generalize across a broad range of tasks, such as scene modeling (Bear et al., 2020), visual reasoning (Yi et al., 2020), and simulating interactions (Bear et al., 2020).",
    "next": "The key to such success lies in the emergence of symbol-like mental representations of object concepts (Whitehead, 1928).",
    "query": "Are there any research papers on unsupervised methods for decomposing complex multi-object scenes into object representations using spatial attention and decomposition, with a focus on scalability and handling large numbers of objects?"
  },
  {
    "index": 112,
    "source_corpus_id": 53452703,
    "ref_id": "b5",
    "citation_corpus_id": 28971531,
    "start": 2480,
    "end": 2502,
    "title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
    "abstract": "Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors (Kiros et al., 2015) on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available 1 .",
    "prev": "Perone et al.",
    "curr": "(2018) have shown that the best encoding architectures are based on recurrent neural networks (RNNs) (Conneau et al., 2017;Peters et al., 2018) or the Transformer architecture (Cer et al., 2018).",
    "next": "These techniques are, however, substantially more expensive to train and apply than word embeddings (Hill et al., 2016;Cer et al., 2018).",
    "query": "Are there any studies that explore supervised learning of universal sentence representations using natural language inference data and their effectiveness compared to unsupervised methods?"
  },
  {
    "index": 114,
    "source_corpus_id": 232320210,
    "ref_id": "b4",
    "citation_corpus_id": 51979536,
    "start": 1995,
    "end": 2015,
    "title": "Large-Scale Study of Curiosity-Driven Learning",
    "abstract": "Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the handdesigned extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github. io/large-scale-curiosity/. * Alphabetical ordering; the first three authors contributed equally.Preprint. Work in progress.",
    "prev": "ut also additionally provides a deterministic compressed representation of the input variable, which is useful for inference tasks that require consistent representation.Moreover, it can jointly learn a feature extractor and select features considering each feature dimension's relevance to the target task, which is unattainable by most neural network-based IB methods.We propose an exploration method based on Drop-Bottleneck for reinforcement learning tasks.In a multitude of noisy and reward sparse maze navigation tasks in VizDoom(Kempka et al., 2016)and DM-Lab (Beattie et al., 2016), our exploration method achieves state-of-the-art performance.As a new IB framework, we demonstrate that Drop-Bottleneck outperforms Variational Information Bottleneck (VIB)(Alemi et al., 2017)in multiple aspects including adversarial robustness and dimensionality reduction.",
    "curr": "INTRODUCTION\n\nData with noise or task-irrelevant information easily harm the training of a model; for instance, the noisy-TV problem (Burda et al., 2019a) is one of well-known such phenomena in reinforcement learning.If observations from the environment are modified to contain a TV screen, which changes its channel randomly based on the agent's actions, the performance of curiosity-based exploration methods dramatically degrades (Burda et al., 2019a;b;Kim et al., 2019;Savinov et al., 2019).",
    "next": "The information bottleneck (IB) theory (Tishby et al., 2000;Tishby & Zaslavsky, 2015) provides a framework for dealing with such task-irrelevant information, and has been actively adopted to exploration in reinforcement learning (Kim et al., 2019;Igl et al., 2019).For an input variable X and a target variable Y , the IB theory introduces another variable Z, which is a compressed representation of X.The IB objective trains Z to contain less information about X but more information about Y as possible, where the two are quantified by mutual information terms of I(Z; X) and I(Z; Y ), respectively.IB methods such as Variational Information Bottleneck (",
    "query": "Are there any research papers on information bottleneck frameworks for reinforcement learning that address noisy or task-irrelevant observations using intrinsic reward mechanisms?"
  },
  {
    "index": 115,
    "source_corpus_id": 219792087,
    "ref_id": "b24",
    "citation_corpus_id": 49411844,
    "start": 2395,
    "end": 2399,
    "title": "DARTS: Differentiable Architecture Search",
    "abstract": "This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques.",
    "prev": "These works usually incur large computation overheads, which make them impractical to use.",
    "curr": "More recent algorithms significantly reduce the search cost including one-shot methods [2,29], a continuous relaxation of the space [25] and network morphisms [5].",
    "next": "In particular, Liu et al.",
    "query": "Are there any research papers on differentiable methods for efficient architecture search in machine learning?"
  },
  {
    "index": 116,
    "source_corpus_id": 247748808,
    "ref_id": "b7",
    "citation_corpus_id": 208857409,
    "start": 2641,
    "end": 2660,
    "title": "YOUR CLASSIFIER IS SECRETLY AN ENERGY BASED MODEL AND YOU SHOULD TREAT IT LIKE ONE",
    "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model.",
    "prev": "INTRODUCTION\n\nAdversarial Training (AT) is one of the most effective approaches developed so far to improve the robustness of deep neural networks (DNNs) (Madry et al., 2018).",
    "curr": "AT solves a minimax optimization problem, with the inner maximization generating adversarial examples by maximizing the classification loss, and the outer minimization finding model parameters by minimizing the loss on adversarial examples generated from the inner maximization (Wang et al., 2019).",
    "next": "Recently, researchers have noticed that such robust classifiers obtained by AT are able to extract features that are perceptually aligned with humans (Engstrom et al., 2019).",
    "query": "Are there any research papers on reinterpretating discriminative classifiers as energy-based models for joint distribution training, focusing on robustness, calibration, and generative capabilities?"
  },
  {
    "index": 118,
    "source_corpus_id": 204734206,
    "ref_id": "b10",
    "citation_corpus_id": 3333039,
    "start": 3038,
    "end": 3059,
    "title": "FIX YOUR CLASSIFIER: THE MARGINAL VALUE OF TRAINING THE LAST WEIGHT LAYER",
    "abstract": "Neural networks are commonly used as models for classification for a wide variety of tasks. Typically, a learned affine transformation is placed at the end of such models, yielding a per-class value used for classification. This classifier can have a vast number of parameters, which grows linearly with the number of possible classes, thus requiring increasingly more resources. In this work we argue that this classifier can be fixed, up to a global scale constant, with little or no loss of accuracy for most tasks, allowing memory and computational benefits. Moreover, we show that by initializing the classifier with a Hadamard matrix we can speed up inference as well. We discuss the implications for current understanding of neural network models.1",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any studies that explore methods for fixing the final classifier layer in neural networks to improve computational efficiency while maintaining classification accuracy?"
  },
  {
    "index": 119,
    "source_corpus_id": 256868547,
    "ref_id": "b13",
    "citation_corpus_id": 208248131,
    "start": 4276,
    "end": 4295,
    "title": "Published as a conference paper at ICLR 2020 ECONOMY STATISTICAL RECURRENT UNITS FOR INFERRING NONLINEAR GRANGER CAUSALITY",
    "abstract": "Granger causality is a widely-used criterion for analyzing interactions in largescale networks. As most physical interactions are inherently nonlinear, we consider the problem of inferring the existence of pairwise Granger causality between nonlinearly interacting stochastic processes from their time series measurements. Our proposed approach relies on modeling the embedded nonlinearities in the measurements using a component-wise time series prediction model based on Statistical Recurrent Units (SRUs). We make a case that the network topology of Granger causal relations is directly inferrable from a structured sparse estimate of the internal parameters of the SRU networks trained to predict the processes' time series measurements. We propose a variant of SRU, called economy-SRU, which, by design has considerably fewer trainable parameters, and therefore less prone to overfitting. The economy-SRU computes a low-dimensional sketch of its high-dimensional hidden state in the form of random projections to generate the feedback for its recurrent processing. Additionally, the internal weight parameters of the economy-SRU are strategically regularized in a group-wise manner to facilitate the proposed network in extracting meaningful predictive features that are highly time-localized to mimic real-world causal events. Extensive experiments are carried out to demonstrate that the proposed economy-SRU based time series prediction model outperforms the MLP, LSTM and attention-gated CNN-based time series models considered previously for inferring Granger causality. . Towards a rigorous assessment of systems biology models: The DREAM3 challenges. PLOS ONE, 5(2):1-18, Feb 2010. . Earth system modeling 2.0: A blueprint for models that learn from observations and targeted high-resolution simulations. . Scalable matrix-valued kernel learning for high-dimensional nonlinear multivariate regression and Granger causality. In Pan. Estimating brain connectivity with varying-length time lags using a recurrent neural network.",
    "prev": "INTRODUCTION\n\nCausal interpretation of the observed time-series data can help answer fundamental causal questions and advance scientific discoveries in various disciplines such as medical and financial fields.",
    "curr": "To enable causal reasoning and counterfactual prediction, researchers in the past decades have been dedicated to discovering causal graphs from observed time-series and made large progress (Gerhardus & Runge, 2020;Tank et al., 2022;Khanna & Tan, 2020;Wu et al., 2022;Pamfil et al., 2020;Löwe et al., 2022;Runge, 2021).",
    "next": "This task is called causal discovery or causal structure learning, which usually formulates causal relationships as Directed Acyclic Graphs (DAGs).",
    "query": "Are there any research papers on methods for inferring nonlinear Granger causality from time-series data using recurrent neural networks or similar approaches?"
  },
  {
    "index": 121,
    "source_corpus_id": 11243593,
    "ref_id": "b10",
    "citation_corpus_id": 14915449,
    "start": 6466,
    "end": 6485,
    "title": "THE GOLDILOCKS PRINCIPLE: READING CHILDREN'S BOOKS WITH EXPLICIT MEMORY REPRESENTATIONS",
    "abstract": "We introduce a new test of how well language models capture meaning in children's books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lowerfrequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words, although this advantage is not observed for syntactic function words. Interestingly, we find that the amount of text encoded in a single memory representation is highly influential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Further, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by applying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance.",
    "prev": "It is also able to generalize to sequences longer than those seen during training.",
    "curr": "Finally, our model also obtains competitive results on the Childrens Book Test (Hill et al., 2016), and performs best among models that read the text in a single pass before receiving knowledge of the question.",
    "next": "MODEL\n\nOur model is designed to process data in sequential form, and consists of three main parts: an input encoder, a dynamic memory and an output layer, which we now describe in detail.",
    "query": "Are there any research papers on language models with explicit memory representations for reading children's books, focusing on the Goldilocks Principle and their effectiveness in semantic prediction and question-answering tasks?"
  },
  {
    "index": 123,
    "source_corpus_id": 3525802,
    "ref_id": "b19",
    "citation_corpus_id": 9615470,
    "start": 7912,
    "end": 7928,
    "title": "A Model of Coherence Based on Distributed Sentence Representation",
    "abstract": "Coherence is what makes a multi-sentence text meaningful, both logically and syntactically. To solve the challenge of ordering a set of sentences into coherent order, existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumentation logic and syntactic relationships. But both argumentation semantics and crosssentence syntax (such as coreference and tense rules) are very hard to formalize. In this paper, we introduce a neural network model for the coherence task based on distributed sentence representation. The proposed approach learns a syntacticosemantic representation for sentences automatically, using either recurrent or recursive neural networks. The architecture obviated the need for feature engineering, and learns sentence representations, which are to some extent able to capture the 'rules' governing coherent sentence structure. The proposed approach outperforms existing baselines and generates the stateof-art performance in standard coherence evaluation tasks 1 .",
    "prev": "Given the first three sentences of a paragraph, choose the next sentence from five sentences later in the paragraph.",
    "curr": "Related to our objective is the local coherence model of Li & Hovy (2014) where a binary classifier is trained to identify coherent/incoherent sentence windows.",
    "next": "In contrast, we only encourage observed contexts to be more plausible than contrastive ones and formulate it as a multi-class classification problem.",
    "query": "Are there any research papers on neural network models for coherence in text using distributed sentence representations and avoiding feature engineering?"
  },
  {
    "index": 125,
    "source_corpus_id": 53081529,
    "ref_id": "b42",
    "citation_corpus_id": 11174813,
    "start": 3303,
    "end": 3322,
    "title": "Learning Distributed Representations for Multilingual Text Sequences",
    "abstract": "We propose a novel approach to learning distributed representations of variable-length text sequences in multiple languages simultaneously. Unlike previous work which often derive representations of multi-word sequences as weighted sums of individual word vectors, our model learns distributed representations for phrases and sentences as a whole. Our work is similar in spirit to the recent paragraph vector approach but extends to the bilingual context so as to efficiently encode meaning-equivalent text sequences of multiple languages in the same semantic space. Our learned embeddings achieve state-of-theart performance in the often used crosslingual document classification task (CLDC) with an accuracy of 92.7 for English to German and 91.5 for German to English. By learning text sequence representations as a whole, our model performs equally well in both classification directions in the CLDC task in which past work did not achieve.",
    "prev": "In our experiments on 9 benchmark text classification datasets and 22 textual similarity tasks, the proposed technique consistently matches or outperforms state-of-the-art techniques, with significantly higher accuracy on problems of short length.",
    "curr": "Introduction\n\nText representation plays an important role in many NLP-based tasks such as document classification and clustering (Zhang et al., 2018;Gui et al., 2016Gui et al., , 2014, sense disambiguation (Gong et al., 2017(Gong et al., , 2018a, machine translation (Mikolov et al., 2013b), document matching (Pham et al., 2015), and sequential alignment (Peng et al., 2016(Peng et al., , 2015.",
    "next": "Since there are no explicit features in text, much work has aimed to develop effective text representations.",
    "query": "Are there any research papers on methods for learning distributed representations of multilingual text sequences for cross-lingual tasks or document classification?"
  },
  {
    "index": 126,
    "source_corpus_id": 203642015,
    "ref_id": "b22",
    "citation_corpus_id": 53015479,
    "start": 2496,
    "end": 2519,
    "title": "PROMP: PROXIMAL META-POLICY SEARCH",
    "abstract": "Credit assignment in Meta-reinforcement learning (Meta-RL) is still poorly understood. Existing methods either neglect credit assignment to pre-adaptation behavior or implement it naively. This leads to poor sample-efficiency during metatraining as well as ineffective task identification strategies. This paper provides a theoretical analysis of credit assignment in gradient-based Meta-RL. Building on the gained insights we develop a novel meta-learning algorithm that overcomes both the issue of poor credit assignment and previous difficulties in estimating meta-policy gradients. By controlling the statistical distance of both pre-adaptation and adapted policies during meta-policy search, the proposed algorithm endows efficient and stable meta-learning. Our approach leads to superior pre-adaptation policy behavior and consistently outperforms previous Meta-RL algorithms in sample-efficiency, wall-clock time, and asymptotic performance. * authors contributed equally to this work arXiv:1810.06784v4 [cs.LG] 11 Feb 2022Published as a conference paper at ICLR 2019We show that the recent formulation introduced by Al-Shedivat et al. (2018)and Stadie et al. (2018)   leads to poor credit assignment, while the MAML formulation(Finn et al., 2017)potentially yields superior meta-policy updates. Second, based on insights from our formal analysis, we highlight both the importance and difficulty of proper meta-policy gradient estimates. In light of this, we propose the low variance curvature (LVC) surrogate objective which yields gradient estimates with a favorable bias-variance trade-off. Finally, building upon the LVC estimator we develop Proximal Meta-Policy Search (ProMP), an efficient and stable meta-learning algorithm for RL. In our experiments, we show that ProMP consistently outperforms previous Meta-RL algorithms in sample-efficiency, wall-clock time, and asymptotic performance.",
    "prev": "However, implementing and running MAML continues to be challenging.",
    "curr": "One major complication is that the standard version of MAML requires estimating second derivatives of the RL reward function, which is difficult when using backpropagation on stochastic policies; indeed, the original implementation of MAML (Finn et al., 2017) did so incorrectly, which spurred the development of unbiased higher-order estimators (DiCE, (Foerster et al., 2018)) and further analysis of the credit assignment mechanism in MAML (Rothfuss et al., 2019).",
    "next": "Another challenge arises from the high variance inherent in policy gradient methods, which can be ameliorated through control variates such as in T-MAML (Liu et al., 2019), through careful adaptive hyperparameter tuning (Behl et al., 2019;Antoniou et al., 2019) and learning rate annealing (Loshchilov & Hutter, 2017).",
    "query": "Are there any research papers on methods for improving credit assignment in meta-reinforcement learning with gradient estimation and variance reduction techniques?"
  },
  {
    "index": 127,
    "source_corpus_id": 256627797,
    "ref_id": "b10",
    "citation_corpus_id": 202573030,
    "start": 2913,
    "end": 2916,
    "title": "NEURAL OBLIVIOUS DECISION ENSEMBLES FOR DEEP LEARNING ON TABULAR DATA",
    "abstract": "Nowadays, deep neural networks (DNNs) have become the main instrument for machine learning tasks within a wide range of domains, including vision, NLP, and speech. Meanwhile, in an important case of heterogenous tabular data, the advantage of DNNs over shallow counterparts remains questionable. In particular, there is no sufficient evidence that deep learning machinery allows constructing methods that outperform gradient boosting decision trees (GBDT), which are often the top choice for tabular problems. In this paper, we introduce Neural Oblivious Decision Ensembles (NODE), a new deep learning architecture, designed to work with any tabular data. In a nutshell, the proposed NODE architecture generalizes ensembles of oblivious decision trees, but benefits from both end-to-end gradient-based optimization and the power of multi-layer hierarchical representation learning. With an extensive experimental comparison to the leading GBDT packages on a large number of tabular datasets, we demonstrate the advantage of the proposed NODE architecture, which outperforms the competitors on most of the tasks. We open-source the PyTorch implementation of NODE and believe that it will become a universal framework for machine learning on tabular data.",
    "prev": "Instead of using a greedy method [9,10] to search splitting rules, soft trees make decision rules soft and simultaneously update the entire model parameters using the gradient method.",
    "curr": "Soft trees have been actively studied in recent years in terms of predictive performance [7,11,12], interpretability [8,13], and potential techniques in real world applications like pre-training and fine-tuning [14,15].",
    "next": "In addition, a soft tree can be interpreted as a Mixture-of-Experts [16][17][18], a practical technique for balancing computational cost and prediction performance.",
    "query": "Are there any studies that explore the use of soft trees in deep learning for tabular data, focusing on predictive performance, interpretability, and applications in pre-training and fine-tuning?"
  },
  {
    "index": 130,
    "source_corpus_id": 221139843,
    "ref_id": "b44",
    "citation_corpus_id": 7186165,
    "start": 30522,
    "end": 30526,
    "title": "Model-Portability Experiments for Textual Temporal Analysis",
    "abstract": "We explore a semi-supervised approach for improving the portability of time expression recognition to non-newswire domains: we generate additional training examples by substituting temporal expression words with potential synonyms. We explore using synonyms both from WordNet and from the Latent Words Language Model (LWLM), which predicts synonyms in context using an unsupervised approach. We evaluate a state-of-the-art time expression recognition system trained both with and without the additional training examples using data from TempEval 2010, Reuters and Wikipedia. We find that the LWLM provides substantial improvements on the Reuters corpus, and smaller improvements on the Wikipedia corpus. We find that WordNet alone never improves performance, though intersecting the examples from the LWLM and WordNet provides more stable results for Wikipedia.",
    "prev": "A Glossary of Notation\n\nWe provide a glossary of notation used throughout the paper.Total consistency loss (Eq 3) L : X 2 → R A distance function, used for CycleGAN consistency losses λ\n\nHyperparameter controlling the strength of the consistency loss KL(•)\n\nThe KL divergence JS(•)\n\nThe Jensen-Shannon divergence (Definition 2)\nI(•)\nThe Mutual Information\n\n\nB Extended Related Work\n\nWe provide a comprehensive overview of related work and highlight connections to our work below.",
    "curr": "B.1 Overview of Data Augmentation\n\nData augmentation is widely used for improving the aggregate performance of machine learning models in computer vision [46,79], natural language processing [45,71,95] and audio [18,43].The theoretical motivation for data augmentation is largely based on the tangent propagation formalism [19,73,74,76] which expresses the desired invariances induced by a data augmentation as tangent constraints on the directional derivatives of the learned model.",
    "next": "Early work considered augmentations as image defects [5] or stroke warping [90] for character recognition.Since then, augmentation is considered an essential ingredient in computer vision [47,75], with commonly used augmentations including random flips, rotations and crops [31,46,79].Applications of augmentation in computer vision include object detection [23,98] and scene understanding [22] In natural language processing, common data augmentation techniques include back-translation [71,91], synonym or word substitution [25,44,45,83,95], noising [89], grammar induction [39], text editing [85] and other heuristics [20,72].In speech and audio applications, augmentation is also commonly used, through tec",
    "query": "Are there any research papers on data augmentation techniques for improving model portability in textual temporal analysis, particularly using synonym substitution methods like those from WordNet or latent context-based models?"
  },
  {
    "index": 131,
    "source_corpus_id": 258461359,
    "ref_id": "b0",
    "citation_corpus_id": 67855815,
    "start": 2219,
    "end": 2241,
    "title": "Massively Multilingual Neural Machine Translation",
    "abstract": "Multilingual neural machine translation (NMT) enables training a single model that supports translation from multiple source languages into multiple target languages. In this paper, we push the limits of multilingual NMT in terms of the number of languages being used. We perform extensive experiments in training massively multilingual NMT models, translating up to 102 languages to and from English within a single model. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages. Our experiments on a large-scale dataset with 102 languages to and from English and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.",
    "prev": "INTRODUCTION\n\nThe rapid development of neural networks opens the path towards the ambitious goal of universal translation that allows converting information between any languages regardless of data modalities (text, audio or video) (Zhang, 2022).",
    "curr": "While the translation for spoken languages (in text and speech) has gained wide attention (Aharoni et al., 2019;Inaguma et al., 2019;Jia et al., 2019), the study of sign language translation (SLT) -a task translating from sign language videos to spoken language texts -still lags behind despite its significance in facilitating the communication between Deaf communities and spoken language communities (Camgoz et al., 2018;.",
    "next": "SLT represents unique challenges: it demands the capability of video understanding and sequence generation.",
    "query": "Are there any research papers on massively multilingual neural machine translation models that support a large number of languages and explore trade-offs between translation quality and training strategies?"
  },
  {
    "index": 132,
    "source_corpus_id": 246867279,
    "ref_id": "b11",
    "citation_corpus_id": 219981345,
    "start": 4505,
    "end": 4524,
    "title": "MULTI-CLASS UNCERTAINTY CALIBRATION VIA MU- TUAL INFORMATION MAXIMIZATION-BASED BINNING",
    "abstract": "Post-hoc multi-class calibration is a common approach for providing high-quality confidence estimates of deep neural network predictions. Recent work has shown that widely used scaling methods underestimate their calibration error, while alternative Histogram Binning (HB) methods often fail to preserve classification accuracy. When classes have small prior probabilities, HB also faces the issue of severe sample-inefficiency after the conversion into K one-vs-rest class-wise calibration problems. The goal of this paper is to resolve the identified issues of HB in order to provide calibrated confidence estimates using only a small holdout calibration dataset for bin optimization while preserving multi-class ranking accuracy. From an information-theoretic perspective, we derive the I-Max concept for binning, which maximizes the mutual information between labels and quantized logits. This concept mitigates potential loss in ranking performance due to lossy quantization, and by disentangling the optimization of bin edges and representatives allows simultaneous improvement of ranking and calibration performance. To improve the sample efficiency and estimates from a small calibration set, we propose a shared class-wise (sCW) calibration strategy, sharing one calibrator among similar classes (e.g., with similar class priors) so that the training sets of their class-wise calibration problems can be merged to train the single calibrator. The combination of sCW and I-Max binning outperforms the state of the art calibration methods on various evaluation metrics across different benchmark datasets and models, using a small calibration set (e.g., 1k samples for ImageNet). *",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on multi-class uncertainty calibration methods that use mutual information maximization for binning and improve sample efficiency with shared class-wise strategies?"
  },
  {
    "index": 133,
    "source_corpus_id": 239616399,
    "ref_id": "b13",
    "citation_corpus_id": 231933963,
    "start": 4278,
    "end": 4282,
    "title": "OFFLINE MODEL-BASED OPTIMIZATION VIA NOR- MALIZED MAXIMUM LIKELIHOOD ESTIMATION",
    "abstract": "In this work we consider data-driven optimization problems where one must maximize a function given only queries at a fixed set of points. This problem setting emerges in many domains where function evaluation is a complex and expensive process, such as in the design of materials, vehicles, or neural network architectures. Because the available data typically only covers a small manifold of the possible space of inputs, a principal challenge is to be able to construct algorithms that can reason about uncertainty and out-of-distribution values, since a naive optimizer can easily exploit an estimated model to return adversarial inputs. We propose to tackle this problem by leveraging the normalized maximum-likelihood (NML) estimator, which provides a principled approach to handling uncertainty and out-of-distribution inputs. While in the standard formulation NML is intractable, we propose a tractable approximation that allows us to scale our method to high-capacity neural network models. We demonstrate that our method can effectively optimize high-dimensional design problems in a variety of disciplines such as chemistry, biology, and materials engineering.Published as a conference paper at ICLR 2021 vides a flexible approach to constructing conservative and robust estimators using high-dimensional models such as neural networks.",
    "prev": "Such a data-driven approach provides three key benefits: (1) it significantly shortens the recurring cost of running large-scale simulation sweeps, (2) it alleviates the need to explicitly bake in domain knowledge or search space pruning, and (3) it enables data re-use by empowering the designer to optimize accelerators for new unseen applications, by the virtue of effective generalization.",
    "curr": "While data-driven approaches have shown promising results in biology [14,5,57], using offline optimization methods to design accelerators has been challenging Figure 1: Overview of PRIME.",
    "next": "We use a one-time collected dataset of prior accelerator designs, including TPU-style [65], NVDLA-style [42], and ShiDianNao-style [10] accelerators to train a conservative surrogate model, which is used to design accelerators to meet desired goals and constraints.",
    "query": "Are there any research papers on offline model-based optimization techniques using normalized maximum likelihood estimation to address uncertainty and out-of-distribution challenges in high-dimensional design problems across fields like materials science, biology, and engineering?"
  },
  {
    "index": 134,
    "source_corpus_id": 220665925,
    "ref_id": "b28",
    "citation_corpus_id": 68137503,
    "start": 2547,
    "end": 2551,
    "title": "FUNCTIONAL VARIATIONAL BAYESIAN NEURAL NETWORKS",
    "abstract": "Variational Bayesian neural networks (BNNs) perform variational inference over weights, but it is difficult to specify meaningful priors and approximate posteriors in a high-dimensional weight space. We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions. We prove that the KL divergence between stochastic processes equals the supremum of marginal KL divergences over all finite sets of inputs. Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator. With fBNNs, we can specify priors entailing rich structures, including Gaussian processes and implicit stochastic processes. Empirically, we find fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and scale to large datasets. * Equal contribution.Theorem 7. For two stochastic processes P, M on a cylindrical measurable space (Ω T , F T ), the KL divergence of P with respect to M satisfies,where the supremum is over all finite indices subsets T d ⊆ T , and P T d , M T d represent the canonical projection maps π T →T d of P, M , respectively.Proof. Recall that stochastic processes are defined over a cylindrical σ-algebra F T . By Lemma 6, for every set H ∈ F T , the restricted index set τ (H) is countable. Our proof proceeds in two steps:",
    "prev": "The variational approach is limited in posterior expressiveness while the implicit approach is computationally slow and costly in terms of storage.",
    "curr": "Moreover, specifying meaningful priors in parameter space is known to be difficult due to the complex relationship between weights and functions in deep networks [29].",
    "next": "In this paper, we present a Bayesian approach to FSC based on Gaussian processes (GPs) [36] that enables efficient marginalization over functions rather than model parameters.",
    "query": "Are there any research papers on Bayesian neural networks that use functional variational methods and Gaussian processes for efficient marginalization over functions and structured priors?"
  },
  {
    "index": 135,
    "source_corpus_id": 263611938,
    "ref_id": "b21",
    "citation_corpus_id": 252683312,
    "start": 8855,
    "end": 8862,
    "title": "OMNIGROK: GROKKING BEYOND ALGORITHMIC DATA",
    "abstract": "Grokking, the unusual phenomenon for algorithmic datasets where generalization happens long after overfitting the training data, has remained elusive. We aim to understand grokking by analyzing the loss landscapes of neural networks, identifying the mismatch between training and test loss landscapes as the cause for grokking. We refer to this as the \"LU mechanism\" because training and test losses (against model weight norm) typically resemble \"L\" and \"U\", respectively. This simple mechanism can nicely explain many aspects of grokking: data size dependence, weight decay dependence, the emergence of representations, etc. Guided by the intuitive picture, we are able to induce grokking on tasks involving images, language and molecules. In the reverse direction, we are able to eliminate grokking for algorithmic datasets. We attribute the dramatic nature of grokking for algorithmic datasets to representation learning.Partial answers to Q1 are provided in recent studies: Liu et al. (2022) attribute grokking to the slow formation of good representations,Thilak et al. (2022)attempts to link grokking to the slingshot mechanism of adaptive optimizers, andBarak et al. (2022)uses Fourier gap to describe hidden progress. This paper aims to understand grokking through the lens of neural loss landscapes. Our landscape analysis is able to explain many aspects of grokking: data size dependence, weight decay dependence, emergence of representations, etc.The paper is organized as follows: In Section 2, we review background on generalization, and introduce the LU mechanism. In Section 3, we show how the LU mechanism leads to grokking for a toy teacher-student setup. In Section 4, we show that the intuition gained from the toy problem can",
    "prev": "[Thi+22] attributed grokking to the slingshot mechanism, which can be measured by the cyclic phase transitions between stable and unstable training regimes.Žunkovič and Ilievski [ŽI22] showed a time separation between achieving zero training error and zero test error in a binary classification task on a linearly separable distribution.",
    "curr": "Liu, Michaud, and Tegmark [LMT23] identified a large initialization scale together with weight decay as a mechanism for grokking.",
    "next": "Barak et al.",
    "query": "Are there any research papers on the analysis of neural network loss landscapes to explain the phenomenon of grokking in algorithmic datasets?"
  },
  {
    "index": 136,
    "source_corpus_id": 213969759,
    "ref_id": "b14",
    "citation_corpus_id": 52055130,
    "start": 3776,
    "end": 3795,
    "title": "Learning deep representations by mutual information estimation and maximization",
    "abstract": "In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals.IntroductionOne core objective of deep learning is to discover \"good\" representations, and the simple idea explored here is to train a representation-learning function (i.e., an encoder) to maximize the mutual information (MI) between its inputs and outputs. MI is notoriously difficult to compute, particularly in continuous and high-dimensional settings. Fortunately, recent advances enable effective computation of MI between high dimensional input/output pairs of deep neural networks(Belghazi et al., 2018). We leverage MI estimation for representation learning, but we will also show that, depending on the downstream task, maximizing MI between the complete input and the encoder output (i.e., global MI) is often not sufficient for learning useful representations. Rather, structure matters: maximizing the average MI between the representation and local regions of the input can greatly improve the representation's quality for, e.g., classification tasks, while global MI plays a stronger role in the ability to reconstruct the full input given the representation.",
    "prev": "A seminal work is the InfoMax principle (Linsker, 1988), where given an input instance x, the goal of the InfoMax principle is to learn a representation E ψ (x) by maximizing the MI between the input and its representation.",
    "curr": "A growing set of recent works have demonstrated promising empirical performance in unsupervised representation learning via MI maximization (Krause et al., 2010;Hu et al., 2017;Alemi et al., 2018b;Oord et al., 2018;Hjelm et al., 2019).",
    "next": "Another closely related work is the Information Bottleneck method Alemi et al., 2017), where MI is used to limit the contents of representations.",
    "query": "Are there any research papers on unsupervised representation learning through mutual information maximization with a focus on incorporating input structure or locality?"
  },
  {
    "index": 137,
    "source_corpus_id": 34984289,
    "ref_id": "b2",
    "citation_corpus_id": 9716222,
    "start": 1855,
    "end": 1881,
    "title": "Training with Exploration Improves a Greedy Stack LSTM Parser",
    "abstract": "We adapt the greedy stack LSTM dependency parser ofDyer et al. (2015)to support a training-with-exploration procedure using dynamic oracles (Goldberg and Nivre, 2013) instead of assuming an error-free action history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser.",
    "prev": "* Equal contribution.",
    "curr": "Introduction\n\nRecurrent neural networks (RNNs) have been recently quite successful in structured prediction applications such as machine translation (Sutskever et al., 2014), parsing (Ballesteros et al., 2016) or caption generation (Vinyals et al., 2015).",
    "next": "These models use the same repeated cell (or unit) to output a sequence of tokens one by one.",
    "query": "Are there any research papers on training methods for recurrent neural networks in structured prediction tasks, such as parsing or machine translation, that use exploration-based techniques or dynamic oracles to improve model performance?"
  },
  {
    "index": 139,
    "source_corpus_id": 235313504,
    "ref_id": "b39",
    "citation_corpus_id": 3488815,
    "start": 2074,
    "end": 2078,
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. * Authors ordered alphabetically.",
    "prev": "Introduction\n\nThis paper considers minimax optimization min x max y f (x, y) in the context of two-player zero-sum games, where the min-player (controlling x) tries to minimize objective f assuming a worst-case opponent (controlling y) that acts so as to maximize it.",
    "curr": "Minimax optimization naturally arises in a variety of important machine learning paradigms, with the most prominent examples being the training of generative adversarial networks (GANs) [20] and adversarially robust models [40].",
    "next": "These applications commonly engage deep neural networks with various techniques such as convolution, recurrent layers, and batch normalization.",
    "query": "Are there any research papers on adversarial robustness in deep learning models using minimax or robust optimization techniques, particularly in generative adversarial networks and adversarial attack defense?"
  },
  {
    "index": 140,
    "source_corpus_id": 258714845,
    "ref_id": "b24",
    "citation_corpus_id": 59608630,
    "start": 20594,
    "end": 20620,
    "title": "ADAPTIVE POSTERIOR LEARNING: FEW-SHOT LEARNING WITH A SURPRISE-BASED MEMORY MODULE",
    "abstract": "The ability to generalize quickly from few observations is crucial for intelligent systems. In this paper we introduce APL, an algorithm that approximates probability distributions by remembering the most surprising observations it has encountered. These past observations are recalled from an external memory module and processed by a decoder network that can combine information from different memory slots to generalize beyond direct recall. We show this algorithm can perform as well as state of the art baselines on few-shot classification benchmarks with a smaller memory footprint. In addition, its memory compression allows it to scale to thousands of unknown labels. Finally, we introduce a meta-learning reasoning task which is more challenging than direct classification. In this setting, APL is able to generalize with fewer than one example per class via deductive reasoning.",
    "prev": "We refer to Le (2021) for an extensive overview of memory-based architectures.",
    "curr": "Architectures with external memories have also been used for meta-learning (Vinyals et al., 2016;Santoro et al., 2016) and few-shot learning (Munkhdalai and Yu, 2017;Ramalho and Garnelo, 2018;Ma et al., 2021 (Rogers and Hahn, 2010) and key molecular physical descriptors, which were defined by RDKit (Landrum et al., 2006).",
    "next": "While methods would be allowed to use other representations of the input molecules, such as the molecular graph, we used a concatenation of these ECFPs and RDKit-based descriptors.",
    "query": "Are there any research papers on few-shot learning algorithms that utilize surprise-based memory modules for adaptive posterior learning?"
  },
  {
    "index": 141,
    "source_corpus_id": 222272443,
    "ref_id": "b28",
    "citation_corpus_id": 166228758,
    "start": 3392,
    "end": 3434,
    "title": "Published as a conference paper at ICLR 2020 N-BEATS: NEURAL BASIS EXPANSION ANALYSIS FOR INTERPRETABLE TIME SERIES FORECASTING",
    "abstract": "We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.Published as a conference paper at ICLR 2020 inject a suitable inductive bias in the model to make its internal operations more interpretable, in the sense of extracting some explainable driving factors combining to produce a given forecast?SUMMARY OF CONTRIBUTIONSDeep Neural Architecture: To the best of our knowledge, this is the first work to empirically demonstrate that pure DL using no time-series specific components outperforms well-established statistical approaches on M3, M4 and TOURISM datasets (on M4, by 11% over statistical benchmark, by 7% over the best statistical entry, and by 3% over the M4 competition winner). In our view, this provides a long-missing proof of concept for the use of pure ML in TS forecasting and strengthens motivation to continue advancing the research in this area.Interpretable DL for Time Series: In addition to accuracy benefits, we also show that it is feasible to design an architecture with interpretable outputs that can be used by practitioners in very much the same way as traditional decomposition techniques such as the \"seasonality-trend-level\" approach (Cleveland et al., 1990).V. Assimakopoulos and K. Nikolopoulos. The theta model: a decomposition approach to forecasting.",
    "prev": "Model Based (MB) approaches typically rely on partial or ordinary differential equations (PDE/ODE) and stem from a deep understanding of the underlying physical phenomena.",
    "curr": "Machine learning (ML) and deep learning methods are more prior agnostic yet have become state-of-the-art for several spatio-temporal prediction tasks (Shi, Chen, Wang, Yeung, Wong & Woo 2015, Wang, Gao, Long, Wang & Yu 2018, Oreshkin, Carpov, Chapados & Bengio 2020, Donà, Franceschi, Lamprier & Gallinari 2020, and connections have been drawn between deep architectures and numerical ODE solvers, e.g.",
    "next": "neural ODEs (Chen, Rubanova, Bettencourt & Duvenaud 2018, Ayed, de Bézenac, Pajot, Brajard & Gallinari 2019.",
    "query": "Are there any research papers on deep learning methods for interpretable time series forecasting that use general neural network architectures without time-series-specific components?"
  },
  {
    "index": 146,
    "source_corpus_id": 52986403,
    "ref_id": "b20",
    "citation_corpus_id": 6628106,
    "start": 37015,
    "end": 37034,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "A mean pooling layer is used to summarize the LSTM outputs across time, followed by a linear projection layer to predict the posterior mean and log variance.",
    "curr": "C DETAILED EXPERIMENTAL SETUP\n\nThe network is trained using the Adam optimizer (Kingma & Ba, 2015), configured with an initial learning rate 10 −3 , and an exponential decay that halved the learning rate every 12.5k steps, beginning after 50k steps.",
    "next": "Table 6 details the list of prior hyperparameters used for each of the four datasets described in Section 4: multi-speaker English data (multi-spk), noisified multi-speaker English data (noisy-multispk), single-speaker story-telling data (audiobooks), and crowd-sourced audiobook data (crowdsourced).",
    "query": "Are there any research papers on the application of the Adam optimizer with specific hyperparameter configurations in training machine learning models across diverse datasets?"
  },
  {
    "index": 147,
    "source_corpus_id": 253237975,
    "ref_id": "b7",
    "citation_corpus_id": 848112,
    "start": 4188,
    "end": 4213,
    "title": "TRAINING GANS WITH OPTIMISM",
    "abstract": "We address the issue of limit cycling behavior in training Generative Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for training Wasserstein GANs. Recent theoretical results have shown that optimistic mirror decent (OMD) can enjoy faster regret rates in the context of zero-sum games. WGANs is exactly a context of solving a zero-sum game with simultaneous noregret dynamics. Moreover, we show that optimistic mirror decent addresses the limit cycling problem in training WGANs. We formally show that in the case of bi-linear zero-sum games the last iterate of OMD dynamics converges to an equilibrium, in contrast to GD dynamics which are bound to cycle. We also portray the huge qualitative difference between GD and OMD dynamics with toy examples, even when GD is modified with many adaptations proposed in the recent literature, such as gradient penalty or momentum. We apply OMD WGAN training to a bioinformatics problem of generating DNA sequences. We observe that models trained with OMD achieve consistently smaller KL divergence with respect to the true underlying distribution, than models trained with GD variants. Finally, we introduce a new algorithm, Optimistic Adam, which is an optimistic variant of Adam. We apply it to WGAN training on CIFAR10 and observe improved performance in terms of inception score as compared to Adam.",
    "prev": "networks (Arjovsky et al., 2017;Goodfellow et al., 2014), adversarial learning (Goodfellow et al., 2015;Miller et al., 2020), reinforcement learning (Dai et al., 2017;Modi et al., 2021), sharpness-aware minimization (Foret et al., 2021), domain-adversarial training (Ganin et al., 2016), etc.",
    "curr": "Albeit theoretically underexplored, adaptive methods are widely deployed in these applications in combination with popular minimax optimization algorithms such as (stochastic) gradient descent ascent (GDA), extragradient (EG) (Korpelevich, 1976), and optimistic GDA (Popov, 1980;Rakhlin and Sridharan, 2013); see, e.g., (Daskalakis et al., 2018;Gulrajani et al., 2017;Mishchenko et al., 2020;Reisizadeh et al., 2020), just to list a few.",
    "next": "While it seems natural to directly extend adaptive stepsizes to minimax optimization algorithms, a recent work by Yang et al.",
    "query": "Are there any research papers on using optimistic algorithms for training Wasserstein GANs to address convergence issues and limit cycling behavior?"
  },
  {
    "index": 150,
    "source_corpus_id": 222209080,
    "ref_id": "b18",
    "citation_corpus_id": 1487550,
    "start": 3081,
    "end": 3097,
    "title": "UNSUPERVISED WORD SENSE DISAMBIGUATION RIVALING SUPERVISED METHODS",
    "abstract": "This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints -that words tend to have one sense per discourse and one sense per collocation -exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%.",
    "prev": "Self-training is a common algorithmic paradigm for leveraging unlabeled data with deep networks.",
    "curr": "Self-training methods train a model to fit pseudolabels, that is, predictions on unlabeled data made by a previously-learned model (Yarowsky, 1995;Grandvalet & Bengio, 2005;Lee, 2013).",
    "next": "Recent work also extends these methods to enforce stability of predictions under input transformations such as adversarial perturbations (Miyato et al., 2018) and data augmentation (Xie et al., 2019).",
    "query": "Are there any research papers on unsupervised word sense disambiguation techniques using self-training or iterative bootstrapping methods?"
  },
  {
    "index": 154,
    "source_corpus_id": 238583191,
    "ref_id": "b5",
    "citation_corpus_id": 220249831,
    "start": 23586,
    "end": 23605,
    "title": "Heteroskedastic and Imbalanced Deep Learning with Adaptive Regularization",
    "abstract": "Real-world large-scale datasets are heteroskedastic and imbalanced -labels have varying levels of uncertainty and label distributions are long-tailed. Heteroskedasticity and imbalance challenge deep learning algorithms due to the difficulty of distinguishing among mislabeled, ambiguous, and rare examples. Addressing heteroskedasticity and imbalance simultaneously is under-explored. We propose a data-dependent regularization technique for heteroskedastic datasets that regularizes different regions of the input space differently. Inspired by the theoretical derivation of the optimal regularization strength in a one-dimensional nonparametric classification setting, our approach adaptively regularizes the data points in higher-uncertainty, lower-density regions more heavily. We test our method on several benchmark tasks, including a real-world heteroskedastic and imbalanced dataset, WebVision. Our experiments corroborate our theory and demonstrate a significant improvement over other methods in noise-robust deep learning.",
    "prev": "In Figure \n\n\nImproving SSL on Imbalanced Datasets with Regularization\n\nIn this section, we aim to further improve the performance of SSL to close the gap between imbalanced and balanced datasets.",
    "curr": "Many prior works on imbalanced supervised learning regularize the rare classes more strongly, motivated by the observation that the rare classes suffer from more overfitting [Cao et al., 2019[Cao et al., , 2021.",
    "next": "Inspired by these works, we compute the generalization gaps (i.e., the differences between empirical and validation pre-training losses) on frequent and rare classes for the step-imbalance CIFAR-10 datasets (where 5 classes are frequent class with 5000 examples per class and the rest are rare with 50 examples per class).",
    "query": "Are there any research papers on adaptive regularization techniques for improving self-supervised learning on imbalanced datasets, particularly focusing on the distinction between rare and frequent classes?"
  },
  {
    "index": 155,
    "source_corpus_id": 238419007,
    "ref_id": "b38",
    "citation_corpus_id": 3488815,
    "start": 48219,
    "end": 48238,
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. * Authors ordered alphabetically.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on methods for improving adversarial robustness in deep learning models through robust optimization techniques?"
  },
  {
    "index": 157,
    "source_corpus_id": 222125116,
    "ref_id": "b9",
    "citation_corpus_id": 5590763,
    "start": 2351,
    "end": 2355,
    "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "prev": "A variety of approaches have been suggested to mitigate the exploding and vanishing gradient problem.",
    "curr": "These include adding gating mechanisms to the RNN in order to control the flow of information in the network, leading to architectures such as long short-term memory (LSTM) [21] and gated recurring units (GRU) [10], that can overcome the vanishing gradient problem on account of the underlying additive structure.",
    "next": "However, the gradients might still explode and learning very long term dependencies remains a challenge [30].",
    "query": "Are there any research papers on RNN Encoder-Decoder models for statistical machine translation and their use of gated mechanisms like LSTM or GRU?"
  },
  {
    "index": 158,
    "source_corpus_id": 4679427,
    "ref_id": "b1",
    "citation_corpus_id": 18828233,
    "start": 2297,
    "end": 2322,
    "title": "TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.",
    "prev": "*\n\nIntroduction\n\nIn order to learn Generative Adversarial Networks (Goodfellow et al., 2014), it is now well established that the generator should mimic the distribution of real data, in the sense of a certain discrepancy measure.",
    "curr": "Discrepancies between distributions that measure the goodness of the fit of the neural generator to the real data distribution has been the subject of many recent studies (Arjovsky & Bottou, 2017;Nowozin et al., 2016;Kaae Sønderby et al., 2017;Mao et al., 2017;Gulrajani et al., 2017;, most of which focus on training stability.",
    "next": "In terms of data modalities, most success was booked in plausible natural image generation after the introduction of Deep Convolutional Generative Adversarial Networks (DCGAN) (Radford et al., 2015).",
    "query": "Are there any research papers on theoretical methods for addressing training instability and saturation in generative adversarial networks?"
  },
  {
    "index": 159,
    "source_corpus_id": 219558760,
    "ref_id": "b36",
    "citation_corpus_id": 52895589,
    "start": 1628,
    "end": 1644,
    "title": "HOW POWERFUL ARE GRAPH NEURAL NETWORKS?",
    "abstract": "Graph Neural Networks (GNNs) for representation learning of graphs broadly follow a neighborhood aggregation framework, where the representation vector of a node is computed by recursively aggregating and transforming feature vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs in capturing different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",
    "prev": "Under review.",
    "curr": "Introduction\n\nGraph neural networks (GNNs) (Scarselli et al., 2008;Micheli, 2009) have seen growing popularity over the last few years (Duvenaud et al., 2015;Hamilton et al., 2017;Xu et al., 2019).",
    "next": "Many domains can be naturally represented as graphs.",
    "query": "Are there any research papers on the theoretical analysis of expressive power and limitations of graph neural networks compared to existing models?"
  },
  {
    "index": 160,
    "source_corpus_id": 88514953,
    "ref_id": "b17",
    "citation_corpus_id": 16209268,
    "start": 3397,
    "end": 3421,
    "title": "Published as a conference paper at ICLR 2015 QUALITATIVELY CHARACTERIZING NEURAL NETWORK OPTIMIZATION PROBLEMS",
    "abstract": "Training neural networks involves solving large-scale non-convex optimization problems. This task has long been believed to be extremely difficult, with fear of local minima and other obstacles motivating a variety of schemes to improve optimization, such as unsupervised pretraining. However, modern neural networks are able to achieve negligible training error on complex tasks, using only direct training with stochastic gradient descent. We introduce a simple analysis technique to look for evidence that such networks are overcoming local optima. We find that, in fact, on a straight path from initialization to solution, a variety of state of the art neural networks never encounter any significant obstacles.",
    "prev": "Ideally, we would like to quantify the probability to converge to a local minimum as a function of the error at this minimum, where the probability is taken with the respect to the randomness of the initialization of the weights, the data and SGD.",
    "curr": "Specifically, we would like to know, under which conditions this probability is very small if the error is high, as was observed empirically (e.g., (Dauphin et al., 2014;Goodfellow et al., 2015)).",
    "next": "However, this seems to be a daunting task for realistic MNNs, since it requires a characterization of the sizes and distributions of the basins of attraction for all local minima.",
    "query": "Are there any studies that investigate the relationship between the probability of converging to high-error local minima in neural networks and factors such as weight initialization, data distribution, and stochastic gradient descent dynamics?"
  },
  {
    "index": 163,
    "source_corpus_id": 263671510,
    "ref_id": "b31",
    "citation_corpus_id": 247595263,
    "start": 10710,
    "end": 10730,
    "title": "SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS",
    "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
    "prev": "We also investigated using only GSM8K to create difficult problems, but we found that the new problems were too similar to the original ones, and the large gap to MATH still exists (more information can be found in Appendix C).",
    "curr": "Self-distillation.Given that we do not have ground truth answers for the new problems, we then generate n different LCE solutions as depicted in (Wang et al., 2023a) for each new problem with our initial MathCoder models, keeping only those solutions for which all n answers match (n is set to 3 in this paper), thus ensuring our dataset's quality.We use MathCoder-Initial here because it demonstrates the potential for effective model distillation using a model much weaker than the powerful closed-source models.As MathCoder-Initial already has an accuracy of 77.3% on GSM8K and 44.0% on MATH, it is plausible that distilling it can produce good results.It also reduces the cost compared to using GPT-4.Some examples can be found in Appendix A.",
    "next": "Combining the new data D 1 with the seed data D 0 yields the MathCodeInstruct dataset D = {D 0 , D 1 }.We fine-tune the base Llama-2 (Touvron et al., 2023) and CodeLlama (Rozière et al., 2023) models using MathCodeInstruct to derive our final MathCoder models.For clarity, we refer to the supervised fine-tuning of base Llama-2 as \"MathCoder-L\" and that of CodeLlama as \"MathCoder-CL\", as shown in Fig.",
    "query": "Are there any research papers on self-consistency methods for improving chain-of-thought reasoning in language models?"
  },
  {
    "index": 164,
    "source_corpus_id": 33513311,
    "ref_id": "b13",
    "citation_corpus_id": 252796,
    "start": 4552,
    "end": 4572,
    "title": "Building a Large Annotated Corpus of English: The Penn Treebank",
    "abstract": "There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenomena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large corpora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valuable for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investigation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models.In this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus 1 consisting of over 4.5 million words of American English. During the first three-year phase of the Penn Treebank Project (1989)(1990)(1991)(1992), this corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure. These materials are available to members of the Linguistic Data Consortium; for details, see Section 5.1.The paper is organized as follows. Section 2 discusses the POS tagging task. After outlining the considerations that informed the design of our POS tagset and presenting the tagset itself, we describe our two-stage tagging process, in which text is first assigned POS tags automatically and then corrected by human annotators. Section 3 briefly presents the results of a comparison between entirely manual and semi-automated tagging, with the latter being shown to be superior on three counts: speed, consistency, and accuracy. In Section 4, we turn to the bracketing task. Just as with the tagging task, we have partially automated the bracketing task: the output of",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on the development and application of large annotated corpora in natural language processing, particularly focusing on part-of-speech tagging and syntactic annotation methods?"
  },
  {
    "index": 165,
    "source_corpus_id": 52978527,
    "ref_id": "b53",
    "citation_corpus_id": 27494814,
    "start": 10231,
    "end": 10249,
    "title": "To prune, or not to prune: exploring the efficacy of pruning for model compression",
    "abstract": "Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports(Han et al., 2015a;Narang et al., 2017)prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.",
    "prev": "Frankle & Carbin (2018) hypothesizes that certain connections, together with their randomly initialized weights, is particularly effective for training, and a pruning algorithm can help find such sub-networks.",
    "curr": "Zhu & Gupta (2018) shows that training a small-dense model cannot achieve the same accuracy as a pruned large-sparse model with identical memory footprint.",
    "next": "In this work, we reveal a different and rather surprising characteristic of network pruning methods: fine-tuning the pruned model with inherited weights is no better than training it from scratch.",
    "query": "Are there any studies that compare the effectiveness of model pruning techniques with alternative model compression strategies, such as reducing hidden unit counts in dense networks, in terms of accuracy and memory footprint trade-offs?"
  },
  {
    "index": 168,
    "source_corpus_id": 250627720,
    "ref_id": "b15",
    "citation_corpus_id": 231847016,
    "start": 6650,
    "end": 6671,
    "title": "DISCOVERING A SET OF POLICIES FOR THE WORST CASE REWARD",
    "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, setmax policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is a policy iteration algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite. . Fast reinforcement learning with generalized policy updates. . Diversity is all you need: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018. Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval research logistics quarterly, 3(1-2):95-110, 1956. Dan Garber and Elad Hazan. A linearly convergent conditional gradient algorithm with applications to online and stochastic optimization. arXiv preprint arXiv:1301.4666, 2013. Reinforcement learning with soft state aggregation. In Advances in neural information processing systems, pp. 361-368, 1995. Nathan Sprague and Dana Ballard. Multiple-goal reinforcement learning with modular sarsa (0). 2003.",
    "prev": "Multiple tasks In standard multitask RL, there is a (possibly infinite) set of tasks (MDPs) M = {M }, usually presented to the agent by sampling from some task distribution P M ∈ P(M).",
    "curr": "Typical objectives include finding either a single policy or a set of policies which maximize worstor average-case value: max π min M ∈M V π M (Zahavy et al., 2021) or max π E P M V π M (Moskovitz et al., 2022a).",
    "next": "When the emphasis is on decreasing the required sample complexity of learning new tasks, a useful metric is cumulative regret: the agent's total shortfall across training compared to an optimal agent.",
    "query": "Are there any research papers on reinforcement learning methods for constructing sets of policies that optimize worst-case performance across multiple tasks?"
  },
  {
    "index": 169,
    "source_corpus_id": 256615813,
    "ref_id": "b27",
    "citation_corpus_id": 3144218,
    "start": 6513,
    "end": 6535,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "Bounding Shape Mask R- CNN (Kang et al., 2020) improves performance on object detection and instance segmentation by its bounding shape mask branch.",
    "curr": "BCNet (Ke et al., 2021) uses two GCN (Welling & Kipf, 2016) layers to detect overlapping instances.",
    "next": "Although these algorithms have yielded promising results, they are still restricted in the low-resolution mask representation and thus do not generate high-quality masks.",
    "query": "Are there any research papers on semi-supervised learning approaches using graph convolutional networks for classification tasks on graph-structured data?"
  },
  {
    "index": 171,
    "source_corpus_id": 263609164,
    "ref_id": "b22",
    "citation_corpus_id": 219530969,
    "start": 33805,
    "end": 33830,
    "title": "Deployment-Efficient Reinforcement Learning via Model-Based Offline Optimization",
    "abstract": "Most reinforcement learning (RL) algorithms assume online access to the environment, in which one may readily interleave updates to the policy with experience collection using that policy. However, in many real-world applications such as health, education, dialogue agents, and robotics, the cost or potential risk of deploying a new data-collection policy is high, to the point that it can become prohibitive to update the data-collection policy more than a few times during learning. With this view, we propose a novel concept of deployment efficiency, measuring the number of distinct data-collection policies that are used during policy learning. We observe that naïvely applying existing model-free offline RL algorithms recursively does not lead to a practical deployment-efficient and sample-efficient algorithm. We propose a novel model-based algorithm, Behavior-Regularized Model-ENsemble (BREMEN) that can effectively optimize a policy offline using 10-20 times fewer data than prior works. Furthermore, the recursive application of BREMEN is able to achieve impressive deployment efficiency while maintaining the same or better sample efficiency, learning successful policies from scratch on simulated robotic environments with only 5-10 deployments, compared to typical values of hundreds to millions in standard RL baselines. Codes and pre-trained models are available at https://github.com/matsuolab/BREMEN. * Equal contribution.Preprint. Under review.",
    "prev": "minimising the number of times the policy used changes from one episode to the next.These works are not directly comparable because they study regret-minimisation for finite-horizon MDPs and we study BPI and PE in the discounted setting.Nevertheless, there are works on tabular MDPs (Qiao et al., 2022;Bai et al., 2019;Zhang et al., 2020), linear MDPs (Gao et al., 2021;Wang et al., 2021;Qiao & Wang, 2023) and MDPs with a linear representation for the action values (Qiao et al., 2023).",
    "curr": "The multi-batch learning model has been studied extensively for bandit algorithms (Perchet et al., 2015;Jun et al., 2016;Gao et al., 2019;Esfandiari et al., 2021;Duchi et al., 2018;Han et al., 2020;Ruan et al., 2021).In RL, it has been studied in the regret-minimisation setting for finite-horizon tabular (Zihan et al., 2022) and linear MDPs (Wang et al., 2021) and MDPs under general function approximation (Xiong et al., 2023).A closely related notion is deployment efficiency (Matsushima et al., 2021), which constrains batches to be of a fixed size consisting of trajectories from a single policy.In finite-horizon linear MDPs, it has been shown that BPI can be solved to arbitrary accuracy with a number of deployments independent of the dimension d (Huang et al., 2022;Qiao & Wang, 2023) where the deployed policy is a finite mixture of deterministic policies.Our results suggest that infinite-horizon discounted MDPs under more general linear representation of action-values are fundamentally harder since the number of deployments must scale with dimension.",
    "next": "The policy finetuning setting assumes access to an offline dataset that can be complemented with online trajectories (Xie et al., 2021) but is different from our setting since there is no adaptivity constraint in the online algorithm, i.e.",
    "query": "Are there any research papers on deployment-efficient reinforcement learning using model-based offline optimization methods?"
  },
  {
    "index": 172,
    "source_corpus_id": 44096233,
    "ref_id": "b23",
    "citation_corpus_id": 2100831,
    "start": 5974,
    "end": 5998,
    "title": "MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text",
    "abstract": "We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text.",
    "prev": "Third, we demonstrate that on synthetic data, DSMN achieves superior performance for answering questions that require visual thinking.",
    "curr": "Related Work\n\nNatural language datasets for QA: Several natural language QA datasets have been proposed to test AI systems on various reasoning abilities (Levesque et al., 2011;Richardson et al., 2013).",
    "next": "Our work differs from them in two key aspects: first, we use synthetic data instead of natural data; and second, we specialize in geometrical reasoning instead of general language understanding.",
    "query": "Are there any research papers on synthetic datasets for open-domain machine comprehension that focus on geometrical reasoning?"
  },
  {
    "index": 173,
    "source_corpus_id": 59600025,
    "ref_id": "b28",
    "citation_corpus_id": 1957433,
    "start": 1226,
    "end": 1250,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on word representation methods that combine global matrix factorization and local context window approaches, leveraging word-word cooccurrence statistics for improved semantic and syntactic modeling?"
  },
  {
    "index": 174,
    "source_corpus_id": 53729760,
    "ref_id": "b19",
    "citation_corpus_id": 205514,
    "start": 2521,
    "end": 2543,
    "title": "DEEP MULTI-SCALE VIDEO PREDICTION BEYOND MEAN SQUARE ERROR",
    "abstract": "Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical flow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still, many vision applications could benefit from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectory. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset.",
    "prev": "INTRODUCTION\n\nGenerative Adversarial Networks (GANs) (Goodfellow et al., 2014) have been able to produce photorealistic images, often indistinguishable from real images.",
    "curr": "This remarkable ability has powered many real-world applications ranging from visual recognition (Wang et al., 2017), to image manipulation (Isola et al., 2017;Zhu et al., 2017), to video prediction (Mathieu et al., 2016).",
    "next": "Since its invention in 2014, many GAN variants have been proposed (Radford et al., 2016;Zhang et al., 2018), often producing more realistic and diverse samples with better training stability.",
    "query": "Are there any research papers on video prediction techniques using GANs and multi-scale architectures with adversarial training and gradient-based loss functions?"
  },
  {
    "index": 175,
    "source_corpus_id": 220769181,
    "ref_id": "b10",
    "citation_corpus_id": 204512179,
    "start": 1924,
    "end": 1928,
    "title": "LEARNING NEARLY DECOMPOSABLE VALUE FUNC- TIONS VIA COMMUNICATION MINIMIZATION",
    "abstract": "Reinforcement learning encounters major challenges in multi-agent settings, such as scalability and non-stationarity. Recently, value function factorization learning emerges as a promising way to address these challenges in collaborative multi-agent systems. However, existing methods have been focusing on learning fully decentralized value function, which are not efficient for tasks requiring communication. To address this limitation, this paper presents a novel framework for learning nearly decomposable value functions with communication, with which agents act on their own most of the time but occasionally send messages to other agents in order for effective coordination. This framework hybridizes value function factorization learning and communication learning by introducing two information-theoretic regularizers. These regularizers are maximizing mutual information between decentralized Q functions and communication messages while minimizing the entropy of messages between agents. We show how to optimize these regularizers in a way that is easily integrated with existing value function factorization methods such as QMIX. Finally, we demonstrate that, on the StarCraft unit micromanagement benchmark, our framework significantly outperforms baseline methods and allows to cut off more than 80% communication without sacrificing the performance. The video of our experiments is available at",
    "prev": "Demonstrative videos are available at https\n\nIntroduction\n\nCooperative multi-agent reinforcement learning (MARL) has achieved great progress in recent years [1][2][3][4][5][6][7].",
    "curr": "Advances in valued-based MARL [8][9][10][11] contribute significantly to the progress, achieving state-of-the-art performance on challenging tasks, such as StarCraft II micromanagement [12].",
    "next": "However, these value-based methods present a major challenge for stability and convergence in multi-agent settings [13], which is further exacerbated in continuous action spaces.",
    "query": "Are there any research papers on methods for improving stability and convergence in value-based multi-agent reinforcement learning through communication minimization and information-theoretic regularizers, particularly in StarCraft II micromanagement tasks?"
  },
  {
    "index": 177,
    "source_corpus_id": 222341655,
    "ref_id": "b32",
    "citation_corpus_id": 199543783,
    "start": 24270,
    "end": 24291,
    "title": "Behaviour Suite for Reinforcement Learning",
    "abstract": "This paper introduces the Behaviour Suite for Reinforcement Learning, or bsuite for short. bsuite is a collection of carefully-designed experiments that investigate core capabilities of reinforcement learning (RL) agents with two objectives. First, to collect clear, informative and scalable problems that capture key issues in the design of general and efficient learning algorithms. Second, to study agent behaviour through their performance on these shared benchmarks. To complement this effort, we open source github.com/deepmind/bsuite, which automates evaluation and analysis of any agent on bsuite. This library facilitates reproducible and accessible research on the core issues in RL, and ultimately the design of superior learning algorithms. Our code is Python, and easy to use within existing projects. We include examples with OpenAI Baselines, Dopamine as well as new reference implementations. Going forward, we hope to incorporate more excellent experiments from the research community, and commit to a periodic review of bsuite from a committee of prominent researchers.",
    "prev": "it requires two independent samples of s starting from the same (s, a) due to the inner conditional expectation (Baird, 1995), whereas IV regression methods do not suffer from this issue.",
    "curr": "We evaluate DFIV on three BSuite (Osband et al., 2019) tasks: catch, mountain car, and cartpole.",
    "next": "See Section E.6.1 for a description of those tasks.",
    "query": "Are there any research papers on benchmark suites for reinforcement learning that evaluate algorithm performance on standard tasks like catch, mountain car, and cartpole?"
  },
  {
    "index": 178,
    "source_corpus_id": 252110923,
    "ref_id": "b9",
    "citation_corpus_id": 210839350,
    "start": 5670,
    "end": 5673,
    "title": "DD-PPO: LEARNING NEAR-PERFECT POINTGOAL NAVIGATORS FROM 2.5 BILLION FRAMES",
    "abstract": "We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments.DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever 'stale'), making it conceptually simple and easy to implement.In our experiments on training virtual robots to navigate in Habitat-Sim(Savva et al., 2019), DD-PPO exhibits near-linear scaling -achieving a speedup of 107x on 128 GPUs over a serial implementation.We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs.This massive-scale training not only sets the state of art on Habitat Autonomous Navigation Challenge 2019, but essentially 'solves' the task -near-perfect autonomous navigation in an unseen environment without access to a map, directly from an RGB-D camera and a GPS+Compass sensor.Fortuitously, error vs computation exhibits a power-law-like distribution; thus, 90% of peak performance is obtained relatively early (at 100 million steps) and relatively cheaply (under 1 day with 8 GPUs).Finally, we show that the scene understanding and navigation policies learned can be transferred to other navigation tasks -the analog of 'ImageNet pre-training + task-specific fine-tuning' for embodied AI.Our model outperforms ImageNet pre-trained CNNs on these transfer tasks and can serve as a universal resource (all models and code are publicly available).",
    "prev": "Second, we study how to translate the start of manipulation skills to the navigation reward, which is used to train the navigation skill to connect manipulation skills.",
    "curr": "Note that the goal position in mobile manipulation plays a very different role from that in point-goal [9,10] navigation.",
    "next": "On the one hand, the position of a target object (e.g., on the table or in the fridge) is often not directly navigable; on the other hand, a navigable position close to the goal position can be infeasible due to kinematic and collision constraints.",
    "query": "Are there any studies that explore methods for mobile manipulation navigation, focusing on addressing goal position constraints and integrating reinforcement learning techniques?"
  },
  {
    "index": 179,
    "source_corpus_id": 211069439,
    "ref_id": "b2",
    "citation_corpus_id": 11212020,
    "start": 2487,
    "end": 2510,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "INTRODUCTION\n\nText sequence transduction systems convert a given text sequence from one domain to another.",
    "curr": "These techniques can be applied to a wide range of natural language processing applications such as machine translation (Bahdanau et al., 2015), summarization (Rush et al., 2015), and dialogue response generation (Zhao et al., 2017).",
    "next": "In many cases, however, parallel corpora for the task at hand are scarce.",
    "query": "Are there any research papers on neural machine translation techniques using encoder-decoder architectures with alignment methods, particularly addressing challenges related to scarce parallel corpora?"
  },
  {
    "index": 181,
    "source_corpus_id": 232105052,
    "ref_id": "b44",
    "citation_corpus_id": 44131019,
    "start": 3421,
    "end": 3439,
    "title": "Scaling Neural Machine Translation",
    "abstract": "Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation. 1 On WMT'14 English-German translation, we match the accuracy ofVaswani et al. (2017)in under 5 hours when training on 8 GPUs and we obtain a new state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT'14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.",
    "prev": "tion, and explore its application in transformers.RFA can be used as a drop-in replacement for conventional softmax attention and offers a straightforward way of learning with recency bias through an optional gating mechanism.Experiments on language modeling and machine translation demonstrate that RFA achieves similar or better performance compared to strong transformer baselines.In the machine translation experiment, RFA decodes twice as fast as a vanilla transformer.Compared to existing efficient transformer variants, RFA is competitive in terms of both accuracy and efficiency on three long text classification datasets.Our analysis shows that RFA's efficiency gains are especially notable on long sequences, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints.",
    "curr": "INTRODUCTION\n\nTransformer architectures (Vaswani et al., 2017) have achieved tremendous success on a variety of sequence modeling tasks (Ott et al., 2018;Radford et al., 2018;Parmar et al., 2018;Devlin et al., 2019;Parisotto et al., 2020, inter alia).Under the hood, the key component is attention (Bahdanau et al., 2015), which models pairwise interactions of the inputs, regardless of their distances from each other.This comes with quadratic time and memory costs, making the transformers computationally expensive, especially for long sequences.A large body of research has been devoted to improving their time and memory efficiency (Tay et al., 2020c).Although better asymptotic complexity and prominent gains for long sequences have been achieved (Lee et al., 2019;Child et al., 2019;Beltagy et al., 2020, inter alia), in practice, many existing approaches are less well-suited for moderatelength ones: the additional computation steps required by some approaches can overshadow the time and memory they save (Kitaev et al., 2020;Wang et al., 2020;Roy et al., 2020, inter alia).",
    "next": "This work proposes random feature attention (RFA), an efficient atte",
    "query": "Are there any research papers on efficient attention mechanisms in transformers that improve computational efficiency for long sequences?"
  },
  {
    "index": 182,
    "source_corpus_id": 257102434,
    "ref_id": "b15",
    "citation_corpus_id": 13046179,
    "start": 10346,
    "end": 10372,
    "title": "A BASELINE FOR DETECTING MISCLASSIFIED AND OUT-OF-DISTRIBUTION EXAMPLES IN NEURAL NETWORKS",
    "abstract": "We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.Published as a conference paper at ICLR 2017 one method which outperforms the baseline on some (but not all) tasks. This new method evaluates the quality of a neural network's input reconstruction to determine if an example is abnormal.",
    "prev": "This function should induce a partial order over instances in X .",
    "curr": "The most common and well-known κ function for a classification model f (with softmax at its last layer) is its softmax response values -κ(x,ŷ|f ) f (x)ŷ (Cordella et al., 1995;De Stefano et al., 2000) -which is also widely accepted as a baseline in the OOD literature (Hendrycks & Gimpel, 2017;Hendrycks et al., 2021;Berger et al., 2021;Shalev et al., 2018).",
    "next": "While this is the primary κ we evaluate for the sake of simplicity, various other κ functions, which are also utilized for OOD detection, exist.",
    "query": "Are there any research papers on methods for detecting misclassified and out-of-distribution examples in neural networks using softmax probabilities or alternative κ functions?"
  },
  {
    "index": 185,
    "source_corpus_id": 225067229,
    "ref_id": "b32",
    "citation_corpus_id": 216553145,
    "start": 2479,
    "end": 2498,
    "title": "Multi-Domain Dialogue Acts and Response Co-Generation",
    "abstract": "Generating fluent and informative responses is of critical importance for task-oriented dialogue systems. Existing pipeline approaches generally predict multiple dialogue acts first and use them to assist response generation. There are at least two shortcomings with such approaches. First, the inherent structures of multi-domain dialogue acts are neglected. Second, the semantic associations between acts and responses are not taken into account for response generation. To address these issues, we propose a neural co-generation model that generates dialogue acts and responses concurrently. Unlike those pipeline approaches, our act generation module preserves the semantic structures of multi-domain dialogue acts and our response generation module dynamically attends to different acts as needed. We train the two modules jointly using an uncertainty loss to adjust their task weights adaptively. Extensive experiments are conducted on the largescale MultiWOZ dataset and the results show that our model achieves very favorable improvement over several state-of-the-art models in both automatic and human evaluations.",
    "prev": "Human evaluations show that COCO-generated conversations perfectly reflect the underlying user goal with more than 95% accuracy and are as human-like as the original conversations, further strengthening its reliability and promise to be adopted as part of the robustness evaluation of DST models.",
    "curr": "INTRODUCTION\n\nTask-oriented dialogue (TOD) systems have recently attracted growing attention and achieved substantial progress (Zhang et al., 2019b;Peng et al., 2020;Wang et al., 2020b;a), partly made possible by the construction of large-scale datasets (Budzianowski et al., 2018;Byrne et al., 2019;Rastogi et al., 2019).",
    "next": "Dialogue state tracking (DST) is a backbone of TOD systems, where it is responsible for extracting the user's goal represented as a set of slot-value pairs (e.g., (area, center), (food, British)), as illustrated in the upper part of Figure 1.",
    "query": "Are there any research papers on co-generation models for multi-domain dialogue acts and responses in task-oriented dialogue systems?"
  },
  {
    "index": 186,
    "source_corpus_id": 215814169,
    "ref_id": "b24",
    "citation_corpus_id": 59310641,
    "start": 7411,
    "end": 7414,
    "title": "PAY LESS ATTENTION WITH LIGHTWEIGHT AND DYNAMIC CONVOLUTIONS",
    "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU. 1 * Work done during an internship at Facebook. 1 Code and pre-trained models available at",
    "prev": "[23] for a review of different pruning strategies.",
    "curr": "Others have worked on lightweight architectures, by modifying existing models [24,25,26] or developing new networks, such as MobileNet [16], ShuffleNet [15], and EfficientNet [4] in vision.",
    "next": "Finally, knowledge distillation [6] has been applied to sentence representation [13,27,28,29,30], to reduce the size of a BERT model [31].",
    "query": "Are there any research papers on lightweight and efficient alternatives to self-attention mechanisms in neural networks, such as dynamic convolutions or knowledge distillation techniques for reducing model size?"
  },
  {
    "index": 187,
    "source_corpus_id": 264426451,
    "ref_id": "b38",
    "citation_corpus_id": 227209335,
    "start": 2332,
    "end": 2350,
    "title": "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024ˆ1024 images for the first time from a score-based generative model.",
    "prev": "ombined with better hyperparameter tuning, these modifications enable consistency models to achieve FID scores of 2.51 and 3.25 on CIFAR-10 and ImageNet 64 ˆ64 respectively in a single sampling step.These scores mark a 3.5ˆand 4ˆimprovement compared to prior consistency training approaches.Through two-step sampling, we further reduce FID scores to 2.24 and 2.77 on these two datasets, surpassing those obtained via distillation in both one-step and two-step settings, while narrowing the gap between consistency models and other state-of-the-art generative models.",
    "curr": "INTRODUCTION\n\nConsistency models (Song et al., 2023) are an emerging family of generative models that produce high-quality samples using a single network evaluation.Unlike GANs (Goodfellow et al., 2014), consistency models are not trained with adversarial optimization and thus sidestep the associated training difficulty.Compared to score-based diffusion models (Sohl-Dickstein et al., 2015;Song & Ermon, 2019;2020;Ho et al., 2020;Song et al., 2021), consistency models do not require numerous sampling steps to generate high-quality samples.They are trained to generate samples in a single step, but still retain important advantages of diffusion models, such as the flexibility to exchange compute for sample quality through multistep sampling, and the ability to perform zero-shot data editing.",
    "next": "We can train consistency models using either consistency distillation (CD) or consistency training (CT).The former requires pre-training a diffusion model and distilling the knowledge therein into a consistency model.The latter allows us to train consistency models directly from data, establishing them as an independent family of generative models.Previous work (Song et al., 2023) demonstrates that CD significantly outperforms CT.However, CD adds computational overhead to the training process since it requires learning a separate diffusion model.Additionally, distillation limits the sample quality of the consistency model to that of the diffu",
    "query": "Are there any research papers on score-based generative models using stochastic differential equations, comparing consistency distillation and training methods for image generation with FID metrics?"
  },
  {
    "index": 191,
    "source_corpus_id": 263829348,
    "ref_id": "b12",
    "citation_corpus_id": 209315300,
    "start": 2766,
    "end": 2786,
    "title": "REFORMER: THE EFFICIENT TRANSFORMER",
    "abstract": "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(L 2 ) to O(L log L), where L is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.Published as a conference paper at ICLR 2020We introduce the Reformer model which solves these problems using the following techniques:• Reversible layers, first introduced in Gomez et al.(2017), enable storing only a single copy of activations in the whole model, so the N factor disappears. • Splitting activations inside feed-forward layers and processing them in chunks removes the d f f factor and saves memory inside feed-forward layers.",
    "prev": "INTRODUCTION\n\nTime series forecasting, i.e., predicting future data based on historical observations, has broad realworld applications, such as health, transportation, finance and so on.",
    "curr": "In the past decade, numerous deep neural network architectures have been applied to time series modeling, including convolutional neural networks (CNN) (Bai et al., 2018), recurrent neural networks (RNN) (Siami-Namini et al., 2018), graph neural networks (GNN) (Li et al., 2018;Cao et al., 2020), and Transformers (Liu et al., 2021;Wu et al., 2021;Zhou et al., 2021;Wu et al., 2023;Zhou et al., 2022;Woo et al., 2022;Kitaev et al., 2020;Nie et al., 2023), leading to state-of-the-arts results.",
    "next": "While achieving strong prediction performance, the previous works on time series mostly benefit from the advance in sequence modeling (from RNN and GNN, to transformers) that captures temporal dependencies but overlooks a series of intricate patterns within time series data, such as seasonality, trend, and residual.",
    "query": "Are there any research papers on efficient Transformer models with techniques like locality-sensitive hashing and reversible residual layers for improving memory and computational efficiency?"
  },
  {
    "index": 194,
    "source_corpus_id": 235313882,
    "ref_id": "b19",
    "citation_corpus_id": 153313159,
    "start": 9071,
    "end": 9092,
    "title": "Sparse Sequence-to-Sequence Models",
    "abstract": "Sequence-to-sequence models are a powerful workhorse of NLP. Most variants employ a softmax transformation in both their attention mechanism and output layer, leading to dense alignments and strictly positive output probabilities. This density is wasteful, making models less interpretable and assigning probability mass to many implausible outputs. In this paper, we propose sparse sequence-to-sequence models, rooted in a new family of α-entmax transformations, which includes softmax and sparsemax as particular cases, and is sparse for any α > 1. We provide fast algorithms to evaluate these transformations and their gradients, which scale well for large vocabulary sizes. Our models are able to produce sparse alignments and to assign nonzero probability to a short list of plausible outputs, sometimes rendering beam search exact. Experiments on morphological inflection and machine translation reveal consistent gains over dense models. the encodings [h 1 , . . . , h J ], using s t as a query vector. This is done by computing token-level scores z j := s t W (z) h j , then taking a weighted averageπ j h j , where π := softmax(z).(1) the anonymous reviewers, for helpful discussion and feedback.",
    "prev": "(2019) replace F c (x) as a weighted sum of features:\nF c (x) = D j=1 x j entmax α (F c ) j = x · entmax α (F c ).",
    "curr": "(2)\nHere F c ∈ R D are the logits for which features to choose, and entmax α (Peters et al., 2019) is the entmax transformation which works like a sparse version of softmax such that the sum of the output equals to 1.",
    "next": "They also replace the I with entmoid which works like a sparse sigmoid that has output values between 0 and 1.",
    "query": "Are there any research papers on sparse sequence-to-sequence models using entmax transformations for attention mechanisms and output layers in natural language processing tasks?"
  },
  {
    "index": 198,
    "source_corpus_id": 253255190,
    "ref_id": "b2",
    "citation_corpus_id": 54203451,
    "start": 3304,
    "end": 3326,
    "title": "SYSTEMATIC GENERALIZATION: WHAT IS REQUIRED AND CAN IT BE LEARNED?",
    "abstract": "Numerous models for grounded language understanding have been recently proposed, including (i) generic models that can be easily adapted to any given task with little adaptation and (ii) intuitively appealing modular models that require background knowledge to be instantiated. We compare both types of models in how much they lend themselves to a particular form of systematic generalization. Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them. Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected. We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization. We find that end-to-end methods from prior work often learn a wrong layout and a spurious parametrization that do not facilitate systematic generalization. Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors.",
    "prev": "In practice, when trained on language data, do transformers instead constrain their computation to look equivalent to a tree-structured bottom-up computation?",
    "curr": "While generalization tests on benchmarks (Lake & Baroni, 2018;Bahdanau et al., 2019;Hupkes et al., 2019;Kim & Linzen, 2020, among others) assess if a transformer's behavior is aligned with tree-like models, they do not measure if the transformer's computation is tree-structured, largely because model behavior on benchmarks could entirely be due to orthogonal properties of the dataset (Patel et al., 2022).",
    "next": "Thus, to understand if transformers implement tree-structured computations, the approach we take is based on directly approximating them with a separate, tree-structured computation.",
    "query": "Are there any research papers on evaluating whether transformers implement tree-structured computations and their impact on systematic generalization in language models?"
  },
  {
    "index": 199,
    "source_corpus_id": 1706438,
    "ref_id": "b3",
    "citation_corpus_id": 8712237,
    "start": 1373,
    "end": 1392,
    "title": "Distributional Semantics in Technicolor",
    "abstract": "Our research aims at building computational models of word meaning that are perceptually grounded. Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models. Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic relatedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words. Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on perceptually grounded models of word meaning that integrate visual and textual distributional semantics, particularly for words with visual correlates?"
  },
  {
    "index": 200,
    "source_corpus_id": 229923250,
    "ref_id": "b1",
    "citation_corpus_id": 11212020,
    "start": 2344,
    "end": 2367,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "arXiv, 2020.",
    "curr": "INTRODUCTION\n\nSequence-to-Sequence (Seq2Seq) learning  has advanced the state of the art in various natural language processing (NLP) tasks, such as machine translation (Bahdanau et al., 2015;Vaswani et al., 2017;Wu et al., 2019), text summarization (Wang et al., 2019b;Zhang et al., 2020), and grammatical error correction (Kiyono et al., 2019;Kaneko et al., 2020).",
    "next": "Seq2Seq models are generally implemented with an encoder-decoder framework, in which a multi-layer encoder summarizes a source sequence into a sequence of representation and another multi-layer decoder produces the target sequence conditioned on the encoded representation.",
    "query": "Are there any research papers on neural machine translation using encoder-decoder models with attention mechanisms for alignment and translation performance improvement?"
  },
  {
    "index": 202,
    "source_corpus_id": 219969405,
    "ref_id": "b19",
    "citation_corpus_id": 71145737,
    "start": 2905,
    "end": 2909,
    "title": "Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples",
    "abstract": "Few-shot classification refers to learning a classifier for new classes given only a few examples. While a plethora of models have emerged to tackle this recently, we find the current procedure and datasets that are used to systematically assess progress in this setting lacking. To address this, we propose META-DATASET: a new benchmark for training and evaluating few-shot classifiers that is large-scale, consists of multiple datasets, and presents more natural and realistic tasks. The aim is to measure the ability of state-ofthe-art models to leverage diverse sources of data to achieve higher generalization, and to evaluate that generalization ability in a more challenging setting. We additionally measure robustness of current methods to variations in the number of available examples and the number of classes. Finally our extensive empirical evaluation leads us to identify weaknesses in Prototypical Networks and MAML, two popular few-shot classification methods, and to propose a new method, Proto-MAML, which achieves improved performance on our benchmark.",
    "prev": "Recently, Triantafillou et al.",
    "curr": "[20] proposed a benchmark for multi-domain few-shot classification, Meta-Dataset, and highlighted some of the challenges that current methods face when training data is heterogeneous.",
    "next": "Crucially, they found that methods which trained on all available domains would normally obtain improved performance on some domains at the expense of others.",
    "query": "Are there any research papers on multi-domain few-shot classification benchmarks that address challenges in heterogeneous training data and evaluate methods like Prototypical Networks and MAML?"
  },
  {
    "index": 203,
    "source_corpus_id": 59279266,
    "ref_id": "b3",
    "citation_corpus_id": 17682909,
    "start": 4146,
    "end": 4166,
    "title": "Spectral Networks and Deep Locally Connected Networks on Graphs",
    "abstract": "Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for lowdimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.",
    "prev": "Spectral methods leverage graph Fourier transform to convert signals defined in vertex domain into spectral domain, e.g., the space spanned by the eigenvectors of the graph Laplacian matrix, and then filter is defined in spectral domain, maintaining the weight sharing property of CNN.",
    "curr": "As the pioneering work of spectral methods, spectral CNN (Bruna et al., 2014) exploited graph data with the graph Fourier transform to implement convolution operator using convolution theorem.",
    "next": "Some subsequent works make spectral methods spectrum-free (Defferrard et al., 2016;Kipf & Welling, 2017;Khasanova & Frossard, 2017), achieving locality in spatial domain and avoiding high computational cost of the eigendecomposition of Laplacian matrix.",
    "query": "Are there any research papers on spectral convolutional neural networks for graph data and their generalizations to improve computational efficiency?"
  },
  {
    "index": 204,
    "source_corpus_id": 252815987,
    "ref_id": "b13",
    "citation_corpus_id": 3480671,
    "start": 6503,
    "end": 6520,
    "title": "NON-AUTOREGRESSIVE NEURAL MACHINE TRANSLATION",
    "abstract": "Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English-German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English-Romanian.",
    "prev": "For example, including an extra mathematical symbol can push everything one line further down.",
    "curr": "Some datasets also have long-term symbolic dependencies, which may be difficult for non-sequential models to handle, analogous to some of the challenges observed in nonautoregressive machine translation (Gu et al., 2018).",
    "next": "Generation with Diffusion Models Denoising diffusion probabilistic models (DDPM) (Ho et al., 2020) parameterize a probabilistic distribution P (y 0 |x) as a Markov chain P (y t−1 |y t ) with an initial distribution P (y T ).",
    "query": "Are there any research papers on non-autoregressive models for machine translation that address challenges with long-term symbolic dependencies and parallel generation strategies?"
  },
  {
    "index": 205,
    "source_corpus_id": 209314627,
    "ref_id": "b0",
    "citation_corpus_id": 3651422,
    "start": 47962,
    "end": 47982,
    "title": "Workshop track -ICLR 2018 UNCERTAINTY ESTIMATION VIA STOCHASTIC BATCH NORMALIZATION",
    "abstract": "In this work, we investigate Batch Normalization technique and propose its probabilistic interpretation. We propose a probabilistic model and show that Batch Normalization maximazes the lower bound of its marginalized log-likelihood. Then, according to the new probabilistic model, we design an algorithm which acts consistently during train and test. However, inference becomes computationally inefficient. To reduce memory and computational cost, we propose Stochastic Batch Normalization -an efficient approximation of proper inference procedure. This method provides us with a scalable uncertainty estimation technique. We demonstrate the performance of Stochastic Batch Normalization on popular architectures (including deep convolutional architectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets.",
    "prev": "It does not provide a consistent ranking of different ensembling techniques.",
    "curr": "(\nSrivastava et al., 2014;Gal & Ghahramani, 2016), variational inference(Blundell et al., 2015;  Kingma et al., 2015; Louizos & Welling, 2017), batch normalization(Ioffe & Szegedy, 2015; Teye  et al., 2018;Atanov et al., 2019), Laplace approximation(Ritter et al., 2018)  and many more.Snapshot-based methods aim to obtain sets of weights for deep learning models and then to average the predictions across these weights.",
    "next": "The weights can be trained independently (e.g.",
    "query": "Are there any research papers on uncertainty estimation in deep learning models using probabilistic batch normalization or stochastic ensembling techniques?"
  },
  {
    "index": 206,
    "source_corpus_id": 203737303,
    "ref_id": "b24",
    "citation_corpus_id": 6212000,
    "start": 2106,
    "end": 2125,
    "title": "UNDERSTANDING DEEP LEARNING REQUIRES RE- THINKING GENERALIZATION",
    "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on the generalization of deep learning models that challenge traditional explanations involving model properties or regularization techniques?"
  },
  {
    "index": 207,
    "source_corpus_id": 222208810,
    "ref_id": "b12",
    "citation_corpus_id": 969555,
    "start": 13384,
    "end": 13406,
    "title": "Pointing the Unknown Words",
    "abstract": "The problem of rare and unknown words is an important issue that can potentially effect the performance of many NLP systems, including both traditional countbased and deep learning models. We propose a novel way to deal with the rare and unseen words for the neural network models with attention. Our model uses two softmax layers in order to predict the next word in conditional language models: one of the softmax layers predicts the location of a word in the source sentence, and the other softmax layer predicts a word in the shortlist vocabulary. The decision of which softmax layer to use at each timestep is adaptively made by an MLP which is conditioned on the context. We motivate this work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known. Using our proposed model, we observe improvements in two tasks, neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset.",
    "prev": "The decoder generates an action sentence a t token-by-token to interact with the game.",
    "curr": "The encoder and decoder are based on a Transformer Seq2Seq model with pointer softmax mechanism (Gulcehre et al., 2016 When playing a game, an agent might get stuck at certain states due to various failures (e.g., action is grammatically incorrect, wrong object name).",
    "next": "The observation for a failed action does not contain any useful feedback, so a fully deterministic model tends to produce the same (wrong) action repeatedly.",
    "query": "Are there any studies that propose methods for handling rare and unknown words in NLP using dual-softmax layers with attention mechanisms and context-based decision making, particularly in tasks like machine translation and text summarization?"
  },
  {
    "index": 211,
    "source_corpus_id": 260126025,
    "ref_id": "b39",
    "citation_corpus_id": 249538336,
    "start": 7152,
    "end": 7156,
    "title": "ESCHER: ESCHEWING IMPORTANCE SAMPLING IN GAMES BY COMPUTING A HISTORY VALUE FUNCTION TO ESTIMATE REGRET",
    "abstract": "Recent techniques for approximating Nash equilibria in very large games leverage neural networks to learn approximately optimal policies (strategies). One promising line of research uses neural networks to approximate counterfactual regret minimization (CFR) or its modern variants. DREAM, the only current CFR-based neural method that is model free and therefore scalable to very large games, trains a neural network on an estimated regret target that can have extremely high variance due to an importance sampling term inherited from Monte Carlo CFR (MCCFR). In this paper we propose an unbiased model-free method that does not require any importance sampling. Our method, ESCHER, is principled and is guaranteed to converge to an approximate Nash equilibrium with high probability. We show that the variance of the estimated regret of ESCHER is orders of magnitude lower than DREAM and other baselines. We then show that ESCHER outperforms the prior state of the art-DREAM and neural fictitious self play (NFSP)-on a number of games and the difference becomes dramatic as game size increases. In the very large game of dark chess, ESCHER is able to beat DREAM and NFSP in a head-to-head competition over 90% of the time.",
    "prev": "There are a number of related deep reinforcement learning methods for two-player zero-sum games.",
    "curr": "CFR-based techniques such as Deep CFR [4], DREAM [62], and ESCHER [40], use deep reinforcement learning to approximate CFR.",
    "next": "Policy-gradient techniques such as RPG [60], NeuRD [23], Friction-FoReL [53; 52], and MMD [59], approximate Nash equilibrium via modified actor-critic algorithms.",
    "query": "Are there any research papers on model-free methods for approximating Nash equilibria in large two-player zero-sum games using deep reinforcement learning, particularly those that avoid importance sampling or employ alternative regret estimation techniques?"
  },
  {
    "index": 213,
    "source_corpus_id": 226237047,
    "ref_id": "b24",
    "citation_corpus_id": 40100965,
    "start": 1791,
    "end": 1812,
    "title": "Universal Language Model Fine-tuning for Text Classification",
    "abstract": "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100× more data. We opensource our pretrained models and code 1 .",
    "prev": "We also demonstrate that the new objective leads to models that are more robust to different levels of noise in the training data, and can generalize better to related tasks with limited labeled task data.",
    "curr": "INTRODUCTION\n\nState-of-the-art for most existing natural language processing (NLP) classification tasks is currently achieved by systems that are first pre-trained on auxiliary language modeling tasks and then fine-tuned on the task of interest with cross-entropy loss (Radford et al., 2019;Howard & Ruder, 2018;Liu et al., 2019;Devlin et al., 2019).",
    "next": "Although commonly used, cross-entropy loss -the KL-divergence between one-hot vectors of labels and the distribution of model's output logits -has several shortcomings.",
    "query": "Are there any studies that explore transfer learning methods for improving robustness and generalization in NLP text classification with limited labeled data?"
  },
  {
    "index": 214,
    "source_corpus_id": 52900371,
    "ref_id": "b10",
    "citation_corpus_id": 65455367,
    "start": 29042,
    "end": 29061,
    "title": "ON THE CONVERGENCE OF ADAM AND BEYOND",
    "abstract": "Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous analysis of ADAM algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with \"long-term memory\" of past gradients, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.",
    "prev": "The figure 2 depicts trajectories for one particular seed, while the figure 3 plots means and error bars over these distinct seeds.",
    "curr": "C ADDITIONAL SIMULATIONS C.1 ADAM VERSUS AMSGRAD Plotted in the figure S1 are the comparisons between Adam (Kingma & Ba, 2014) and AMS-Grad (J. Reddi et al., 2018) algorithms with the default hyperparameters α = 10 −3 , (β 1 , β 2 ) = (0.9, 0.999), and = 10 −8 .",
    "next": "The AMSGrad algorithm marginally outperforms the Adam algorithm for the tasks at hand and thus the results with the AMSGrad are presented in the main text.",
    "query": "Are there any research papers on the convergence issues of Adam optimization algorithms and methods to address them, such as AMSGrad or variants with long-term memory of past gradients?"
  },
  {
    "index": 215,
    "source_corpus_id": 226254365,
    "ref_id": "b22",
    "citation_corpus_id": 202712906,
    "start": 2743,
    "end": 2746,
    "title": "Published as a conference paper at ICLR 2020 RAPID LEARNING OR FEATURE REUSE? TOWARDS UNDERSTANDING THE EFFECTIVENESS OF MAML",
    "abstract": "An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains -is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse, with the meta-initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of the underlying neural network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly. * Equal contribution arXiv:1909.09157v2 [cs.LG] . Recasting gradientbased meta-learning as hierarchical bayes. arXiv preprint arXiv:1801.08930, 2018.James Harrison, Apoorva Sharma, and Marco Pavone. Meta-learning priors for efficient online bayesian regression. arXiv preprint arXiv:1807.08912, 2018. . Gradient-based meta-learning with learned layerwise metric and subspace. arXiv preprint arXiv:1801.05558, 2018.",
    "prev": "Introduction\n\nTraining, regularizing, and understanding complex neural network models is challenging.",
    "curr": "There remain central open questions on making training faster and more data-efficient [11,23,24], ensuring better generalisation [32] and improving transparency and robustness [2,20].",
    "next": "A promising approach for addressing these questions is learning to teach [35], in which learned auxiliary information about a task is provided to a neural network to inform the training process and help downstream objectives.",
    "query": "Are there any research papers on the effectiveness of MAML in few-shot learning, particularly focusing on whether its success is due to rapid learning or feature reuse?"
  },
  {
    "index": 216,
    "source_corpus_id": 245836975,
    "ref_id": "b25",
    "citation_corpus_id": 5959482,
    "start": 3603,
    "end": 3625,
    "title": "Efficient Estimation of Word Representations in Vector Space",
    "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities. arXiv:1301.3781v3 [cs.CL] 7 Sep 2013 1 The test set is available at www.fit.vutbr.cz/˜imikolov/rnnlm/word-test.v1.txt 2",
    "prev": "Zero-shot methods, on the other hand, commonly leverage word embeddings to discover or generate related features between seen and unseen classes (Bucher et al., 2019;Gu et al., 2020) without the need for additional annotations.",
    "curr": "Existing works in this space use standard word embeddings (Mikolov et al., 2013) and focus on the image encoder.",
    "next": "In this work, we present a simple approach to leveraging modern language models to increase the flexibility and generality of semantic segmentation models.",
    "query": "Are there any research papers on leveraging modern language models to enhance semantic segmentation models with improved flexibility and generality?"
  },
  {
    "index": 218,
    "source_corpus_id": 52890982,
    "ref_id": "b12",
    "citation_corpus_id": 3338083,
    "start": 1887,
    "end": 1912,
    "title": "Published as a conference paper at ICLR 2018 SYNTHESIZING REALISTIC NEURAL POPULATION ACTIVITY PATTERNS USING GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "The ability to synthesize realistic patterns of neural activity is crucial for studying neural information processing. Here we used the Generative Adversarial Networks (GANs) framework to simulate the concerted activity of a population of neurons. We adapted the Wasserstein-GAN variant to facilitate the generation of unconstrained neural population activity patterns while still benefiting from parameter sharing in the temporal domain. We demonstrate that our proposed GAN, which we termed Spike-GAN, generates spike trains that match accurately the first-and second-order statistics of datasets of tens of neurons and also approximates well their higher-order statistics. We applied Spike-GAN to a real dataset recorded from salamander retina and showed that it performs as well as state-ofthe-art approaches based on the maximum entropy and the dichotomized Gaussian frameworks. Importantly, Spike-GAN does not require to specify a priori the statistics to be matched by the model, and so constitutes a more flexible method than these alternative approaches. Finally, we show how to exploit a trained Spike-GAN to construct 'importance maps' to detect the most relevant statistical structures present in a spike train. Spike-GAN provides a powerful, easy-to-use technique for generating realistic spiking neural activity and for describing the most relevant features of the large-scale neural population recordings studied in modern systems neuroscience.",
    "prev": "However, audio signals have high temporal resolution, and strategies that learn such a latent representation must perform effectively in high dimensions.",
    "curr": "Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are one such unsupervised strategy for mapping low-dimensional latent vectors to high-dimensional data.",
    "next": "The potential advantages of GAN-based approaches to audio synthesis are numerous.",
    "query": "Are there any research papers on generative adversarial networks for synthesizing realistic neural population activity patterns?"
  },
  {
    "index": 220,
    "source_corpus_id": 256416405,
    "ref_id": "b21",
    "citation_corpus_id": 3488815,
    "start": 4363,
    "end": 4383,
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. * Authors ordered alphabetically.",
    "prev": "• We compare interpretations of the proof features for standard DNNs and state-of-the-art robustly trained DNNs for the MNIST and CIFAR10 datasets.",
    "curr": "We observe that the proof features corresponding to the standard networks rely on spurious input features while the proofs of adversarially trained DNNs (Madry et al., 2018) filter out some of the spurious features.",
    "next": "In contrast, the networks trained with certifiable training  produce proofs that do not rely on any spurious features but they also miss out on some meaningful features.",
    "query": "Are there any research papers on adversarial training methods for improving neural network robustness and their impact on spurious feature reliance and security guarantees?"
  },
  {
    "index": 221,
    "source_corpus_id": 249097375,
    "ref_id": "b6",
    "citation_corpus_id": 211146562,
    "start": 6732,
    "end": 6749,
    "title": "Published as a conference paper at ICLR 2020 DIVIDEMIX: LEARNING WITH NOISY LABELS AS SEMI-SUPERVISED LEARNING",
    "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at",
    "prev": "Semi-supervised learning is also popular and effective on learning with noisy labels in recent years.",
    "curr": "Some works [Li et al., 2020a, Nguyen et al., 2020 first perform clustering on the sample loss and divide the samples into clean ones and noisy ones.",
    "next": "Then drop the labels of the \"noisy samples\" and perform semi-supervised learning on all the samples.",
    "query": "Are there any studies that explore semi-supervised learning techniques for handling noisy labels by clustering samples based on loss and dynamically separating clean and noisy data?"
  },
  {
    "index": 224,
    "source_corpus_id": 246607791,
    "ref_id": "b6",
    "citation_corpus_id": 213896662,
    "start": 4842,
    "end": 4858,
    "title": "Published as a conference paper at ICLR 2020 GRADIENTS AS FEATURES FOR DEEP REPRESENTATION LEARNING",
    "abstract": "We address the challenging problem of deep representation learning -the efficient adaption of a pre-trained deep network to different tasks. Specifically, we propose to explore gradient-based features. These features are gradients of the model parameters with respect to a task-specific loss given an input sample. Our key innovation is the design of a linear model that incorporates both gradient and activation of the pre-trained network. We demonstrate that our model provides a local linear approximation to an underlying deep model, and discuss important theoretical insights. Moreover, we present an efficient algorithm for the training and inference of our model without computing the actual gradients. Our method is evaluated across a number of representation-learning tasks on several datasets and using different network architectures. Strong results are obtained in all settings, and are well-aligned with our theoretical insights 1 .arXiv:2004.05529v1 [cs.LG] 12 Apr 2020Published as a conference paper at ICLR 2020 is evaluated across tasks, datasets and network architectures and compared against a set of baseline methods. We observe empirically that our model with the gradient features outperforms the traditional activation-based logistic regressor by a significant margin in all settings. Moreover, our model compares favorably against fine-tuning of network parameters.Our main contributions are thus summarized as follows.• We propose a novel representation-learning method. At the core of our method lies in a linear model that builds on gradients of model parameters as the feature representation. • From a theoretical perspective, we claim that our linear model provides a local approximation of fine-tuning an underlying deep model. From a practical perspective, we devise an efficient and scalable algorithm for the training and inference of our method. • We demonstrate strong results of our method across various representation-learning tasks, different network architectures and several datasets. Furthermore, these empirical results are wellaligned with our theoretical insight.",
    "prev": "Furthermore, although that GANs and VAEs are known to be able to generate high-quality samples from the data distribution, there is no strong evidence that they encode explicit layerwise representations to similar quality as in supervised learning models, which implies that there does not exist a natural way to explicitly extract a representation from intermediate layer activations in unsupervisedly pre-trained generative models.",
    "curr": "Additionally, layer activations alone do not suffice to reach the full power of learned representations hidden in neural network models, as shown in recent works (Mu et al., 2020) that incorporating additional gradients-based features into representation leads to substantial improvement over solely using activations-based features.",
    "next": "In light of these constraints, we are interested in the question: is there a principled method for representation extraction beyond layer activations?",
    "query": "Are there any research papers on methods for deep representation learning that incorporate gradient-based features alongside layer activations to improve model performance?"
  },
  {
    "index": 225,
    "source_corpus_id": 257079072,
    "ref_id": "b19",
    "citation_corpus_id": 30535508,
    "start": 15896,
    "end": 15917,
    "title": "LEARNING SPARSE NEURAL NETWORKS THROUGH L 0 REGULARIZATION",
    "abstract": "We propose a practical method for L 0 norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of L 0 regularization. However, since the L 0 norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected L 0 norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the hard concrete distribution for the gates, which is obtained by \"stretching\" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.",
    "prev": "As opposed to when using a straight through estimator (Qiao et al., 2021), non-zero gradients are ensured during the backward pass.",
    "curr": "To overcome training limitations, we use a hard concrete distribution (Qiao et al., 2021;Louizos et al., 2018).",
    "next": "It rescales the weights and the random variable introduced during training prevents from obtaining local minima (Appendix B).",
    "query": "Are there any research papers on methods for L 0 regularization in neural networks that use stochastic gates or hard concrete distributions to achieve sparsity?"
  },
  {
    "index": 226,
    "source_corpus_id": 262083735,
    "ref_id": "b9",
    "citation_corpus_id": 212747810,
    "start": 20442,
    "end": 20466,
    "title": "Calibration of Pre-trained Transformers",
    "abstract": "Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT (Devlin et al., 2019)  and RoBERTa  (Liu et al., 2019)  in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging outof-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pretrained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5× lower;(2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain. 1",
    "prev": "onstructed by the meteorological community.Hallenbeck (1920), for example, presents the performance of a certain rain forecasting method by aggregating results over 6 months into a table: Among the days forecast to have between 10%−20% chance of rain, the table records the true fraction of days which were rainy -and similarly for every forecast interval.This early account of calibration already applies the practice of binning-discretizing predictions into bins, and estimating frequencies conditional on each bin.Plots of these tables turned into binned reliability diagrams (Murphy and Winkler, 1977;DeGroot and Fienberg, 1983), which was recently popularized in the machine learning community by a series of works including Zadrozny and Elkan (2001); Niculescu-Mizil and Caruana (2005); Guo et al.",
    "curr": "(2017).Binned reliability diagrams continue to be used in studies of calibration in machine learning, including in the GPT-4 tech report (Guo et al., 2017;Nixon et al., 2019;Minderer et al., 2021;Desai and Durrett, 2020;?",
    "next": ";OpenAI, 2023).",
    "query": "Are there any research papers on the calibration of pre-trained transformers in natural language processing using binned reliability diagrams and techniques like temperature scaling or label smoothing?"
  },
  {
    "index": 227,
    "source_corpus_id": 253107476,
    "ref_id": "b20",
    "citation_corpus_id": 3144218,
    "start": 2476,
    "end": 2498,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "* Equal contribution.",
    "curr": "INTRODUCTION\n\nDeep learning has made tremendous advances in the past decade, leading to state-of-the-art performance on various learning tasks such as computer vision (He et al., 2016), natural language processing (Devlin et al., 2019) and graph learning (Kipf & Welling, 2017).",
    "next": "While some progress has been made regarding the theoretical understanding of these deep models (Arora et al., 2018;Bartlett et al., 2019;Neyshabur et al., 2015;Dziugaite & Roy, 2017), the considered settings are unfortunately often very restrictive and the insights made are only qualitative or very loose.",
    "query": "Are there any research papers on scalable semi-supervised classification methods using graph convolutional networks for graph-structured data?"
  },
  {
    "index": 230,
    "source_corpus_id": 220961494,
    "ref_id": "b68",
    "citation_corpus_id": 24069181,
    "start": 5767,
    "end": 5790,
    "title": "Published as a conference paper at ICLR 2017 DECOMPOSING MOTION AND CONTENT FOR NATURAL VIDEO SEQUENCE PREDICTION",
    "abstract": "We propose a deep neural network for the prediction of future frames in natural video sequences. To effectively handle complex evolution of pixels in videos, we propose to decompose the motion and content, two key components generating dynamics in videos. Our model is built upon the Encoder-Decoder Convolutional Neural Network and Convolutional LSTM for pixel-level prediction, which independently capture the spatial layout of an image and the corresponding temporal dynamics. By independently modeling motion and content, predicting the next frame reduces to converting the extracted content features into the next frame content by the identified motion features, which simplifies the task of prediction. Our model is end-to-end trainable over multiple time steps, and naturally learns to decompose motion and content without separate training. We evaluate the proposed network architecture on human activity videos using KTH, Weizmann action, and UCF-101 datasets. We show state-of-the-art performance in comparison to recent approaches. To the best of our knowledge, this is the first end-to-end trainable network architecture with motion and content separation to model the spatio-temporal dynamics for pixel-level future prediction in natural videos. * This work was done while SH and XL were visiting the University of Michigan.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on deep learning methods for decomposing motion and content in natural video sequences to predict future frames?"
  },
  {
    "index": 231,
    "source_corpus_id": 29154793,
    "ref_id": "b12",
    "citation_corpus_id": 26100519,
    "start": 8200,
    "end": 8204,
    "title": "DEEP VOICE 3: 2000-SPEAKER NEURAL TEXT-TO-SPEECH",
    "abstract": "We present Deep Voice 3, a fully-convolutional attention-based neural textto-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training ten times faster. We scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers. In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods. We also describe how to scale inference to ten million queries per day on one single-GPU server.2. We show that our architecture trains quickly and scales to the LibriSpeech dataset(Panayotov et al., 2015), which consists of nearly 820 hours of audio data from 2484 speakers.3. We demonstrate that we can generate monotonic attention behavior, avoiding error modes commonly occurred in speech synthesis. 4. We compare the quality of several waveform synthesis methods for a single speaker, including WORLD (Morise et al., 2016), Griffin-Lim (Griffin & Lim, 1984), and WaveNet (Oord et al., 2016). 5. We describe the implementation of an inference kernel for Deep Voice 3, which can serve up to ten million queries per day on one single-GPU server. * Authors listed in reverse alphabetical order. † These authors contributed to this work while members of Baidu Research. Under review as a conference paper at ICLR 2018 2 RELATED WORK Our work builds upon the state-of-the-art in neural speech synthesis and attention-based sequenceto-sequence learning. Several recent works tackle the problem of synthesizing speech with neural networks, including Deep Voice 1 (Arık et al., 2017), Deep Voice 2 (Arık et al., 2017), Tacotron (Wang et al., 2017), Char2Wav (Sotelo et al., 2017), VoiceLoop (Taigman et al., 2017), SampleRNN (Mehri et al., 2017), and WaveNet (Oord et al., 2016). Deep Voice 1 & 2 retain the traditional structure of TTS pipelines, separating grapheme-to-phoneme conversion, duration and frequency prediction, and waveform synthesis. In contrast to Deep Voice 1 & 2, Deep Voice 3 employs an attention-based sequenceto-sequence model, yielding a more compact architecture. Similar to Deep Voice 3, Tacotron and Char2Wav are the two proposed sequence-to-sequence models for neural TTS. Tacotron is a neural text-to-spectrogram conversion model, used with Griffin-Lim for spectrogram-to-waveform synthesis. Char2Wav predicts the parameters of WORLD vocoder (Morise et al., 2016) and uses a Sam-pleRNN conditioned upon WORLD parameters for waveform generation. In contrast to Char2Wav and Tacotron, Deep Voice 3 avoids Recurrent Neural Networks (RNNs) 1 to speed up training and alleviates several challenging error modes that attention models fall into. Thus, Deep Voice 3 makes attention-based TTS feasible for a production TTS system with no compromise on accuracy. Finally, WaveNet and SampleRNN are proposed as neural vocoder models for waveform synthesis. It is also worth noting that there are numerous alternatives for high-quality hand-engineered vocoders in the literature, such as STRAIGHT(Kawahara et al., 1999), Vocaine (Agiomyrgiannakis, 2015), and WORLD (Morise et al., 2016). Deep Voice 3 adds no novel vocoder, but has the potential to be integrated with different waveform synthesis methods with slight modifications of its architecture.",
    "prev": "In [12], the authors have used it for denoising waveforms by predicting the middle ground-truth sample from its noisy input support.",
    "curr": "Recent contributions in Text-To-Speech(TTS) [13,14] have successfully conditioned wavenet on linguistic and acoustic features to obtain state of the art performance.",
    "next": "In our encoder-decoder architecture, we use WaveNet as the output of the decoder, and backpropagate through it down to the encoder.",
    "query": "Are there any research papers on attention-based neural text-to-speech systems that utilize WaveNet for waveform synthesis and compare different methods for large-scale multi-speaker datasets?"
  },
  {
    "index": 232,
    "source_corpus_id": 247244739,
    "ref_id": "b28",
    "citation_corpus_id": 604334,
    "start": 1399,
    "end": 1402,
    "title": "Intriguing properties of neural networks",
    "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
    "prev": "E for details).",
    "curr": "Today's best-performing classifiers are vulnerable to adversarial attacks [17,46] and exhibit high robust error : for many inputs, their predictions change under adversarial perturbations, even though the true class stays the same.",
    "next": "For example, in image classification tasks, we distinguish between two categories of such attacks that are contentpreserving [16] (or consistent [38]) if their strength is limited -perceptible and imperceptible perturbations.",
    "query": "Are there any research papers on adversarial vulnerabilities in neural networks and the impact of imperceptible perturbations on their classification accuracy?"
  },
  {
    "index": 236,
    "source_corpus_id": 258686472,
    "ref_id": "b42",
    "citation_corpus_id": 208910151,
    "start": 6960,
    "end": 6979,
    "title": "Optimism in Reinforcement Learning with Generalized Linear Function Approximation",
    "abstract": "We design a new provably efficient algorithm for episodic reinforcement learning with generalized linear function approximation. We analyze the algorithm under a new expressivity assumption that we call \"optimistic closure,\" which is strictly weaker than assumptions from prior analyses for the linear setting. With optimistic closure, we prove that our algorithm enjoys a regret bound ofÕ(where d is the dimensionality of the state-action features and T is the number of episodes. This is the first statistically and computationally efficient algorithm for reinforcement learning with generalized linear functions.",
    "prev": "Related Work\n\nRL with linear function approximation To make MDPs with large state space amenable for provable RL, there has been an explosion of works relying on MDP classes with various linear structures (Jiang et al., 2017;Sun et al., 2019;Du et al., 2021;Jin et al., 2021).",
    "curr": "Among different assumptions made in recent work Wang et al., 2020b;Jin et al., 2020b;Du et al., 2019;Zanette et al., 2020;Ayoub et al., 2020;Jia et al., 2020;Weisz et al., 2021;Zhou et al., 2021;He et al., 2022b;Zhou and Gu, 2022;He et al., 2022a), we consider the linear mixture MDP setting Ayoub et al., 2020;Zhou et al., 2021;Zhang et al., 2021a;He et al., 2022b), where the transition kernel is a linear combination of d given models.",
    "next": "More specifically, we focus on the adversarial linear mixture MDP of He et al.",
    "query": "Are there any research papers on reinforcement learning with generalized linear function approximation, particularly those addressing linear mixture MDPs and optimistic closure assumptions?"
  },
  {
    "index": 237,
    "source_corpus_id": 264812826,
    "ref_id": "b4",
    "citation_corpus_id": 52967399,
    "start": 2040,
    "end": 2061,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "xpressiveness limitations.We show that despite the continuous embedding space being more expressive than the discrete token space, soft-prompting and prefix-tuning are strictly less expressive than full fine-tuning, even with the same number of learnable parameters.Concretely, context-based fine-tuning cannot change the relative attention pattern over the content and can only bias the outputs of an attention layer in a fixed direction.This suggests that while techniques like prompting, in-context learning, soft prompting, and prefixtuning can effectively elicit skills present in the pretrained model, they cannot learn novel tasks that require new attention patterns.",
    "curr": "INTRODUCTION\n\nLanguage model advances are largely driven by larger models and more training data (Kaplan et al., 2020;Rae et al., 2021).Training cutting-edge models is out of reach for most academic researchers, small enterprises, and individuals, and it has become common to instead fine-tune open-source pretrained models (Devlin et al., 2019;Min et al., 2021).Yet, due to escalating computational demands, even fine-tuning of the larger models has become prohibitively expensive (Lialin et al., 2023).",
    "next": "As a result, there is an acute need for more efficient fine-tuning methods, either by sparsely modifying the parameters of the model or modifying its input context.Examples of the first type include adapter modules which introduce a few trainable layers to modify the behaviour of the frozen pretrained network (Rebuffi et al., 2017;Houlsby et al., 2019;Hu et al., 2023).One can also use low-rank updates, which also results in a reduced number of trainable parameters (Hu et al., 2021).",
    "query": "Are there any studies on efficient fine-tuning methods for language models, such as adapter modules or low-rank updates, to address computational costs and improve parameter efficiency?"
  },
  {
    "index": 241,
    "source_corpus_id": 254854553,
    "ref_id": "b31",
    "citation_corpus_id": 3488815,
    "start": 3190,
    "end": 3210,
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. * Authors ordered alphabetically.",
    "prev": "Among a variety of robustness evaluation methods, gradient-based adversarial attack generation makes a tremendous success in the computer vision (CV) domain (Croce & Hein, 2020;Dong et al., 2020).",
    "curr": "For example, the projected gradient descent (PGD)-based methods have been widely used to benchmark the adversarial robustness of CV models (Madry et al., 2018;Zhang et al., 2019b;Shafahi et al., 2019;Wong et al., 2020;Zhang et al., 2019a;Athalye et al., 2018).",
    "next": "However, in the natural language processing (NLP) area, the predominant robustness evaluation tool belongs to query-based attack generation methods Garg & Ramakrishnan, 2020;Li et al., 2019), which do not make the full use of gradient information.",
    "query": "Are there any research papers on methods for improving adversarial robustness in deep learning models through robust optimization and gradient-based attack generation techniques?"
  },
  {
    "index": 242,
    "source_corpus_id": 252531169,
    "ref_id": "b0",
    "citation_corpus_id": 11212020,
    "start": 1690,
    "end": 1713,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "INTRODUCTION\n\nSummarization is the process of condensing a source text into a shorter version while preserving its information content.",
    "curr": "Thanks to neural encoder-decoder models (Bahdanau et al., 2015;Sutskever et al., 2014), Transformer-based architectures (Vaswani et al., 2017), and large-scale pretraining (Liu & Lapata, 2019;Zhang et al., 2020;Lewis et al., 2020), the past few years have witnessed a huge leap forward in summarization technology.",
    "next": "Abstractive methods fluently paraphrase the main content of the input, using a vocabulary different from the original document, while extractive approaches are less creative -they produce summaries by identifying and subsequently concatenating the most important sentences in a document -but manage to avoid hallucinations, false statements and inconsistencies.",
    "query": "Are there any research papers on summarization techniques using neural encoder-decoder models, Transformer-based architectures, and large-scale pretraining methods?"
  },
  {
    "index": 243,
    "source_corpus_id": 245837268,
    "ref_id": "b18",
    "citation_corpus_id": 13046179,
    "start": 2607,
    "end": 2633,
    "title": "A BASELINE FOR DETECTING MISCLASSIFIED AND OUT-OF-DISTRIBUTION EXAMPLES IN NEURAL NETWORKS",
    "abstract": "We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.Published as a conference paper at ICLR 2017 one method which outperforms the baseline on some (but not all) tasks. This new method evaluates the quality of a neural network's input reconstruction to determine if an example is abnormal.",
    "prev": "Since there is little prior warning of phase transitions, they pose a challenge to monitoring the safety of ML systems.",
    "curr": "Spurred by this challenge, we propose an anomaly detection task (Hendrycks & Gimpel, 2017;Tack et al., 2020): Can we detect when the true reward starts to drop, while maintaining a low false positive rate in benign cases?",
    "next": "We instantiate our proposed task, POLYNOMALY, for the traffic and COVID environments (Section 5).",
    "query": "Are there any research papers on methods for detecting misclassified and out-of-distribution examples in neural networks using softmax probabilities and input reconstruction techniques?"
  },
  {
    "index": 244,
    "source_corpus_id": 257078985,
    "ref_id": "b10",
    "citation_corpus_id": 233024779,
    "start": 28797,
    "end": 28816,
    "title": "Published as a conference paper at ICLR 2021 DOMAIN GENERALIZATION WITH MIXSTYLE",
    "abstract": "Though convolutional neural networks (CNNs) have demonstrated remarkable ability in learning discriminative features, they often generalize poorly to unseen domains. Domain generalization aims to address this problem by learning from a set of source domains a model that is generalizable to any unseen domain. In this paper, a novel approach is proposed based on probabilistically mixing instancelevel feature statistics of training samples across source domains. Our method, termed MixStyle, is motivated by the observation that visual domain is closely related to image style (e.g., photo vs. sketch images). Such style information is captured by the bottom layers of a CNN where our proposed style-mixing takes place. Mixing styles of training instances results in novel domains being synthesized implicitly, which increase the domain diversity of the source domains, and hence the generalizability of the trained model. MixStyle fits into mini-batch training perfectly and is extremely easy to implement. The effectiveness of MixStyle is demonstrated on a wide range of tasks including category classification, instance retrieval and reinforcement learning.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on domain generalization techniques using style-based feature mixing in convolutional neural networks?"
  },
  {
    "index": 245,
    "source_corpus_id": 20472740,
    "ref_id": "b51",
    "citation_corpus_id": 1957433,
    "start": 9627,
    "end": 9652,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "Embedding Layer converts each word or phrase to a vector representation and construct the representation matrix for sentences.",
    "curr": "In embedding layer, a model can map tokens to vectors with the pre-trained word representation such as GloVe (Pennington et al., 2014), word2Vec (Mikolov et al., 2013) and fasttext (Joulin et al., 2016).",
    "next": "It can also utilize the preprocessing tool, e.g.",
    "query": "Are there any research papers on pre-trained word vector models such as GloVe and their training methods combining global matrix factorization with local context window approaches for capturing semantic and syntactic regularities?"
  },
  {
    "index": 247,
    "source_corpus_id": 247594724,
    "ref_id": "b0",
    "citation_corpus_id": 67915085,
    "start": 1475,
    "end": 1478,
    "title": "STOCHASTIC OPTIMIZATION OF SORTING NETWORKS VIA CONTINUOUS RELAXATIONS",
    "abstract": "Sorting input objects is an important step in many machine learning pipelines. However, the sorting operator is non-differentiable with respect to its inputs, which prohibits end-to-end gradient-based optimization. In this work, we propose NeuralSort, a general-purpose continuous relaxation of the output of the sorting operator from permutation matrices to the set of unimodal row-stochastic matrices, where every row sums to one and has a distinct arg max. This relaxation permits straight-through optimization of any computational graph involve a sorting operation. Further, we use this relaxation to enable gradient-based stochastic optimization over the combinatorially large space of permutations by deriving a reparameterized gradient estimator for the Plackett-Luce family of distributions over permutations. We demonstrate the usefulness of our framework on three tasks that require learning semantic orderings of high-dimensional objects, including a fully differentiable, parameterized extension of the k-nearest neighbors algorithm. ;Chierichetti et al., 2018). By learning distributions over unobserved permutations, we can account for the uncertainty in these permutations in a principled manner. However, the challenge with stochastic optimization over discrete distributions lies in gradient estimation with respect to the distribution parameters. Vanilla REINFORCE estimators are impractical for most cases, or necessitate custom control variates for low-variance gradient estimation (Glasserman, 2013).In this regard, we consider the Plackett-Luce (PL) family of distributions over permutations (Plackett, 1975; Luce, 1959). A common modeling choice for ranking models, the PL distribution is parameterized by n scores, with its support defined over the symmetric group consisting of n! permutations. We derive a reparameterizable sampler for stochastic optimization with respect to this distribution, based on Gumbel perturbations to the n (log-)scores. However, the reparameterized sampler requires sorting these perturbed scores, and hence the gradients of a downstream learning objective with respect to the scores are not defined. By using NeuralSort instead, we can approximate the objective and obtain well-defined reparameterized gradient estimates for stochastic optimization.Finally, we apply NeuralSort to tasks that require us to learn semantic orderings of complex, highdimensional input data. First, we consider sorting images of handwritten digits, where the goal is to learn to sort images by their unobserved labels. Our second task extends the first one to quantile regression, where we want to estimate the median (50-th percentile) of a set of handwritten numbers. In addition to identifying the index of the median image in the sequence, we need to learn to map the inferred median digit to its scalar representation. In the third task, we propose an algorithm that learns a basis representation for the k-nearest neighbors (kNN) classifier in an end-to-end procedure. Because the choice of the k nearest neighbors requires a non-differentiable sorting, we use NeuralSort to obtain an approximate, differentiable surrogate. On all tasks, we observe significant empirical improvements due to NeuralSort over the relevant baselines and competing relaxations to permutation matrices.PRELIMINARIESAn n-dimensional permutation z = [z 1 , z 2 , . . . , z n ] T is a list of unique indices {1, 2, . . . , n}. Every permutation z is associated with a permutation matrix P z ∈ {0, 1} n×n with entries given as: P z [i, j] = 1 if j = z i 0 otherwise. multimodal graph matching: Sparse coding meets graph matching. In Advances in Neural Information Processing Systems, 2013.",
    "prev": "INTRODUCTION\n\nRecently, the idea of end-to-end training of neural networks with ordering supervision via continuous relaxation of the sorting function has been presented by Grover et al.",
    "curr": "[1].",
    "next": "The idea of ordering supervision is that the ground truth order of some samples is known while their absolute values remain unsupervised.",
    "query": "Are there any research papers on continuous relaxation methods for sorting networks in stochastic optimization, particularly those addressing non-differentiable sorting operators and enabling gradient-based learning of permutations?"
  },
  {
    "index": 249,
    "source_corpus_id": 1859294,
    "ref_id": "b10",
    "citation_corpus_id": 5590763,
    "start": 1836,
    "end": 1840,
    "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "prev": "Such success has been enabled by the appearance of larger datasets, more powerful computing resources and improved architectures and training algorithms.",
    "curr": "Gated units, such as the Long Short-Term Memory [24] (LSTM) and the Gated Recurrent Unit [11] (GRU), were designed to deal with the vanishing gradients problem commonly found in RNNs [8].",
    "next": "These architectures have become popularized thanks to their impressive results in a variety of tasks such as machine translation [5], language modeling [53] or speech recognition [19].",
    "query": "Are there any research papers on RNN encoder-decoder models for machine translation that utilize gated units like LSTM or GRU to address vanishing gradients?"
  },
  {
    "index": 252,
    "source_corpus_id": 9655643,
    "ref_id": "b5",
    "citation_corpus_id": 5590763,
    "start": 1799,
    "end": 1816,
    "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "prev": "* Work done at Google Brain.",
    "curr": "INTRODUCTION\n\nSequence-to-sequence (seq2seq) models (Sutskever et al., 2014;Cho et al., 2014) with attention  have been successfully applied to many applications including machine translation (Luong et al., 2015;Jean et al., 2015), parsing (Vinyals et al., 2015a), image captioning (Vinyals et al., 2015b;Xu et al., 2015) and Automatic Speech Recognition (ASR) (Chan et al., 2016;Bahdanau et al., 2016a).",
    "next": "Previous work has assumed a fixed deterministic decomposition for each output sequence.",
    "query": "Are there any research papers on RNN-based encoder-decoder models for statistical machine translation that use attention mechanisms and improve phrase representation learning?"
  },
  {
    "index": 254,
    "source_corpus_id": 8728609,
    "ref_id": "b35",
    "citation_corpus_id": 1957433,
    "start": 2450,
    "end": 2474,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "1\n\nINTRODUCTION\n\nWord embeddings-continuous-valued vector representations of words-are an almost ubiquitous component of recent natural language processing (NLP) research.",
    "curr": "Word embeddings can be learned using spectral methods (Deerwester et al., 1990) or, more commonly in recent work, via neural networks (Bengio et al., 2003;Mnih & Hinton, 2007;Mikolov et al., 2013;Pennington et al., 2014).",
    "next": "Word embeddings can also be composed to form embeddings of phrases, sentences, or documents Kiros et al., 2015;Wieting et al., 2016;Iyyer et al., 2015).",
    "query": "Are there any research papers on models that combine global matrix factorization and local context window methods for learning word embeddings using word-word cooccurrence matrices?"
  },
  {
    "index": 257,
    "source_corpus_id": 49907212,
    "ref_id": "b12",
    "citation_corpus_id": 9059612,
    "start": 2991,
    "end": 2998,
    "title": "ADVERSARIAL MACHINE LEARNING AT SCALE",
    "abstract": "Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet (Russakovsky  et al., 2014). Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than singlestep attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a \"label leaking\" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.",
    "prev": "Introduction\n\nRecent research has shown that neural networks exhibit significant vulnerability to adversarial examples, or slightly perturbed inputs designed to fool the network prediction.",
    "curr": "This vulnerability is present in a wide range of settings, from situations in which inputs are fed directly to classifiers [SZS + 14, CMV + 16] to highly variable real-world environments [KGB16,AEIK18].",
    "next": "Researchers have developed a host of methods to construct such attacks [GSS15, MFF, CW17, MMS + 18], most of which correspond to first order (i.e., gradient based) methods.",
    "query": "Are there any research papers on adversarial training methods for improving robustness in large-scale machine learning models, such as those applied to ImageNet?"
  },
  {
    "index": 260,
    "source_corpus_id": 52895589,
    "ref_id": "b14",
    "citation_corpus_id": 3292002,
    "start": 2047,
    "end": 2071,
    "title": "GRAPH ATTENTION NETWORKS",
    "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-theart results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a proteinprotein interaction dataset (wherein test graphs remain unseen during training).",
    "prev": "INTRODUCTION\n\nLearning with graph structured data, such as molecules, social, biological, and financial networks, requires effective representation of their graph structure (Hamilton et al., 2017b).",
    "curr": "Recently, there has been a surge of interest in Graph Neural Network (GNN) approaches for representation learning of graphs (Li et al., 2016;Hamilton et al., 2017a;Kipf & Welling, 2017;Velickovic et al., 2018;Xu et al., 2018).",
    "next": "GNNs broadly follow a recursive neighborhood aggregation (or message passing) scheme, where each node aggregates feature vectors of its neighbors to compute its new feature vector (Gilmer et al., 2017;Xu et al., 2018).",
    "query": "Are there any research papers on graph attention networks that leverage self-attention mechanisms for representation learning in graph-structured data, such as citation networks and biological systems?"
  },
  {
    "index": 261,
    "source_corpus_id": 222379753,
    "ref_id": "b34",
    "citation_corpus_id": 49868626,
    "start": 8978,
    "end": 8996,
    "title": "META-LEARNING WITH LATENT EMBEDDING OPTIMIZATION",
    "abstract": "Gradient-based meta-learning techniques are both widely applicable and proficient at solving challenging few-shot learning and fast adaptation problems. However, they have practical difficulties when operating on high-dimensional parameter spaces in extreme low-data regimes. We show that it is possible to bypass these limitations by learning a data-dependent latent generative representation of model parameters, and performing gradient-based meta-learning in this lowdimensional latent space. The resulting approach, latent embedding optimization (LEO), decouples the gradient-based adaptation procedure from the underlying high-dimensional space of model parameters. Our evaluation shows that LEO can achieve state-of-the-art performance on the competitive miniImageNet and tieredImageNet few-shot classification tasks. Further analysis indicates LEO is able to capture uncertainty in the data, and can perform adaptation more effectively by optimizing in latent space. arXiv:1807.05960v3 [cs.LG]",
    "prev": "For example, they may assume that features that are discriminative for the base classes are also discriminative for the novel classes, suggesting a metric learning-based approach Qi et al., 2018;Snell et al., 2017;Vinyals et al., 2016;Sung et al., 2018;Hou et al., 2019).",
    "curr": "Alternatively, they may assume that model initializations that lead to rapid convergence on the base classes are also good initializations for the novel classes (Finn et al., 2017;Ravi & Larochelle, 2017;Nichol & Schulman;Rusu et al., 2019;.",
    "next": "Other methods assume that modes of intra-class variation are shared, suggesting the possibility of learned, class-agnostic augmentation policies (Hariharan & Girshick, 2017;Chen et al., 2019b).",
    "query": "Are there any research papers on meta-learning approaches for few-shot learning that utilize latent embedding optimization to address challenges in high-dimensional parameter spaces?"
  },
  {
    "index": 263,
    "source_corpus_id": 263334074,
    "ref_id": "b88",
    "citation_corpus_id": 259370805,
    "start": 3118,
    "end": 3137,
    "title": "DT-Solver: Automated Theorem Proving with Dynamic-Tree Sampling Guided by Proof-level Value Function",
    "abstract": "Recent advances in neural theorem-proving resort to large language models and tree searches. When proving a theorem, a language model advises single-step actions based on the current proving state and the tree search finds a sequence of correct steps using actions given by the language model. However, prior works often conduct constant computation efforts for each proving state while ignoring that the hard states often need more exploration than easy states. Moreover, they evaluate and guide the proof search solely depending on the current proof state instead of considering the whole proof trajectory as human reasoning does. Here, to accommodate general theorems, we propose a novel Dynamic-Tree Driven Theorem Solver (DT-Solver) by guiding the search procedure with state confidence and proof-level values. Specifically, DT-Solver introduces a dynamic-tree Monte-Carlo search algorithm, which dynamically allocates computing budgets for different state confidences, guided by a new proof-level value function to discover proof states that require substantial exploration. Experiments on two popular theorem-proving datasets, PISA and Mathlib, show significant performance gains by our DT-Solver over the state-of-the-art approaches, with a 6.65% improvement on average in terms of success rate. And especially under low computing resource settings (11.03% improvement on average).",
    "prev": " further bridges the gap between human proofs and formal proofs by making it easier to impute missing steps.LEGO-Prover advances the stateof-the-art pass rate on miniF2F-valid (48.0% to 57.0%) and miniF2F-test (45.5% to 50.0%).During the proving process, LEGO-Prover also manages to generate over 20,000 skills (theorems/lemmas) and adds them to the growing library.Our ablation study indicates that these newly added skills are indeed helpful for proving theorems, resulting in an improvement from a success rate of 47.1% to 50.4%.We also release our code and all the generated skills.",
    "curr": "1\n\nINTRODUCTION\n\nThe automation of formal reasoning tasks, such as theorem proving and mathematical proof formalization, represents a formidable challenge and an active area of research within the domain of artificial intelligence (Polu & Sutskever, 2020a;Han et al., 2022;Jiang et al., 2022a;First et al., 2023;Bansal et al., 2019;Lample et al., 2022;Jiang et al., 2022b;2021;Zhao et al., 2023;Yang et al., 2023;Wang et al., 2023b;Liu et al., 2023).The process of formalizing mathematical proofs typically relies on human experts to transcribe intricate mathematical concepts into structured formal languages verifiable by interactive theorem prover like Lean (de Moura et al., 2015) or Isabelle (Paulson, 1994).This process, while robust, is often labor-intensive and demands a high level of expertise.",
    "next": "In the past few years, large language models (LLMs) have emerged as a promising avenue, with their capacity to process and produce human-like text, opening doors to the idea of LLM-based neural LEGO Prover (* lemma 1.",
    "query": "Are there any research papers on automated theorem proving with dynamic-tree sampling guided by proof-level value functions and large language models?"
  },
  {
    "index": 264,
    "source_corpus_id": 238198403,
    "ref_id": "b3",
    "citation_corpus_id": 221447287,
    "start": 4451,
    "end": 4471,
    "title": "WAVEGRAD: ESTIMATING GRADIENTS FOR WAVEFORM GENERATION",
    "abstract": "This paper introduces WaveGrad, a conditional model for waveform generation through estimating gradients of the data density. This model is built on the prior work on score matching and diffusion probabilistic models. It starts from Gaussian white noise and iteratively refines the signal via a gradient-based sampler conditioned on the mel-spectrogram. WaveGrad is non-autoregressive, and requires only a constant number of generation steps during inference. It can use as few as 6 iterations to generate high fidelity audio samples. WaveGrad is simple to train, and implicitly optimizes for the weighted variational lower-bound of the log-likelihood. Empirical experiments reveal WaveGrad to generate high fidelity audio samples matching a strong likelihood-based autoregressive baseline with less sequential operations. * Work done during an internship at Google Brain. † Equal contribution.",
    "prev": "Among other popular solutions of the disentanglement problem one can mention applying vector quantization technique to the content information (Wu et al., 2020;Wang et al., 2021), utilizing features of Variational AutoEncoders (Luong & Tran, 2021; The model we propose in this paper solves the disentanglement problem by employing the encoder predicting \"average voice\": it is trained to transform mel features corresponding to each phoneme into mel features corresponding to this phoneme averaged across a large multi-speaker dataset.",
    "curr": "As for decoder, in our VC model, it is designed as a part of a Diffusion Probabilistic Model (DPM) since this class of generative models has shown very good results in speech-related tasks like raw waveform generation (Chen et al., 2021a;Kong et al., 2021) and mel feature generation (Popov et al., 2021;Jeong et al., 2021).",
    "next": "However, this decoder choice poses a problem of slow inference because DPM forward pass scheme is iterative and to obtain high-quality results it is typically necessary to run it for hundreds of iterations (Ho et al., 2020;Nichol & Dhariwal, 2021).",
    "query": "Are there any research papers on gradient-based methods for waveform generation using diffusion probabilistic models with improved inference efficiency?"
  },
  {
    "index": 266,
    "source_corpus_id": 60441438,
    "ref_id": "b31",
    "citation_corpus_id": 16326763,
    "start": 3399,
    "end": 3422,
    "title": "CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING",
    "abstract": "We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies \"end-to-end\": directly from raw pixel inputs. * These authors contributed equally.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on deep reinforcement learning approaches for continuous control using actor-critic algorithms and deterministic policy gradients in simulated physics environments?"
  },
  {
    "index": 268,
    "source_corpus_id": 264490454,
    "ref_id": "b9",
    "citation_corpus_id": 246285344,
    "start": 1626,
    "end": 1649,
    "title": "NATURAL LANGUAGE DESCRIPTIONS OF DEEP VISUAL FEATURES",
    "abstract": "Some neurons in deep networks specialize in recognizing highly specific perceptual, structural, or semantic features of inputs. In computer vision, techniques exist for identifying neurons that respond to individual concept categories like colors, textures, and object classes. But these techniques are limited in scope, labeling only a small subset of neurons and behaviors in any network. Is a richer characterization of neuron-level computation possible? We introduce a procedure (called MILAN, for mutual-information-guided linguistic annotation of neurons) that automatically labels neurons with open-ended, compositional, natural language descriptions. Given a neuron, MILAN generates a description by searching for a natural language string that maximizes pointwise mutual information with the image regions in which the neuron is active. MILAN produces fine-grained descriptions that capture categorical, relational, and logical structure in learned features. These descriptions obtain high agreement with human-generated feature descriptions across a diverse set of model architectures and tasks, and can aid in understanding and controlling learned models. We highlight three applications of natural language neuron descriptions. First, we use MILAN for analysis, characterizing the distribution and importance of neurons selective for attribute, category, and relational information in vision models. Second, we use MILAN for auditing, surfacing neurons sensitive to human faces in datasets designed to obscure them. Finally, we use MILAN for editing, improving robustness in an image classifier by deleting neurons sensitive to text features spuriously correlated with class labels. 1",
    "prev": "iently large model from the Pythia and LLaMA families.Using causal interventions, we show that LMs' internal activations represent binding information by attaching binding ID vectors to corresponding entities and attributes.We further show that binding ID vectors form a continuous subspace, in which distances between binding ID vectors reflect their discernability.Overall, our results uncover interpretable strategies in LMs for representing symbolic knowledge in-context, providing a step towards understanding general in-context reasoning in large-scale LMs.",
    "curr": "INTRODUCTION\n\nModern language models (LMs) excel at many reasoning benchmarks, suggesting that they can perform general purpose reasoning across many domains.However, the mechanisms that underlie LM reasoning remain largely unknown (Räuker et al., 2023).The deployment of LMs in society has led to calls to better understand these mechanisms (Hendrycks et al., 2021), so as to know why they work and when they fail (Mu & Andreas, 2020;Hernandez et al., 2021;Vig et al., 2020b).",
    "next": "In this work, we seek to understand binding, a foundational skill that underlies reasoning.How humans solve binding, i.e.",
    "query": "Are there any studies that explore techniques for automatically generating natural language descriptions of deep visual features using mutual information-guided approaches?"
  },
  {
    "index": 269,
    "source_corpus_id": 52893515,
    "ref_id": "b26",
    "citation_corpus_id": 4429876,
    "start": 3650,
    "end": 3654,
    "title": "Global optimality conditions for deep neural networks",
    "abstract": "We study the error landscape of deep linear and nonlinear neural networks with square error loss. We build on the recent results in the literature and present necessary and sufficient conditions for a critical point of the empirical risk function to be a global minimum in the deep linear network case. Our simple conditions can also be used to determine whether a given critical point is a global minimum or a saddle point. We further extend these results to deep nonlinear neural networks and prove similar sufficient conditions for global optimality in the function space.",
    "prev": "There is a large and rapidly expanding literature of optimization of neural networks.",
    "curr": "Some works focus on the loss surface [1,10,14,[16][17][18][19][20][21][24][25][26][27][28], while others study the convergence of gradient-based methods for optimizing this loss [3,6,22].",
    "next": "In particular, our focus is on the loss surface itself, independent of any algorithmic concerns; this is reflected in the works summarized below.",
    "query": "Are there any research papers on error landscape analysis and global optimality conditions for deep neural networks with square error loss?"
  },
  {
    "index": 270,
    "source_corpus_id": 51926976,
    "ref_id": "b14",
    "citation_corpus_id": 8820379,
    "start": 1483,
    "end": 1502,
    "title": "Summarizing Source Code using a Neural Attention Model",
    "abstract": "High quality source code is often paired with high level summaries of the computation it performs, for example in code documentation or in descriptions posted in online forums. Such summaries are extremely useful for applications such as code search but are expensive to manually author, hence only done for a small fraction of all code that is produced. In this paper, we present the first completely datadriven approach for generating high level summaries of source code. Our model, CODE-NN , uses Long Short Term Memory (LSTM) networks with attention to produce sentences that describe C# code snippets and SQL queries. CODE-NN is trained on a new corpus that is automatically collected from StackOverflow, which we release. Experiments demonstrate strong performance on two tasks: (1) code summarization, where we establish the first end-to-end learning results and outperform strong baselines, and (2) code retrieval, where our learned model improves the state of the art on a recently introduced C# benchmark by a large margin.",
    "prev": "Our model significantly outperforms previous models that were specifically designed for programming languages, as well as state-of-the-art NMT models.",
    "curr": "INTRODUCTION\n\nModeling the relation between source code and natural language can be used for automatic code summarization (Allamanis et al., 2016), documentation (Iyer et al., 2016), retrieval (Allamanis et al., 2015b), and even generation (Balog et al., 2016;Rabinovich et al., 2017;Yin and Neubig, 2017;Devlin et al., 2017;Murali et al., 2017).",
    "next": "In this work, we consider the general problem of generating a natural language sequence from a given snippet of source code.",
    "query": "Are there any research papers on neural attention models for code summarization and natural language generation from source code?"
  },
  {
    "index": 272,
    "source_corpus_id": 65455367,
    "ref_id": "b3",
    "citation_corpus_id": 6628106,
    "start": 2638,
    "end": 2657,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "This problem is especially exacerbated in high dimensional problems arising in deep learning.",
    "curr": "To tackle this issue, several variants of ADAGRAD, such as RMSPROP (Tieleman & Hinton, 2012), ADAM (Kingma & Ba, 2015), ADADELTA (Zeiler, 2012), NADAM (Dozat, 2016), etc, have been proposed which mitigate the rapid decay of the learning rate using the exponential moving averages of squared past gradients, essentially limiting the reliance of the update to only the past few gradients.",
    "next": "While these algorithms have been successfully employed in several practical applications, they have also been observed to not converge in some other settings.",
    "query": "Are there any research papers on adaptive optimization algorithms for stochastic gradient descent in deep learning, focusing on convergence properties and variants like ADAM?"
  },
  {
    "index": 273,
    "source_corpus_id": 3300406,
    "ref_id": "b8",
    "citation_corpus_id": 7774489,
    "start": 2328,
    "end": 2350,
    "title": "Published as a conference paper at ICLR 2017 STOCHASTIC NEURAL NETWORKS FOR HIERARCHICAL REINFORCEMENT LEARNING",
    "abstract": "Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks. Our approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks. To efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments 1 show 2 that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.",
    "prev": "This problem is known as the problem of option discovery.",
    "curr": "Option discovery has received ample attention over many years, with varied solutions being proposed (e.g., Bacon et al., 2017;Ş imsek & Barto, 2004;Daniel et al., 2016;Florensa et al., 2017;Konidaris & Barto, 2009;Mankowitz et al., 2016;McGovern & Barto, 2001).",
    "next": "Recently, Machado et al.",
    "query": "Are there any research papers on hierarchical reinforcement learning methods that use stochastic neural networks for option discovery or skill pre-training?"
  },
  {
    "index": 274,
    "source_corpus_id": 219956317,
    "ref_id": "b25",
    "citation_corpus_id": 6628106,
    "start": 21833,
    "end": 21844,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "Acceleration\n\nAs the last comparison, we discuss the combination of compression and acceleration/momentum.",
    "curr": "This setting is very important to consider as essentially all state-of-the-art methods for training deep learning models, including Adam [Kingma andBa, 2015, Reddi et al., 2018], rely on the use of momentum in one form or another.",
    "next": "One can treat the unbiased compressed gradient as a stochastic gradient [Gorbunov et al., 2020] and the theory for momentum SGD [Yang et al., 2016, Gadat et al., 2018, Loizou and Richtárik, 2017 would be applicable with an extra smoothness assumption.",
    "query": "Are there any research papers on combining momentum with gradient compression techniques in stochastic optimization algorithms?"
  },
  {
    "index": 275,
    "source_corpus_id": 3720457,
    "ref_id": "b8",
    "citation_corpus_id": 6628106,
    "start": 2779,
    "end": 2798,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "A related hyperparameter is momentum; typically fixed to a reasonable value such as 0.9, careful tuning can also give significant performance gains (Sutskever et al., 2013).",
    "curr": "While optimizers such as Adam (Kingma & Ba, 2015) are often described as adapting coordinate-specific learning rates, in fact they also have global learning rate and momentum hyperparameters analogously to SGD, and tuning at least the learning rate can be important to good performance.",
    "next": "In light of this, it is not surprising that there have been many attempts to adapt learning rates, either online during optimization (Schraudolph, 1999;Schaul et al., 2013), or offline by fitting a learning rate schedule (Maclaurin et al., 2015).",
    "query": "Are there any research papers on adaptive learning rate methods in stochastic optimization, particularly focusing on hyperparameter tuning in Adam and related algorithms?"
  },
  {
    "index": 276,
    "source_corpus_id": 263831046,
    "ref_id": "b14",
    "citation_corpus_id": 53802740,
    "start": 19203,
    "end": 19224,
    "title": "META-LEARNING PROBABILISTIC INFERENCE FOR PREDICTION",
    "abstract": "This paper introduces a new framework for data efficient and versatile learning. Specifically: 1) We develop ML-PIP, a general framework for Meta-Learning approximate Probabilistic Inference for Prediction. ML-PIP extends existing probabilistic interpretations of meta-learning to cover a broad class of methods. 2) We introduce VERSA, an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass. VERSA substitutes optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training. 3) We evaluate VERSA on benchmark datasets where the method sets new state-of-the-art results, handles arbitrary numbers of shots, and for classification, arbitrary numbers of classes at train and test time. The power of the approach is then demonstrated through a challenging few-shot ShapeNet view reconstruction task. * Authors contributed equally.",
    "prev": "icy to enforce fairness within the optimization objective.",
    "curr": "5 Making Large-Scale Problem Tractable: Amortized Policy  7) and Update θ i in Θ; end\n\nIn practice, managing a city's extensive traffic network is a challenging large-scale problem due to its sheer size.To tackle this, we employ a divide-and-conquer approach, breaking the problem down into manageable subproblems.For instance, we segment the vast network into smaller, more manageable subgraphs, each representing a tractable subproblem.While this strategy makes the overall problem more manageable, it raises a crucial question: How can we utilize optimal policies from previous subproblems to streamline the optimization of new ones?To address this, we introduce the Amortized Network Interventions (ANI) framework.Amortized Intervention In the previous section, our assumption was that each agent operates solely with local information, without utilizing global data.In this section, our objective is to learn a shared amortized policy (Gordon et al., 2019) that can be applied across different regions with distinct dynamics.We hypothesize the existence of collective behavior among these various local temporal dynamic systems.Given a sequence of local policies {π i } M i=1 addressing M distinct sub-problems, our goal is to create an amortized policy π amo .This policy should extract invariant representations and enable the adoption of similar policy structures among similar temporal dynamic systems.Permutation Equivalent Property Inspired from policy similarity embeddings (PSM) (Agarwal et al., 2021) and the policy permutation invariant property in SensoryNeuron (Tang & Ha, 2021), we devise an agent that can extract permutation equivalent embeddings and is policy permutation equivalent to the latent state space h t .Since each dimension of h t corresponds to one node in the excitatory point process, the permutation equivalent property along the node dimension characterizes the collective behavior within complex dynamic systems.We present",
    "next": null,
    "query": "Are there any research papers on meta-learning frameworks for probabilistic inference that employ amortization networks to enable efficient few-shot learning with arbitrary numbers of shots and classes?"
  },
  {
    "index": 277,
    "source_corpus_id": 257405190,
    "ref_id": "b79",
    "citation_corpus_id": 232269984,
    "start": 7844,
    "end": 7848,
    "title": "LARGE SCALE IMAGE COMPLETION VIA CO-MODUL- ATED GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "Numerous task-specific variants of conditional generative adversarial networks have been developed for image completion. Yet, a serious limitation remains that all existing algorithms tend to fail when handling large-scale missing regions. To overcome this challenge, we propose a generic new approach that bridges the gap between image-conditional and recent modulated unconditional generative architectures via co-modulation of both conditional and stochastic style representations. Also, due to the lack of good quantitative metrics for image completion, we propose the new Paired/Unpaired Inception Discriminative Score (P-IDS/U-IDS), which robustly measures the perceptual fidelity of inpainted images compared to real images via linear separability in a feature space. Experiments demonstrate superior performance in terms of both quality and diversity over state-of-the-art methods in free-form image completion and easy generalization to image-to-image translation. Code is available at https://github.com/zsyzzsoft/co-mod-gan.",
    "prev": "Besides, Zhao et al.",
    "curr": "[80] propose a novel architecture, bridging the gap between image-conditional and unconditional generation, improving free-form large-scale image completion.",
    "next": "There are also attempts to study the progressive generation.",
    "query": "Are there any research papers on methods for large-scale image completion using co-modulated generative adversarial networks that bridge conditional and unconditional generation?"
  },
  {
    "index": 278,
    "source_corpus_id": 3502468,
    "ref_id": "b5",
    "citation_corpus_id": 5273326,
    "start": 20511,
    "end": 20533,
    "title": "Published as a conference paper at ICLR 2016 FAST AND ACCURATE DEEP NETWORK LEARNING BY EXPONENTIAL LINEAR UNITS (ELUS)",
    "abstract": "We introduce the \"exponential linear unit\" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PRe-LUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network.",
    "prev": "FearNet was implemented in Tensorflow.",
    "curr": "For mPFC and BLA, each fully connected layer uses an exponential linear unit activation function (Clevert et al., 2016).",
    "next": "The output of the encoder also connects to a softmax output layer.",
    "query": "Are there any research papers on the use of exponential linear units (ELUs) in deep neural networks for improving learning speed and classification accuracy?"
  },
  {
    "index": 279,
    "source_corpus_id": 235899205,
    "ref_id": "b26",
    "citation_corpus_id": 9586240,
    "start": 14851,
    "end": 14873,
    "title": "Annotated Gigaword",
    "abstract": "We have created layers of annotation on the English Gigaword v.5 corpus to render it useful as a standardized corpus for knowledge extraction and distributional semantics. Most existing large-scale work is based on inconsistent corpora which often have needed to be re-annotated by research teams independently, each time introducing biases that manifest as results that are only comparable at a high level. We provide to the community a public reference set based on current state-of-the-art syntactic analysis and coreference resolution, along with an interface for programmatic access. Our goal is to enable broader involvement in large-scale knowledge-acquisition efforts by researchers that otherwise may not have had the ability to produce such a resource on their own.",
    "prev": "summary/article).",
    "curr": "We provide an example of auto-prompting for a sample from the Gigaword summarization dataset (Napoles et al., 2012) with the respective masking in Figure 2 .",
    "next": "For our generation experiments, we denote HTLM-Auto-NS (not-sized) as the auto-prompt without using size hints, where HTLM-Auto-S uses the size hints based policy described in the previous section.",
    "query": "Are there any research papers on creating standardized annotated corpora for knowledge extraction and distributional semantics using the Gigaword dataset?"
  },
  {
    "index": 280,
    "source_corpus_id": 252668463,
    "ref_id": "b2",
    "citation_corpus_id": 6021932,
    "start": 5171,
    "end": 5174,
    "title": "IN SEARCH OF THE REAL INDUCTIVE BIAS: ON THE ROLE OF IMPLICIT REGULARIZATION IN DEEP LEARNING",
    "abstract": "We present experiments demonstrating that some other form of capacity control, different from network size, plays a central role in learning multi-layer feedforward networks. We argue, partially through analogy to matrix factorization, that this is an inductive bias that can help shed light on deep learning.",
    "prev": "We summarize our contributions as follows:\n\nIntroduction\n\nThough neural networks (NNs) have experienced extraordinary success, understanding the generalizability of NNs and successfully using them in real-world contexts still faces a number of obstacles [1,2].",
    "curr": "It is a well-known enigma, for instance, why such NNs generalize well and do not suffer from overfitting [3,4,5].",
    "next": "Recent research on the loss landscape of NNs seeks to reduce these obstacles.",
    "query": "Are there any studies on the role of implicit regularization in the generalization of deep learning models?"
  },
  {
    "index": 281,
    "source_corpus_id": 173990564,
    "ref_id": "b13",
    "citation_corpus_id": 6706414,
    "start": 22678,
    "end": 22682,
    "title": "Published as a conference paper at ICLR 2015 EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES",
    "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
    "prev": "Consider a worst-case scenario, in which an adversary can arbitrarily alter the output of the NN with minuscule changes to the input.",
    "curr": "It is well-known that there are NNs for which this is possible [14].",
    "next": "In this case, with the help of the I/O kernel, the model becomes highly uncertain with respect to the output kernel.",
    "query": "Are there any research papers on the vulnerability of neural networks to adversarial examples and techniques for generating or mitigating them?"
  },
  {
    "index": 283,
    "source_corpus_id": 236170938,
    "ref_id": "b31",
    "citation_corpus_id": 184486852,
    "start": 4552,
    "end": 4569,
    "title": "Causal Discovery with Reinforcement Learning",
    "abstract": "Discovering causal structure among a set of variables is a fundamental problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local heuristics to search for a directly acyclic graph (DAG) according to a predefined score function. While these methods, e.g., greedy equivalence search (GES), may have attractive results with infinite samples and certain model assumptions, they are less satisfactory in practice due to finite data and possible violation of assumptions. Motivated by recent advances in neural combinatorial optimization, we propose to use reinforcement learning (RL) to search for the DAG with the best scoring. Our encoder-decoder model takes observable data as input and generates graph adjacency matrices that are used to compute corresponding rewards. The reward incorporates both the predefined score function and two penalty terms for enforcing acyclicity. In contrast with typical RL applications where the goal is to learn a policy, we use RL as a search strategy and our final output would be the graph, among all graphs generated during training, that achieves the best reward. We conduct experiments on both synthetic and real data, and show that the proposed approach not only has an improved search ability but also allows for a flexible score function under the acyclicity constraint.Preprint. Under review.",
    "prev": "Current methods are typically applied to a few dozens of variables and cannot scale so well, which is imperative for modern applications like learning causal relations with gene editing interventions (Dixit et al., 2016;Macosko et al., 2015).",
    "curr": "A promising new direction for scaling up DAG discovery methods are continuous-optimization methods (Zheng et al., 2018;2020;Zhu et al., 2020;Ke et al., 2019;Brouillard et al., 2020;Yu et al., 2019).",
    "next": "In contrast to score-based and constrained-based (Peters et al., 2017;Guo et al., 2020) methods, continuous-optimization methods reinterpret the search over discrete graph topologies as a continuous problem with neural networks as function approximators, for which efficient solvers are amenable.",
    "query": "Are there any research papers on causal discovery methods using reinforcement learning or continuous-optimization approaches for scaling up DAG discovery in high-dimensional data?"
  },
  {
    "index": 284,
    "source_corpus_id": 231648113,
    "ref_id": "b37",
    "citation_corpus_id": 12713052,
    "start": 2086,
    "end": 2102,
    "title": "NEURAL ARCHITECTURE SEARCH WITH REINFORCEMENT LEARNING",
    "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214. * Work done as a member of the Google Brain Residency program (g.co/brainresidency.) Under review as a conference paper at ICLR 2017 neural network can be typically specified by a variable-length string. It is therefore possible to use a recurrent network -the controller -to generate such string. Training the network specified by the string -the \"child network\" -on the real data will result in an accuracy on a validation set. Using this accuracy as the reward signal, we can compute the policy gradient to update the controller. As a result, in the next iteration, the controller will give higher probabilities to architectures that receive high accuracies. In other words, the controller will learn to improve its search over time.Our experiments show that Neural Architecture Search can design good models from scratch, an achievement considered not possible with other methods. On image recognition with CIFAR-10, Neural Architecture Search can find a novel ConvNet model that is better than most human-invented architectures. Our CIFAR-10 model achieves a 3.65 test set error, while being 1.05x faster than the current best model. On language modeling with Penn Treebank, Neural Architecture Search can design a novel recurrent cell that is also better than previous RNN and LSTM architectures. The cell that our model found achieves a test set perplexity of 62.4 on the Penn Treebank dataset, which is 3.6 perplexity better than the previous state-of-the-art.",
    "prev": "INTRODUCTION\n\nInstead of manually designing neural networks, neural architecture search (NAS) algorithms are used to automatically discover the best ones (Tan & Le, 2019a;Bender et al., 2018).",
    "curr": "Early work by Zoph & Le (2017) proposed using a reinforcement learning (RL) controller that constructs candidate architectures, these are evaluated and then feedback is provided to the controller based on the performance of the candidate.",
    "next": "One major problem with this basic NAS methodology is that each evaluation is very costly -typically on the order of hours or days to train a single neural network fully.",
    "query": "Are there any research papers on neural architecture search using reinforcement learning with a controller for generating model descriptions in image and language tasks?"
  },
  {
    "index": 286,
    "source_corpus_id": 202573030,
    "ref_id": "b25",
    "citation_corpus_id": 153313159,
    "start": 4662,
    "end": 4683,
    "title": "Sparse Sequence-to-Sequence Models",
    "abstract": "Sequence-to-sequence models are a powerful workhorse of NLP. Most variants employ a softmax transformation in both their attention mechanism and output layer, leading to dense alignments and strictly positive output probabilities. This density is wasteful, making models less interpretable and assigning probability mass to many implausible outputs. In this paper, we propose sparse sequence-to-sequence models, rooted in a new family of α-entmax transformations, which includes softmax and sparsemax as particular cases, and is sparse for any α > 1. We provide fast algorithms to evaluate these transformations and their gradients, which scale well for large vocabulary sizes. Our models are able to produce sparse alignments and to assign nonzero probability to a short list of plausible outputs, sometimes rendering beam search exact. Experiments on morphological inflection and machine translation reveal consistent gains over dense models. the encodings [h 1 , . . . , h J ], using s t as a query vector. This is done by computing token-level scores z j := s t W (z) h j , then taking a weighted averageπ j h j , where π := softmax(z).(1) the anonymous reviewers, for helpful discussion and feedback.",
    "prev": "Furthermore, NODE allows constructing multi-layer architectures, which resembles \"deep\" GBDT that is trained end-to-end, which was never proposed before.",
    "curr": "Besides the usage of oblivious decision tables, another important design choice is the recent entmax transformation (Peters et al., 2019), which effectively performs a \"soft\" splitting feature choice in decision trees inside the NODE architecture.",
    "next": "As discussed in the following sections, these design choices are critical to obtain state-of-the-art performance.",
    "query": "Are there any research papers on sparse sequence-to-sequence models using α-entmax transformations for improving alignment and output sparsity in natural language processing tasks?"
  },
  {
    "index": 288,
    "source_corpus_id": 227337121,
    "ref_id": "b6",
    "citation_corpus_id": 49416020,
    "start": 7541,
    "end": 7561,
    "title": "DIFFUSION SCATTERING TRANSFORMS ON GRAPHS",
    "abstract": "Stability is a key aspect of data analysis. In many applications, the natural notion of stability is geometric, as illustrated for example in computer vision. Scattering transforms construct deep convolutional representations which are certified stable to input deformations. This stability to deformations can be interpreted as stability with respect to changes in the metric structure of the domain. In this work, we show that scattering transforms can be generalized to non-Euclidean domains using diffusion wavelets, while preserving a notion of stability with respect to metric changes in the domain, measured with diffusion maps. The resulting representation is stable to metric perturbations of the domain while being able to capture \"high-frequency\" information, akin to the Euclidean Scattering.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on diffusion scattering transforms applied to graphs using diffusion wavelets, focusing on stability with respect to metric changes in the domain?"
  },
  {
    "index": 289,
    "source_corpus_id": 13206339,
    "ref_id": "b25",
    "citation_corpus_id": 6018348,
    "start": 2374,
    "end": 2392,
    "title": "Distant Supervision for Relation Extraction with an Incomplete Knowledge Base",
    "abstract": "Distant supervision, heuristically labeling a corpus using a knowledge base, has emerged as a popular choice for training relation extractors. In this paper, we show that a significant number of \"negative\" examples generated by the labeling process are false negatives because the knowledge base is incomplete. Therefore the heuristic for generating negative examples has a serious flaw. Building on a state-of-the-art distantly-supervised extraction algorithm, we proposed an algorithm that learns from only positive and unlabeled labels at the pair-of-entity level. Experimental results demonstrate its advantage over existing algorithms.",
    "prev": "We are interested in automated reasoning on large knowledge bases (KB) with rich and diverse semantics (Suchanek et al., 2007;Bollacker et al., 2008;Carlson et al., 2010).",
    "curr": "KBs are highly incomplete (Min et al., 2013), and facts not directly stored in a KB can often be inferred from those that are, creating exciting opportunities and challenges for automated reasoning.",
    "next": "For example, consider the small knowledge graph in figure 1.",
    "query": "Are there any research papers on relation extraction techniques for incomplete knowledge bases using distant supervision and methods to address false negatives in labeling?"
  },
  {
    "index": 290,
    "source_corpus_id": 247058691,
    "ref_id": "b12",
    "citation_corpus_id": 12639289,
    "start": 2684,
    "end": 2714,
    "title": "Recurrent Continuous Translation Models",
    "abstract": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.",
    "prev": "This search problem is a hard combinatorial optimization problem, and as a result, constraints are frequently imposed on the structure of the model to make solving or approximating the search problem easier.",
    "curr": "In neural machine translation, an autoregressive factorization of the output probability distribution is widely used (Kalchbrenner & Blunsom, 2013;Sutskever et al., 2014;Vaswani et al., 2017), and a variety of conditional independence assumptions are made in other model classes from statistical translation models (Brown et al., 1993;Koehn et al., 2003) to non-autoregressive neural models (Lee et al., 2018).",
    "next": "Although these assumptions enable fast and accurate approximations to the search problem with simple and efficient algorithms (e.g., beam search), which can be crucial for efficient production applications, they limit the form of the models and thereby restricting the kinds of architectures that can be used to address observed model failures.",
    "query": "Are there any research papers on continuous translation models that avoid alignments and phrasal units, and compare their performance with alignment-based approaches in machine translation?"
  },
  {
    "index": 293,
    "source_corpus_id": 3481593,
    "ref_id": "b10",
    "citation_corpus_id": 17597823,
    "start": 22717,
    "end": 22736,
    "title": "SwissCheese at SemEval-2016 Task 4: Sentiment Classification Using an Ensemble of Convolutional Neural Networks with Distant Supervision",
    "abstract": "In this paper, we propose a classifier for predicting message-level sentiments of English micro-blog messages from Twitter. Our method builds upon the convolutional sentence embedding approach proposed by (Severyn and Moschitti, 2015a; Severyn and Moschitti, 2015b). We leverage large amounts of data with distant supervision to train an ensemble of 2-layer convolutional neural networks whose predictions are combined using a random forest classifier. Our approach was evaluated on the datasets of the SemEval-2016 competition (Task 4) outperforming all other approaches for the Message Polarity Classification task.",
    "prev": "Each training sample x consists of a sentence s and its sentiment labelỹ.",
    "curr": "The student for the sentiment classification task is a convolutional model which has been shown to perform best on the dataset we used (Deriu et al., 2017;Severyn & Moschitti, 2015a;b;Deriu et al., 2016).",
    "next": "The first layer of the network learns the function ψ(.)",
    "query": "Are there any research papers on sentiment classification using convolutional neural networks and ensemble methods with distant supervision?"
  },
  {
    "index": 295,
    "source_corpus_id": 257219926,
    "ref_id": "b0",
    "citation_corpus_id": 246867209,
    "start": 11467,
    "end": 11490,
    "title": "Published as a conference paper at ICLR 2022 PROSPECT PRUNING: FINDING TRAINABLE WEIGHTS AT INITIALIZATION USING META-GRADIENTS",
    "abstract": "Pruning neural networks at initialization would enable us to find sparse models that retain the accuracy of the original network while consuming fewer computational resources for training and inference. However, current methods are insufficient to enable this optimization and lead to a large degradation in model performance. In this paper, we identify a fundamental limitation in the formulation of current methods, namely that their saliency criteria look at a single step at the start of training without taking into account the trainability of the network. While pruning iteratively and gradually has been shown to improve pruning performance, explicit consideration of the training stage that will immediately follow pruning has so far been absent from the computation of the saliency criterion. To overcome the short-sightedness of existing methods, we propose Prospect Pruning (ProsPr), which uses meta-gradients through the first few steps of optimization to determine which weights to prune. ProsPr combines an estimate of the higherorder effects of pruning on the loss and the optimization trajectory to identify the trainable sub-network. Our method achieves state-of-the-art pruning performance on a variety of vision classification tasks, with less data and in a single shot compared to existing pruning-at-initialization methods. Our code is available online at https://github.com/",
    "prev": "We refer to a variant of sparsify-during-training as early pruning here, which only exerts pruning to network in the early stage of training.",
    "curr": "It includes pruning at initialization, e.g., GraSP , SynFlow (Tanaka et al., 2020), SBP-SR (Hayou et al., 2021), ProsPr (Alizadeh et al., 2022), and the conventional early pruning methods which stop pruning after several epochs of training (You et al., 2020;Liu et al., 2021b;Rachwan et al., 2022).",
    "next": "Most of these works are inspired by the discovery of Lottery Ticket Hypothesis (LTH) (Frankle & Carbin, 2019) or SNIP (Lee et al., 2019), if not both.",
    "query": "Are there any research papers on methods for pruning neural networks at initialization that address the trainability of the network post-pruning using meta-gradients or similar techniques?"
  },
  {
    "index": 297,
    "source_corpus_id": 202750230,
    "ref_id": "b35",
    "citation_corpus_id": 44131019,
    "start": 1691,
    "end": 1708,
    "title": "Scaling Neural Machine Translation",
    "abstract": "Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation. 1 On WMT'14 English-German translation, we match the accuracy ofVaswani et al. (2017)in under 5 hours when training on 8 GPUs and we obtain a new state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT'14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.",
    "prev": "Moreover, we show that our approach leads to small BERT-like models of higher quality compared to training from scratch or using distillation.",
    "curr": "INTRODUCTION\n\nTransformer architectures (Vaswani et al., 2017) have become the dominant architecture in natural language processing, with state-of-the-art performance across a variety of tasks, including machine translation (Vaswani et al., 2017;Ott et al., 2018), language modeling Baevski & Auli, 2018) and sentence representation (Devlin et al., 2018;.",
    "next": "Each of its layers contains millions of parameters accessed during the forward pass, making it computationally demanding in terms of memory and latency during both training and inference.",
    "query": "Are there any studies on methods for accelerating training of Transformer-based models in natural language processing, particularly for machine translation, using techniques like reduced precision and large batch training?"
  },
  {
    "index": 300,
    "source_corpus_id": 261530381,
    "ref_id": "b1",
    "citation_corpus_id": 208076137,
    "start": 2431,
    "end": 2455,
    "title": "GENERATIVE MODELS FOR EFFECTIVE ML ON PRIVATE, DECENTRALIZED DATASETS",
    "abstract": "To improve real-world applications of machine learning, experienced modelers develop intuition about their datasets, their models, and how the two interact. Manual inspection of raw data-of representative samples, of outliers, of misclassifications-is an essential tool in a) identifying and fixing problems in the data, b) generating new modeling hypotheses, and c) assigning or refining human-provided labels. However, manual data inspection is problematic for privacy-sensitive datasets, such as those representing the behavior of realworld individuals. Furthermore, manual data inspection is impossible in the increasingly important setting of federated learning, where raw examples are stored at the edge and the modeler may only access aggregated outputs such as metrics or model parameters. This paper demonstrates that generative models-trained using federated methods and with formal differential privacy guarantees-can be used effectively to debug many commonly occurring data issues even when the data cannot be directly inspected. We explore these methods in applications to text with differentially private federated RNNs and to images using a novel algorithm for differentially private federated GANs.Published as a conference paper at ICLR 2020 McMahan et al., 2017), which is the motivation and focus of this work-the data cannot be inspected. In FL, raw data remains distributed across a fleet of devices, such as mobile phones, while an orchestrating server coordinates training of a shared global model. Only the final model parameters and statistics are gathered and made available to the modeler. 1 How can a modeler effectively debug when training data is privacy sensitive or decentralized? This paper demonstrates that the novel application of auxiliary models, namely privacy-preserving generative models, can stand in for direct data examination during the process of debugging data errors during inference or training. By combining ideas from deep generative models, FL, and user-level differential privacy (DP), we show how some needs traditionally met with data inspection can instead be met by generating synthetic examples from a privacy-preserving federated generative model. These examples could be representative of all or a subset of the non-inspectable data, while at the same time preserving the privacy of individuals. Our contributions include:• Identifying key challenges in implementing end-to-end workflows with non-inspectable data, e.g., for debugging a 'primary' ML model used in a mobile application. • Proposing a methodology that allows (sufficiently powerful) 'auxiliary' generative models to resolve these challenges. • Demonstrating how privacy preserving federated generative models-RNNs for text and GANs for images-can be trained to high enough fidelity to discover introduced data errors matching those encountered in real world scenarios. This requires a novel adaption of generative adversarial networks (GANs) to the federated setting with user-level DP guarantees.",
    "prev": "nciples of FL.We propose a novel method, Federated Orthogonal Training (FOT), to overcome these drawbacks and address the global catastrophic forgetting in CFL.Our algorithm extracts the global input subspace of each layer for old tasks and modifies the aggregated updates of new tasks such that they are orthogonal to the global principal subspace of old tasks for each layer.This decreases the interference between tasks, which is the main cause for forgetting.We empirically show that FOT outperforms state-of-the-art continual learning methods in the CFL setting, achieving an average accuracy gain of up to 15% with 27% lower forgetting while only incurring a minimal computation and communication cost.",
    "curr": "INTRODUCTION\n\nFederated learning (FL) is a decentralized training solution born from the need of keeping the local data of clients private to train a global model (McMahan et al., 2017).Most of the FL works focus on the global learning of a single task (Hard et al., 2018;Yang et al., 2021;Augenstein et al., 2020).However, in real life, new tasks might arrive to the clients over time while previous data disappear due to storage limitations.For instance, assume a malware classifier is trained over multiple FL clients.The emergence of new malware families (new tasks) is inevitable, making the update of the classifier a necessity.Another real-life scenario can be the emergence of new viruses in some clients due to epidemics.The global model also has to learn to classify these new viruses (new tasks) (Yoon et al., 2021).In both of these scenarios, the model should not forget its prior knowledge while learning new tasks.",
    "next": "Continual Learning (CL) addresses this issue in centralized machine learning (ML), the problem of learning sequentially arrived tasks without forgetting (Kirkpatrick et al., 2017).Learning a global model while new tasks appear in the clients in an online manner is a problem of Continual Federated Learning (CFL) (Ma et al., 2022).An ideal CFL algorithm solving global catastrophic fo",
    "query": "Are there any studies that explore the use of privacy-preserving generative models in federated learning for debugging data issues with non-inspectable, decentralized datasets?"
  },
  {
    "index": 301,
    "source_corpus_id": 235606307,
    "ref_id": "b7",
    "citation_corpus_id": 233386821,
    "start": 3669,
    "end": 3686,
    "title": "Published as a conference paper at ICLR 2021 CAPTURING LABEL CHARACTERISTICS IN VAEs",
    "abstract": "We present a principled approach to incorporating labels in variational autoencoders (VAEs) that captures the rich characteristic information associated with those labels. While prior work has typically conflated these by learning latent variables that directly correspond to label values, we argue this is contrary to the intended effect of supervision in VAEs-capturing rich label characteristics with the latents. For example, we may want to capture the characteristics of a face that make it look young, rather than just the age of the person. To this end, we develop the characteristic capturing VAE (CCVAE), a novel VAE model and concomitant variational objective which captures label characteristics explicitly in the latent space, eschewing direct correspondences between label values and latents. Through judicious structuring of mappings between such characteristic latents and labels, we show that the CCVAE can effectively learn meaningful representations of the characteristics of interest across a variety of supervision schemes. In particular, we show that the CCVAE allows for more effective and more general interventions to be performed, such as smooth traversals within the characteristics for a given label, diverse conditional generation, and transferring characteristics across datapoints 1 . * work done while at Oxford † equal contribution 1 Link to code: https://github.com/thwjoy/ccvae",
    "prev": "Prior approaches typically combine information through explicit specification as products (Wu & Goodman, 2018), mixtures (Shi et al., 2019), combinations of such (Sutter et al., 2021), or through additional regularisers on the representations (Suzuki et al., 2016;Sutter et al., 2020).",
    "curr": "Here, we explore an alternative approach that leverages advances in semi-supervised VAEs (Siddharth et al., 2017; Joy et al., 2021) to repurpose existing regularisation in the VAE framework as an implicit means by which information is combined across modalities (see Figure 1).",
    "next": "We develop a novel formulation for multimodal VAEs that views the combination of information through a semi-supervised lens, as mutual supervision between modalities.",
    "query": "Are there any research papers on methods for capturing label characteristics in variational autoencoders using semi-supervised approaches or mutual supervision between modalities?"
  },
  {
    "index": 303,
    "source_corpus_id": 214002473,
    "ref_id": "b31",
    "citation_corpus_id": 76667896,
    "start": 2633,
    "end": 2650,
    "title": "UNSUPERVISED DISCOVERY OF PARTS, STRUCTURE, AND DYNAMICS",
    "abstract": "Humans easily recognize object parts and their hierarchical structure by watching how they move; they can then predict how each part moves in the future. In this paper, we propose a novel formulation that simultaneously learns a hierarchical, disentangled object representation and a dynamics model for object parts from unlabeled videos. Our Parts, Structure, and Dynamics (PSD) model learns to, first, recognize the object parts via a layered image representation; second, predict hierarchy via a structural descriptor that composes low-level concepts into a hierarchical structure; and third, model the system dynamics by predicting the future. Experiments on multiple real and synthetic datasets demonstrate that our PSD model works well on all three tasks: segmenting object parts, building their hierarchical structure, and capturing their motion distributions.",
    "prev": "INTRODUCTION\n\nSystem identification or physical parameter estimation is commonly required for control or state estimation for physical modelling, and typically relies on dedicated sensing equipment and carefully constructed experiments.",
    "curr": "Current machine learning approaches to physical modeling from video either require training by supervised regression from video to object coordinates before estimating explicit physics (Watters et al., 2017;Wu et al., 2017b;Belbute-Peres et al., 2018), or are able to discover and segment objects from video in an unsupervised manner, but do not naturally integrate with a physics engine for long-term predictions or generation of interpretable locations and physical parameters for physical reasoning (Xu et al., 2019;van Steenkiste et al., 2018).",
    "next": "In this work, we bridge the gap between unsupervised discovery of objects from video and learning the physical dynamics of a system, by learning unknown physical parameters and explicit trajectory coordinates.",
    "query": "Are there any research papers on unsupervised methods for discovering object parts, hierarchical structure, and dynamics from unlabeled video data?"
  },
  {
    "index": 305,
    "source_corpus_id": 220793552,
    "ref_id": "b6",
    "citation_corpus_id": 54101493,
    "start": 3214,
    "end": 3217,
    "title": "IMAGENET-TRAINED CNNS ARE BIASED TOWARDS TEXTURE; INCREASING SHAPE BIAS IMPROVES ACCURACY AND ROBUSTNESS",
    "abstract": "Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNettrained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on 'Stylized-ImageNet', a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation. de Beeck. Deep neural networks as a computational model for human shape sensitivity. DiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual cortex.",
    "prev": "While we are collecting datasets with millions of training samples, DNNs are still vulnerable to domain shift, small perturbations, and adversarial examples to which humans are remarkably robust [20,4].",
    "curr": "Recent research has shown that neural networks tend to use superficial features rather than global shape information for prediction even when trained on large scale datasets such as ImageNet [7].",
    "next": "These superficial features can be local textures or even patterns imperceptible to humans but detectable to the DNNs, as is the case for adversarial examples [11].",
    "query": "Are there any research papers on the texture bias in CNNs and methods to enhance shape-based representations for improved robustness?"
  },
  {
    "index": 306,
    "source_corpus_id": 256105351,
    "ref_id": "b2",
    "citation_corpus_id": 67855617,
    "start": 3840,
    "end": 3857,
    "title": "ROTATE: KNOWLEDGE GRAPH EMBEDDING BY RELA- TIONAL ROTATION IN COMPLEX SPACE",
    "abstract": "We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.Published as a conference paper at ICLR 2019ModelScore FunctionSE (Bordes et al., 2011)− Wr,1h − Wr,2t h, t ∈ R k , Wr,· ∈ R k×k TransE(Bordes et al., 2013)− h + r − t h, r, t ∈ R k TransX − gr,1(h) + r − gr,2(t) h, r, t ∈ R k DistMult(Yang et al., 2014)r, h, t h, r, t ∈ R k ComplEx(Trouillon et al., 2016)Re( r, h, t ) h, r, t ∈ C k HolE(Nickel et al., 2016)r, h ⊗ t h, r, t ∈ R k ConvE(Dettmers et al., 2017)σ (vec(σ([r, h]   * Ω))W ), t h, r, t ∈ R k RotatE − h • r − t 2 h, r, t ∈ C k , |ri| = 1",
    "prev": "This issue is well known as the Open World Assumption (OWA) (Ji et al., 2021).",
    "curr": "Representation learning methods are employed to mitigate the incompleteness issue by learning representations from the observed KG triples and generalizing them to unseen triples (Bordes et al., 2013;Trouillon et al., 2016;Sun et al., 2018;Zhang et al., 2019;Chami et al., 2020).",
    "next": "When considering logical queries over incomplete knowledge graphs, the query answering models are required to not only predict the unseen knowledge but also execute logical operators, such as conjunction, disjunction, and negation (Ren & Leskovec, 2020;Wang et al., 2021b).",
    "query": "Are there any research papers on knowledge graph embedding techniques that model relational patterns through rotations in complex space and address logical query answering with representation learning?"
  },
  {
    "index": 308,
    "source_corpus_id": 259075184,
    "ref_id": "b25",
    "citation_corpus_id": 245704504,
    "start": 3001,
    "end": 3020,
    "title": "SDEDIT: GUIDED IMAGE SYNTHESIS AND EDITING WITH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Guided image synthesis enables everyday users to create and edit photo-realistic images with minimum effort. The key challenge is balancing faithfulness to the user inputs (e.g., hand-drawn colored strokes) and realism of the synthesized images. Existing GAN-based methods attempt to achieve such balance using either conditional GANs or GAN inversions, which are challenging and often require additional training data or loss functions for individual applications. To address these issues, we introduce a new image synthesis and editing method, Stochastic Differential Editing (SDEdit), based on a diffusion model generative prior, which synthesizes realistic images by iteratively denoising through a stochastic differential equation (SDE). Given an input image with user guide in a form of manipulating RGB pixels, SDEdit first adds noise to the input, then subsequently denoises the resulting image through the SDE prior to increase its realism. SDEdit does not require task-specific training or inversions and can naturally achieve the balance between realism and faithfulness. SDEdit outperforms state-of-the-art GAN-based methods by up to 98.09% on realism and 91.72% on overall satisfaction scores, according to a human perception study, on multiple tasks, including stroke-based image synthesis and editing as well as image compositing.",
    "prev": " in various applications, including but not limited to object detection (Misra et al., 2021;Liu et al., 2021b;Wang et al., 2022a), segmentation (Qian et al., 2022;Tang et al., 2022;Zhao et al., 2021), and tracking (Qi et al., 2020;Zheng et al., 2021;Shan et al., 2021).",
    "curr": "Despite the significant advances in 3D point cloud processing, acquiring task-specific 3D annotations is a highly expensive and severely limited process due to the geometric complexity.The shortage of data annotations highlights the need for adapting pre-training paradigms.Instead of training the deep network from randomly initialized weights, prior work suggests that pre-training the network on a relevant but different pre-task and later fine-tuning the weights using task-specific labels often leads to superior performance.In natural language processing and 2D vision, pre-trained models are the backbones of many exciting applications, such as real-time chatbots (Touvron et al., 2023;OpenAI, 2023) and graphic designers (Meng et al., 2021;Wang et al., 2022b).However, pre-training on point clouds has yet to demonstrate a universal performance improvement.From-scratch training remains a common practice in 3D vision.",
    "next": "Initial attempts towards 3D point-cloud pre-training primarily leverage contrastive learning (Chopra et al., 2005), especially when the point clouds are collected from indoor scenes (Xie et al., 2020;Rao et al., 2021;Liu et al., 2021a;Zhang et al., 2021;Chen et al., 2022).However, the broad application of contrastive learning-based pre-training techniques is impeded by the requirement of large batch sizes and the necessity to carefully define positive and negative pairs.In contrast to natural language processing and 2D vision, pre-training on 3D point clouds presents two unique challenges.First, the data is extremely scarce, even without annotations.Public 3D datasets are orders of magnitude smaller than 2D image datasets.Second, the lack of data annotations necessitates 3D pre-training methods to adhere to t",
    "query": "Are there any research papers on pre-training methods for 3D point cloud processing that address data scarcity and annotation challenges?"
  },
  {
    "index": 309,
    "source_corpus_id": 263153393,
    "ref_id": "b44",
    "citation_corpus_id": 252715594,
    "start": 1751,
    "end": 1773,
    "title": "PHENAKI: VARIABLE LENGTH VIDEO GENERATION FROM OPEN DOMAIN TEXTUAL DESCRIPTIONS",
    "abstract": "We present Phenaki, a model capable of realistic video synthesis, given a sequence of textual prompts. Generating videos from text is particularly challenging due to the computational cost, limited quantities of high quality text-video data and variable length of videos. To address these issues, we introduce a new model for learning video representation which compresses the video to a small representation of discrete tokens. This tokenizer uses causal attention in time, which allows it to work with variable-length videos. To generate video tokens from text we are using a bidirectional masked transformer conditioned on pre-computed text tokens. The generated video tokens are subsequently de-tokenized to create the actual video. To address data issues, we demonstrate how joint training on a large corpus of image-text pairs as well as a smaller number of video-text examples can result in generalization beyond what is available in the video datasets. Compared to the previous video generation methods, Phenaki can generate arbitrary long videos conditioned on a sequence of prompts (i.e. time variable text or a story) in open domain. To the best of our knowledge, this is the first time a paper studies generating videos from time variable prompts. In addition, compared to the perframe baselines, the proposed video encoder-decoder computes fewer tokens per video but results in better spatio-temporal consistency. ‡ Equal contribution.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any studies on variable-length video generation from open-domain textual descriptions using causal attention and bidirectional masked transformers?"
  },
  {
    "index": 310,
    "source_corpus_id": 258048805,
    "ref_id": "b16",
    "citation_corpus_id": 8968704,
    "start": 2926,
    "end": 2942,
    "title": "A Neural Representation of Sketch Drawings",
    "abstract": "We present sketch-rnn, a recurrent neural network (RNN) able to construct stroke-based drawings of common objects. The model is trained on a dataset of human-drawn images representing many different classes. We outline a framework for conditional and unconditional sketch generation, and describe new robust training methods for generating coherent sketch drawings in a vector format.",
    "prev": "Recently however, with the advent of large-scale datasets, generative modelling of chirographic data started to gain traction.",
    "curr": "Specifically, models have been trained on generic doodles/drawings data (Ha & Eck, 2018), or more \"specialized\" entities like fonts (Lopes et al., 2019), diagrams (Gervais et al., 2020;, SVG Icons (Carlier et al., 2020) etc.",
    "next": "Building unconditional neural generative models not only allows understanding the distribution of chirographic data but also enables further downstream tasks (e.g.",
    "query": "Are there any research papers on generative models for sketch drawings using recurrent neural networks and large-scale datasets of doodles, fonts, and diagrams?"
  },
  {
    "index": 311,
    "source_corpus_id": 221971169,
    "ref_id": "b18",
    "citation_corpus_id": 202539918,
    "start": 2223,
    "end": 2243,
    "title": "Mogrifier LSTM",
    "abstract": "Many advances in Natural Language Processing have been based upon more expressive models for how inputs interact with the context in which they occur. Recurrent networks, which have enjoyed a modicum of success, still lack the generalization and systematicity ultimately required for modelling language. In this work, we propose an extension to the venerable Long Short-Term Memory in the form of mutual gating of the current input and the previous output. This mechanism affords the modelling of a richer space of interactions between inputs and their context. Equivalently, our model can be viewed as making the transition function given by the LSTM context-dependent. Experiments demonstrate markedly improved generalization on language modelling in the range of 3-4 perplexity points on Penn Treebank and Wikitext-2, and 0.01-0.05 bpc on four character-based datasets. We establish a new state of the art on all datasets with the exception of Enwik8, where we close a large gap between the LSTM and Transformer models.",
    "prev": "For example, syntactic effects evolve at the timescale of words, whereas semantics, emotions, and narratives can evolve at much longer timescales of tens to hundreds or thousands of words.",
    "curr": "The importance of long timescale information is evident in results showing that neural networks have outperformed classical n-gram models on many language modeling benchmarks (Melis et al., 2019;Krause et al., 2019;Dai et al., 2019).",
    "next": "This difference is attributed to these networks' ability to capture long timescale dependencies that that are impossible for n-gram models.",
    "query": "Are there any research papers on models that capture long timescale dependencies in natural language processing using LSTM or similar architectures?"
  },
  {
    "index": 312,
    "source_corpus_id": 259203153,
    "ref_id": "b42",
    "citation_corpus_id": 211678094,
    "start": 2479,
    "end": 2498,
    "title": "ADAPTIVE FEDERATED OPTIMIZATION",
    "abstract": "Federated learning is a distributed machine learning paradigm in which a large number of clients coordinate with a central server to learn a model without sharing their own training data. Standard federated optimization methods such as Federated Averaging (FEDAVG) are often difficult to tune and exhibit unfavorable convergence behavior. In non-federated settings, adaptive optimization methods have had notable success in combating such issues. In this work, we propose federated versions of adaptive optimizers, including ADAGRAD, ADAM, and YOGI, and analyze their convergence in the presence of heterogeneous data for general nonconvex settings. Our results highlight the interplay between client heterogeneity and communication efficiency. We also perform extensive experiments on these methods and show that the use of adaptive optimizers can significantly improve the performance of federated learning. * Authors contributed equally to this work arXiv:2003.00295v5 [cs.LG] 8 Sep 2021Published as a conference paper at ICLR 2021 adaptivity by using adaptive optimizers as client or server optimizers. Building upon this, we develop novel adaptive optimization techniques for FL by using per-coordinate methods as server optimizers. By focusing on adaptive server optimization, we enable use of adaptive learning rates without increase in client storage or communication costs, and ensure compatibility with cross-device FL.Main contributions In light of the above, we highlight the main contributions of the paper.• We study a general framework for federated optimization using server and client optimizers. This framework generalizes many existing federated optimization methods, including FEDAVG. • We use this framework to design novel, cross-device compatible, adaptive federated optimization methods, and provide convergence analysis in general nonconvex settings. To the best of our knowledge, these are the first methods for FL using adaptive server optimization. We show an important interplay between the number of local steps and the heterogeneity among clients. • We introduce comprehensive and reproducible empirical benchmarks for comparing federated optimization methods. These benchmarks consist of seven diverse and representative FL tasks involving both image and text data, with varying amounts of heterogeneity and numbers of clients. • We demonstrate strong empirical performance of our adaptive optimizers throughout, improving upon commonly used baselines. Our results show that our methods can be easier to tune, and highlight their utility in cross-device settings.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on adaptive federated optimization methods, such as those using ADAGRAD, ADAM, or YOGI, and their effectiveness in handling heterogeneous data in nonconvex settings?"
  },
  {
    "index": 313,
    "source_corpus_id": 252715881,
    "ref_id": "b22",
    "citation_corpus_id": 235614244,
    "start": 9912,
    "end": 9916,
    "title": "IS ATTENTION BETTER THAN MATRIX DECOMPOSITION?",
    "abstract": "As an essential ingredient of modern deep learning, attention mechanism, especially self-attention, plays a vital role in the global correlation discovery. However, is hand-crafted attention irreplaceable when modeling the global context? Our intriguing finding is that self-attention is not better than the matrix decomposition (MD) model developed 20 years ago regarding the performance and computational cost for encoding the long-distance dependencies. We model the global context issue as a low-rank recovery problem and show that its optimization algorithms can help design global information blocks. This paper then proposes a series of Hamburgers, in which we employ the optimization algorithms for solving MDs to factorize the input representations into sub-matrices and reconstruct a low-rank embedding. Hamburgers with different MDs can perform favorably against the popular global context module self-attention when carefully coping with gradients back-propagated through MDs. Comprehensive experiments are conducted in the vision tasks where it is crucial to learn the global context, including semantic segmentation and image generation, demonstrating significant improvements over self-attention and its variants. Code is available.",
    "prev": "In terms of training implicit models, [25] also proposed a novel gradient estimate called phantom gradient which relies on fixed-point unrolling and a Neumann series to provide a new update direction; computation of precise gradient is forgone.",
    "curr": "Implicit models have also been extended to more complex learning frameworks, such as attention mechanisms [24] and Graph Neural Networks [30].",
    "next": "Preliminary: Differentiable optimization layers\n\nWe consider a parameterized convex optimization problems with polyhedral constraints:\nmin x f (x; θ) s.t.",
    "query": "Are there any studies comparing attention mechanisms with matrix decomposition approaches in vision tasks using optimization-based methods?"
  },
  {
    "index": 314,
    "source_corpus_id": 52894384,
    "ref_id": "b5",
    "citation_corpus_id": 17263016,
    "start": 4997,
    "end": 5017,
    "title": "Low Resource Dependency Parsing: Cross-lingual Parameter Sharing in a Neural Network Parser",
    "abstract": "Training a high-accuracy dependency parser requires a large treebank. However, these are costly and time-consuming to build. We propose a learning method that needs less data, based on the observation that there are underlying shared structures across languages. We exploit cues from a different source language in order to guide the learning process. Our model saves at least half of the annotation effort to reach the same accuracy compared with using the purely supervised method.",
    "prev": "RELATED WORK\n\nMulti-task learning.",
    "curr": "The dominant approach to multi-task learning is to have a model that shares parameters in a soft (Duong et al., 2015;Yang & Hospedales, 2017) or hard way (Caruana, 1993).",
    "next": "Soft sharing refers to each task having independent weights that are constrained to be similar (e.g.",
    "query": "Are there any research papers on cross-lingual parameter sharing in neural network parsers for low-resource dependency parsing?"
  },
  {
    "index": 317,
    "source_corpus_id": 257631995,
    "ref_id": "b22",
    "citation_corpus_id": 3687922,
    "start": 2887,
    "end": 2909,
    "title": "SEMI-PARAMETRIC TOPOLOGICAL MEMORY FOR NAVIGATION",
    "abstract": "We introduce a new memory architecture for navigation in previously unseen environments, inspired by landmark-based navigation in animals. The proposed semiparametric topological memory (SPTM) consists of a (non-parametric) graph with nodes corresponding to locations in the environment and a (parametric) deep network capable of retrieving nodes from the graph based on observations. The graph stores no metric information, only connectivity of locations corresponding to the nodes. We use SPTM as a planning module in a navigation system. Given only 5 minutes of footage of a previously unseen maze, an SPTM-based navigation agent can build a topological map of the environment and use it to confidently navigate towards goals. The average success rate of the SPTM agent in goal-directed navigation across test environments is higher than the best-performing baseline by a factor of three.",
    "prev": "This implies that a goal-conditioned policy is replaceable by a policy conditioned on a \"subgoal\" existing between the goal and the agent.",
    "curr": "Based on this insight, researchers have investigated graph-based planning to construct a goal-reaching path by (a) proposing a series of subgoals and (b) executing policies conditioned on the nearest subgoal (Savinov et al., 2018;Eysenbach et al., 2019;Huang et al., 2019).",
    "next": "Since the nearby subgoals are easier to reach than the faraway goal, such planning improves the success ratio of the agent reaching the target-goal during sample collection.",
    "query": "Are there any research papers on graph-based planning methods for navigation using subgoals and topological memory architectures?"
  },
  {
    "index": 318,
    "source_corpus_id": 231839495,
    "ref_id": "b28",
    "citation_corpus_id": 3162051,
    "start": 3130,
    "end": 3150,
    "title": "mixup: BEYOND EMPIRICAL RISK MINIMIZATION",
    "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks. * Alphabetical order.",
    "prev": "To improve the generalization performance of the neural networks, a body of research has been proposed to develop regularizers based on priors or to augment the training data with task-dependent transforms (Bishop, 2006;Cubuk et al., 2019).",
    "curr": "Recently, a new taskindependent data augmentation technique, called mixup, has been proposed (Zhang et al., 2018).",
    "next": "The original mixup, called Input Mixup, linearly interpolates a given pair of input data and can be easily applied to various data and tasks, improving the generalization performance and robustness of neural networks.",
    "query": "Are there any research papers on task-independent data augmentation techniques that improve neural network generalization and robustness to adversarial examples?"
  },
  {
    "index": 320,
    "source_corpus_id": 67856680,
    "ref_id": "b1",
    "citation_corpus_id": 2129889,
    "start": 1851,
    "end": 1874,
    "title": "Learning End-to-End Goal-Oriented Dialog",
    "abstract": "End-to-end dialog systems, in which all components are learnt simultaneously, have recently obtained encouraging successes. However these were mostly on conversations related to chit-chat with no clear objective and for which evaluation is difficult. This paper proposes a set of tasks to test the capabilities of such systems on goal-oriented dialogs, where goal completion ensures a well-defined measure of performance. Built in the context of restaurant reservation, our tasks require to manipulate sentences and symbols, in order to properly conduct conversations, issue API calls and use the outputs of such calls. We show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations. We confirm those results by comparing our system to a hand-crafted slot-filling baseline on data from the second Dialog State Tracking Challenge(Henderson et al., 2014a).Currently, the most useful applications of dialog systems are goal-oriented and transactional: the system is expected to understand a user request and complete a related task with a clear goal within",
    "prev": "Based on our results, we argue that AQM+ is a general task-oriented dialog algorithm that can be applied for non-yes-or-no responses.",
    "curr": "INTRODUCTION\n\nRecent advances in deep learning have led an end-to-end neural approach to task-oriented dialog problems that can reduce a laborious labeling task on states and intents (Bordes & Weston, 2017).",
    "next": "Many researchers have applied sequence-to-sequence models (Vinyals & Le, 2015) that are trained in a supervised learning (SL) and a reinforcement learning (RL) fashion to generate an appropriate sentence for the task.",
    "query": "Are there any research papers on end-to-end goal-oriented dialog systems using neural networks and sequence-to-sequence models trained with supervised and reinforcement learning?"
  },
  {
    "index": 323,
    "source_corpus_id": 222380524,
    "ref_id": "b13",
    "citation_corpus_id": 49411844,
    "start": 7315,
    "end": 7332,
    "title": "DARTS: Differentiable Architecture Search",
    "abstract": "This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques.",
    "prev": "AutoML is a long-pursued target of machine learning (He et al., 2019).",
    "curr": "Recently a sub-field of AutoML, neural architecture search (NAS), has attracted much attention due to its success in automating the process of neural network architecture design (Zoph & Le, 2017;Pham et al., 2018;Liu et al., 2018).",
    "next": "As an essential element, loss function has also raised the interest of researchers to automate its design process.",
    "query": "Are there any research papers on differentiable methods for neural architecture search and automated design of neural network architectures?"
  },
  {
    "index": 325,
    "source_corpus_id": 238419701,
    "ref_id": "b17",
    "citation_corpus_id": 3568073,
    "start": 5182,
    "end": 5203,
    "title": "PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION",
    "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 1024 2 . We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.",
    "prev": "Existing generative models are mostly designed to synthesize images of fixed sizes.",
    "curr": "A few methods (Karras et al., 2018; have been recently developed to train latent generative models on high-resolution images, up to 1024×1024 pixels.",
    "next": "However, latent generative models generate images from dense latent vectors that require synthesizing all structural contents at once.",
    "query": "Are there any studies that explore progressive training methodologies for generative adversarial networks to enhance high-resolution image synthesis with improved quality and stability?"
  },
  {
    "index": 326,
    "source_corpus_id": 209202457,
    "ref_id": "b14",
    "citation_corpus_id": 108304275,
    "start": 2334,
    "end": 2357,
    "title": "DISCRIMINATOR-ACTOR-CRITIC: ADDRESSING SAMPLE INEFFICIENCY AND REWARD BIAS IN ADVERSARIAL IMITATION LEARNING",
    "abstract": "We identify two issues with the family of algorithms based on the Adversarial Imitation Learning framework. The first problem is implicit bias present in the reward functions used in these algorithms. While these biases might work well for some environments, they can also lead to sub-optimal behavior in others. Secondly, even though these algorithms can learn from few expert demonstrations, they require a prohibitively large number of interactions with the environment in order to imitate the expert for many real-world applications. In order to address these issues, we propose a new algorithm called Discriminator-Actor-Critic that uses off-policy Reinforcement Learning to reduce policy-environment interaction sample complexity by an average factor of 10. Furthermore, since our reward function is designed to be unbiased, we can apply our algorithm to many problems without making any task-specific adjustments.",
    "prev": "This realization is at the heart of imitation learning (Ho & Ermon, 2016;Pomerleau, 1989), in which one aims to learn a behavior policy from a set of expert demonstrations -logged experience data of a near-optimal policy interacting with the environment -without explicit knowledge of rewards.",
    "curr": "Distribution matching via adversarial learning, or Adversarial Imitation Learning (AIL), has recently become a popular approach for imitation learning (Ho & Ermon, 2016;Fu et al., 2017;Ke et al., 2019;Kostrikov et al., 2019).",
    "next": "These methods interpret the states and actions provided in the expert demonstrations as a finite sample from a target distribution.",
    "query": "Are there any studies that address sample inefficiency and reward bias in adversarial imitation learning?"
  },
  {
    "index": 328,
    "source_corpus_id": 258999368,
    "ref_id": "b12",
    "citation_corpus_id": 6628106,
    "start": 25083,
    "end": 25102,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "(2020).",
    "curr": "All models were trained for 200 epochs using Adam (Kingma & Ba, 2015) with the standard hyperparameters of β 1 = 0.9, β 2 = 0.999, a learning rate of 0.001, and a batch size of 500 on a single A5000 GPU using PyTorch (Paszke et al., 2019).",
    "next": "We did not use any additional training heuristics such as Dropout (Srivastava et al., 2014).",
    "query": "Are there any research papers on the Adam optimization method for stochastic optimization and its implementation details?"
  },
  {
    "index": 329,
    "source_corpus_id": 248239812,
    "ref_id": "b18",
    "citation_corpus_id": 233714270,
    "start": 28337,
    "end": 28354,
    "title": "Published as a conference paper at ICLR 2021 LATENT SKILL PLANNING FOR EXPLORATION AND TRANSFER",
    "abstract": "To quickly solve new tasks in complex environments, intelligent agents need to build up reusable knowledge. For example, a learned world model captures knowledge about the environment that applies to new tasks. Similarly, skills capture general behaviors that can apply to new tasks. In this paper, we investigate how these two approaches can be integrated into a single reinforcement learning agent. Specifically, we leverage the idea of partial amortization for fast adaptation at test time. For this, actions are produced by a policy that is learned over time while the skills it conditions on are chosen using online planning. We demonstrate the benefits of our design decisions across a suite of challenging locomotion tasks and demonstrate improved sample efficiency in single tasks as well as in transfer from one task to another, as compared to competitive baselines. Videos are available at: https://sites.google.com/view/latent-skill-planning/ * Kevin and Homanga contributed equally to this work.arXiv:2011.13897v2 [cs.LG] 2 May 2021Published as a conference paper at ICLR 2021 search procedure such as Cross-Entropy Method (CEM; Rubinstein, 1997) on the learned models to determine which action to execute next. Since rollouts from the learned dynamics and reward models are not executed in the actual environment during training, these learned models are sometimes also referred to as imagination models(Hafner et al., 2018;. Fully amortized methods such as Dreamer(Hafner et al., 2019), train a reactive policy with many rollouts from the imagination model. They then execute the resulting policy in the environment.",
    "prev": "RELATED WORKS\n\nVisual model-based RL.",
    "curr": "Recent developments in video prediction and contrastive learning have enabled learning of world-models from images (Watter et al., 2015;Babaeizadeh et al., 2017;Hafner et al., 2019a;Ha & Schmidhuber, 2018;Hafner et al., 2019b;Xie et al., 2020).",
    "next": "All of these approaches learn latent representations through reconstruction objectives that are amenable for planning.",
    "query": "Are there any research papers on integrating latent skill planning with model-based reinforcement learning for exploration and transfer in complex environments?"
  },
  {
    "index": 330,
    "source_corpus_id": 219721308,
    "ref_id": "b13",
    "citation_corpus_id": 15329665,
    "start": 9319,
    "end": 9323,
    "title": "Discovering Diverse and Salient Threads in Document Collections",
    "abstract": "We propose a novel probabilistic technique for modeling and extracting salient structure from large document collections. As in clustering and topic modeling, our goal is to provide an organizing perspective into otherwise overwhelming amounts of information. We are particularly interested in revealing and exploiting relationships between documents. To this end, we focus on extracting diverse sets of threads-singlylinked, coherent chains of important documents. To illustrate, we extract research threads from citation graphs and construct timelines from news articles. Our method is highly scalable, running on a corpus of over 30 million words in about four minutes, more than 75 times faster than a dynamic topic model. Finally, the results from our model more closely resemble human news summaries according to several metrics and are also preferred by human judges.",
    "prev": "As in prior DPP work, our decomposition has inner dimension K that could be as large as M , but is usually much smaller in practice.",
    "curr": "Our algorithms work well for modest values of K. In cases where the natural K is larger (e.g., natural language processing applications where K might be the number of words), random projections can often be used to significantly reduce K [14].",
    "next": "New kernel decomposition and scalable learning\n\nPrior work on NDPPs proposed a maximum likelihood estimation (MLE) learning algorithm [13].",
    "query": "Are there any research papers on probabilistic methods for discovering diverse and salient document threads with scalable learning algorithms, particularly in applications like news summarization and citation analysis?"
  },
  {
    "index": 331,
    "source_corpus_id": 204734215,
    "ref_id": "b4",
    "citation_corpus_id": 52889459,
    "start": 20843,
    "end": 20862,
    "title": "LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS",
    "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by truncating the latent space. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128×128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.3 and Fréchet Inception Distance (FID) of 9.6, improving over the previous best IS of 52.52 and FID of 18.65.",
    "prev": "(2019) attempt to solve the undesirable convergence issue of GDA by exploiting curvature information, but they focus on simultaneous game on finding local Nash and it is unclear how to extend their algorithm to sequential games.",
    "curr": "For GAN training, there is a rich literature on different strategies to make the GAN-game welldefined, e.g., by adding instance noise (Salimans et al., 2016), by using different objectives (Nowozin et al., 2016;Gulrajani et al., 2017;Mao et al., 2017) or by tweaking the architectures (Radford et al., 2015;Brock et al., 2019).",
    "next": "While these strategies try to make the overall optimization problem easily, our work deals with a specific optimization problem and convergence issues arise in theory and in practice; hence our algorithm is orthogonal to these work.",
    "query": "Are there any research papers on large-scale GAN training techniques that address convergence issues and improve high-fidelity natural image synthesis?"
  },
  {
    "index": 334,
    "source_corpus_id": 234790212,
    "ref_id": "b26",
    "citation_corpus_id": 1957433,
    "start": 25519,
    "end": 25544,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "Scene Prior (SP) (Yang et al., 2018) learns a graph neural network from the FastText database (Joulin et al., 2016) and leverages the scene prior knowledge and category relationships for navigation.",
    "curr": "Word Embedding (WE) uses GloVe embedding (Pennington et al., 2014) to indicate the target category rather than detection.",
    "next": "The association between object appearances and GloVe embeddings is learned through trail and error.",
    "query": "Are there any research papers on global logbilinear regression models for word representation using co-occurrence statistics?"
  },
  {
    "index": 335,
    "source_corpus_id": 236965836,
    "ref_id": "b7",
    "citation_corpus_id": 28202810,
    "start": 25760,
    "end": 25783,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "SQIL (Reddy et al., 2020) denotes the SQIL heuristic.",
    "curr": "In order to ensure a fair comparison among algorithms, we re-implemented all of them using the same basis RL algorithm, SAC (Haarnoja et al., 2018).",
    "next": "All imitation learning agents were run for 500000 environment interactions.",
    "query": "Are there any research papers on off-policy maximum entropy reinforcement learning algorithms, particularly those using SAC and comparing imitation learning agents in continuous control tasks?"
  },
  {
    "index": 336,
    "source_corpus_id": 247518687,
    "ref_id": "b3",
    "citation_corpus_id": 231592851,
    "start": 3189,
    "end": 3215,
    "title": "Published as a conference paper at ICLR 2021 ESTIMATING AND EVALUATING REGRESSION PREDIC- TIVE UNCERTAINTY IN DEEP OBJECT DETECTORS",
    "abstract": "Predictive uncertainty estimation is an essential next step for the reliable deployment of deep object detectors in safety-critical tasks. In this work, we focus on estimating predictive distributions for bounding box regression output with variance networks. We show that in the context of object detection, training variance networks with negative log likelihood (NLL) can lead to high entropy predictive distributions regardless of the correctness of the output mean. We propose to use the energy score as a non-local proper scoring rule and find that when used for training, the energy score leads to better calibrated and lower entropy predictive distributions than NLL. We also address the widespread use of non-proper scoring metrics for evaluating predictive distributions from deep object detectors by proposing an alternate evaluation approach founded on proper scoring rules. Using the proposed evaluation tools, we show that although variance networks can be used to produce high quality predictive distributions, adhoc approaches used by seminal object detectors for choosing regression targets during training do not provide wide enough data support for reliable variance learning. We hope that our work helps shift evaluation in probabilistic object detection to better align with predictive uncertainty evaluation in other machine learning domains. Code for all models, evaluation, and datasets is available at: https://github.com/asharakeh/probdet.git. distance-sensitive proper scoring rule based on energy statistics(Székely & Rizzo, 2013), as an alternative for training variance networks. We show that predictive distributions learnt with the energy score are lower entropy, better calibrated, and of higher quality when evaluated using proper scoring rules.Pitfalls of EvaluationWe address the widespread use of non-proper scoring rules for evaluating probabilistic object detectors by providing evaluation tools based on well established proper scoring rules(Gneiting & Raftery, 2007)that are only minimized if the estimated predictive distribution is equal to the true target distribution, for both classification and regression. Using the proposed tools, we benchmark probabilistic extensions of three common object detection architectures on in-distribution, shifted, and out-of-distribution data. Our results show that variance networks can differentiate between in-distribution, shifted, and out-of-distribution data using their predictive entropy. We find that ad-hoc approaches used by seminal object detectors for choosing their regression targets during training do not provide a wide enough data support for reliable learning in variance networks. Finally, we provide clear recommendations in Sec. 5 to avoid the pitfalls described above.",
    "prev": "On the other hand, quantifying aleatoric uncertainty enables learning of dynamics models of stochastic processes (e.g.",
    "curr": "for model-based or offline reinforcement learning) (Chua et al., 2018;Yu et al., 2020), improves performance in semantic segmentation, depth regression and object detection (Kendall & Gal, 2017;Harakeh & Waslander, 2021), and allows for risk-sensitive decision making (Dabney et al., 2018;Vlastelica et al., 2021).",
    "next": "We examine a common approach for quantifying aleatoric uncertainty in neural network regression.",
    "query": "Are there any research papers on methods for estimating and evaluating predictive uncertainty in deep object detectors using variance networks and proper scoring rules?"
  },
  {
    "index": 337,
    "source_corpus_id": 247594344,
    "ref_id": "b8",
    "citation_corpus_id": 208309981,
    "start": 13058,
    "end": 13076,
    "title": "Published as a conference paper at ICLR 2021 HOW MUCH OVER-PARAMETERIZATION IS SUFFI- CIENT TO LEARN DEEP RELU NETWORKS?",
    "abstract": "A recent line of research on deep learning focuses on the extremely overparameterized setting, and shows that when the network width is larger than a high degree polynomial of the training sample size n and the inverse of the target error ´1 , deep neural networks learned by (stochastic) gradient descent enjoy nice optimization and generalization guarantees. Very recently, it is shown that under certain margin assumptions on the training data, a polylogarithmic width condition suffices for two-layer ReLU networks to converge and generalize (Ji and Telgarsky, 2020). However, whether deep neural networks can be learned with such a mild over-parameterization is still an open question. In this work, we answer this question affirmatively and establish sharper learning guarantees for deep ReLU networks trained by (stochastic) gradient descent. In specific, under certain assumptions made in previous work, our optimization and generalization guarantees hold with network width polylogarithmic in n and ´1 . Our results push the study of over-parameterized deep neural networks towards more practical settings. * Equal contribution.",
    "prev": "REGRET ANALYSIS\n\nIn this section, we provide finite time regret analysis of NPR, where the time horizon T is set beforehand.",
    "curr": "The theoretical analysis is built on the recent studies about the generalization of DNN models Chen et al., 2020;Daniely, 2017;Arora et al., 2019), which illustrate that with (stochastic) gradient descent, the learned parameters of a DNN locate in a particular regime with the generalization error being characterized by the best function in the corresponding neural tangent kernel (NTK) space (Jacot et al., 2018).",
    "next": "We leave the background of NTK in the appendix and focus on the key steps of our proof in this section.",
    "query": "Are there any research papers on the sufficient over-parameterization for learning deep ReLU networks using regret analysis and generalization guarantees through neural tangent kernels?"
  },
  {
    "index": 339,
    "source_corpus_id": 5034059,
    "ref_id": "b27",
    "citation_corpus_id": 3626819,
    "start": 3984,
    "end": 4005,
    "title": "Deep contextualized word representations",
    "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
    "prev": "We find that unified multi-task trained models slightly outperform comparable models trained on each task separately.",
    "curr": "Our best multitask model makes use of ELMo (Peters et al., 2018), a recently proposed pre-training technique.",
    "next": "However, this model still achieves a fairly low absolute score, indicating room for improved general NLU systems.",
    "query": "Are there any research papers on deep contextualized word representations using pre-trained bidirectional language models and their impact on natural language understanding tasks?"
  },
  {
    "index": 341,
    "source_corpus_id": 233204603,
    "ref_id": "b37",
    "citation_corpus_id": 224705241,
    "start": 19699,
    "end": 19703,
    "title": "PSEUDOSEG: DESIGNING PSEUDO LABELS FOR SEMANTIC SEGMENTATION",
    "abstract": "Recent advances in semi-supervised learning (SSL) demonstrate that a combination of consistency regularization and pseudo-labeling can effectively improve image classification accuracy in the low-data regime. Compared to classification, semantic segmentation tasks require much more intensive labeling costs. Thus, these tasks greatly benefit from data-efficient training methods. However, structured outputs in segmentation render particular difficulties (e.g., designing pseudo-labeling and augmentation) to apply existing SSL strategies. To address this problem, we present a simple and novel re-design of pseudo-labeling to generate well-calibrated structured pseudo labels for training with unlabeled or weakly-labeled data. Our proposed pseudo-labeling strategy is network structure agnostic to apply in a one-stage consistency training framework. We demonstrate the effectiveness of the proposed pseudo-labeling strategy in both low-data and high-data regimes. Extensive experiments have validated that pseudo labels generated from wisely fusing diverse sources and strong data augmentation are crucial to consistency training for segmentation. The source code is available at https://github.com/googleinterns/wss.",
    "prev": "In fully supervised learning, we simply applied ReCo on top of standard supervised learning.",
    "curr": "[38].",
    "next": "The percentage and the number of labelled data used are listed in the first row.",
    "query": "Are there any research papers on pseudo-labeling strategies for semi-supervised semantic segmentation with consistency training?"
  },
  {
    "index": 342,
    "source_corpus_id": 208202124,
    "ref_id": "b9",
    "citation_corpus_id": 12730344,
    "start": 2902,
    "end": 2927,
    "title": "An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks",
    "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models \"forget\" how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithmthe dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests that the choice of activation function should always be cross-validated.",
    "prev": "Excessive adaptation could lead to inadvertent forgetting of how to perform earlier tasks.",
    "curr": "Indeed, catastrophic forgetting is one of the main pathologies in continual learning (McCloskey and Cohen, 1989;Ratcliff, 1990;Robins, 1993Robins, , 1995French, 1999;Pape et al., 2011;Goodfellow et al., 2014a;Achille et al., 2018;Kemker et al., 2018;Kemker and Kanan, 2018;Diaz-Rodriguez et al., 2018;Zeno et al., 2018;Ahn et al., 2019;Parisi et al., 2019;Pfulb and Gepperth, 2019;Rajasegaran et al., 2019).",
    "next": "Many approaches to continual learning employ an architecture which is divided a priori into (i) a slowly evolving, global part; and (ii) a quickly evolving, task-specific, local part.",
    "query": "Are there any research papers on methods to mitigate catastrophic forgetting in gradient-based neural networks for continual learning?"
  },
  {
    "index": 343,
    "source_corpus_id": 257365037,
    "ref_id": "b21",
    "citation_corpus_id": 3366315,
    "start": 3577,
    "end": 3598,
    "title": "Published as a conference paper at ICLR 2018 SPECTRAL NORMALIZATION FOR GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. The code with Chainer (Tokui et al., 2015), generated images and pretrained models are available at https://github.com/pfnet-research/sngan_ projection.Published as a conference paper at ICLR 2018• Lipschitz constant is the only hyper-parameter to be tuned, and the algorithm does not require intensive tuning of the only hyper-parameter for satisfactory performance. • Implementation is simple and the additional computational cost is small.",
    "prev": " in our original ICLR paper are not accurate.We elaborate on the issue and provides some fix in Appendix D.\n\nINTRODUCTION\n\nThe robustness of deep neural networks is nowadays a great challenge to establish confidence in their decisions for real-life applications.Addressing this challenge requires guarantees on the stability of the prediction, with respect to adversarial attacks.In this context, the Lipschitz constant of neural networks is a key property at the core of many recent advances.Along with the margin of the classifier, this property allows us to certify the robustness against worst-case adversarial perturbations.This certification is based on a sphere of stability within which the decision remains the same for any perturbation inside the sphere (Tsuzuku et al., 2018).",
    "curr": "The design of 1-Lipschitz layers provides a successful approach to enforce this property for the whole neural network.For this purpose, many different techniques have been devised such as spectral normalization (Miyato et al., 2018;Farnia et al., 2019), orthogonal parameterization (Trockman et al., 2021;Li et al., 2019;Singla et al., 2021;Yu et al., 2022;Xu et al., 2022), Convex Potential Layers (CPL) (Meunier et al., 2022), and Almost-Orthogonal-Layers (AOL) (Prach et al., 2022).While all these techniques share the same goal, their motivations, and derivations can greatly differ, delivering different solutions.Nevertheless, their raw experimental comparison fails to really gain insight into their peculiar performance, soundness, and in the end their possible complementarity.Therefore a question acts as a barrier for an in-depth analysis and future development:\n\nAre there common principles underlying the developments of 1-Lipschitz Layers?",
    "next": "In this paper, we propose a novel perspective to answer this question based on a unified Semidefinite Programming (SDP) approach.We introduce a common algebraic condition underlying various types of methods like spectral normalization, orthogonality-based methods, AOL, and CPL",
    "query": "Are there any research papers on the common principles underlying methods for enforcing 1-Lipschitz layers in neural networks, such as spectral normalization, orthogonal parameterization, and Convex Potential Layers?"
  },
  {
    "index": 345,
    "source_corpus_id": 238354021,
    "ref_id": "b23",
    "citation_corpus_id": 12663716,
    "start": 7909,
    "end": 7932,
    "title": "PIXELCNN++: IMPROVING THE PIXELCNN WITH DISCRETIZED LOGISTIC MIXTURE LIKELIHOOD AND OTHER MODIFICATIONS",
    "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.",
    "prev": "Order Agnostic ARDMs The main difficulty of parameterizing an autoregressive model from an engineering perspective, is the need to enforce the triangular or causal dependence.",
    "curr": "Especially for 2D signals, this triangular dependence is difficult to enforce for arbitrary orders (Jain et al., 2020) and tedious design is needed for multi-scale architectures (Salimans et al., 2017).",
    "next": "To relax this requirement, we take inspiration from modern diffusion-based generative models.",
    "query": "Are there any research papers on autoregressive generative models with modifications to address triangular dependence in 2D signals using diffusion-inspired approaches?"
  },
  {
    "index": 346,
    "source_corpus_id": 218720006,
    "ref_id": "b10",
    "citation_corpus_id": 28202810,
    "start": 5107,
    "end": 5130,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "We then show that if we define the trust-region w.r.t.",
    "curr": "the uniform policy, instead of the old one, our off-policy MDPO coincides with the popular soft actor-critic (SAC) algorithm (Haarnoja et al., 2018).",
    "next": "We discuss this connection in detail (Section 4.2) and empirically compare these algorithms using the same set of continuous control problems (Section 5.4).",
    "query": "Are there any research papers on off-policy maximum entropy reinforcement learning algorithms, such as soft actor-critic, and their applications in continuous control tasks?"
  },
  {
    "index": 347,
    "source_corpus_id": 247618912,
    "ref_id": "b13",
    "citation_corpus_id": 3633127,
    "start": 67142,
    "end": 67164,
    "title": "Published as a conference paper at ICLR 2018 CGANS WITH PROJECTION DISCRIMINATOR",
    "abstract": "We propose a novel, projection based way to incorporate the conditional information into the discriminator of GANs that respects the role of the conditional information in the underlining probabilistic model. This approach is in contrast with most frameworks of conditional GANs used in application today, which use the conditional information by concatenating the (embedded) conditional vector to the feature vectors. With this modification, we were able to significantly improve the quality of the class conditional image generation on ILSVRC2012 (Im-ageNet) 1000-class image dataset from the current state-of-the-art result, and we achieved this with a single pair of a discriminator and a generator. We were also able to extend the application to super-resolution and succeeded in producing highly discriminative super-resolution images. This new structure also enabled high quality category transformation based on parametric functional transformation of conditional batch normalization layers in the generator. The code with Chainer (Tokui et al., 2015), generated images and pretrained models are available at https://github.com/pfnet-research/sngan_projection.",
    "prev": "A potential downside to modeling latent factors in generative models is a decrease in image quality of generated samples that has been noted when disentanglement terms are added(Burgess et al., 2018; Khrulkov et al., 2021).",
    "curr": "Prior work has studied how to integrate additional information into GAN training, in particular ground truth class labels(Mirza & Osindero, 2014;Salimans et al., 2016;Odena, 2016;Odena et al., 2017;  Brock et al., 2019;Thekumparampil et al., 2018;Miyato & Koyama, 2018; Lučić et al., 2019), also considering noisy scenarios(Kaneko et al., 2019).",
    "next": "However, in the programmatic weak supervision setting, having multiple noisy sources of imperfect labels that include abstains present large hurdles to similar conditional modeling.",
    "query": "Are there any research papers on projection-based methods for incorporating conditional information into conditional GANs to improve image quality and handle noisy label scenarios?"
  },
  {
    "index": 348,
    "source_corpus_id": 238857286,
    "ref_id": "b25",
    "citation_corpus_id": 14124313,
    "start": 13094,
    "end": 13122,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "The pixel-wise loss L pixel combines 1 and 2 distance as:\nL pixel (p,p) = λ 1 1 (p,p) + λ 2 2 (p,p),(9)\nwhere λ 1 and λ 2 are two hyper-parameters to control the relative importance.",
    "curr": "For the perceptual loss L perceptual , we extract features from conv5 in a VGG-16 network (Simonyan & Zisserman, 2015) pretrained on ImageNet (Krizhevsky et al., 2012) and combine the 1 and 2 distance as:\nL perceptual (p,p) = λ 3 1 (φ(p), φ(p)) + λ 4 2 (φ(p), φ(p)),(10)\nwhere φ(·) represents the output of conv5 in the VGG-16 network, and λ 3 and λ 4 are two hyperparameters.",
    "next": "Compared to the pixel-wise loss, the perceptual loss is better to capture the region-wise structure, which reflects the waveform coherence.",
    "query": "Are there any studies on the use of perceptual loss with VGG-16 features in deep convolutional networks for image recognition?"
  },
  {
    "index": 349,
    "source_corpus_id": 259287063,
    "ref_id": "b46",
    "citation_corpus_id": 213729382,
    "start": 2381,
    "end": 2398,
    "title": "Published as a conference paper at ICLR 2020 DYNAMIC MODEL PRUNING WITH FEEDBACK",
    "abstract": "Deep neural networks often have millions of parameters. This can hinder their deployment to low-end devices, not only due to high memory requirements but also because of increased latency at inference. We propose a novel model compression method that generates a sparse trained model without additional overhead: by allowing (i) dynamic allocation of the sparsity pattern and (ii) incorporating feedback signal to reactivate prematurely pruned weights we obtain a performant sparse model in one single training pass (retraining is not needed, but can further improve the performance). We evaluate our method on CIFAR-10 and ImageNet, and show that the obtained sparse models can reach the state-of-the-art performance of dense models. Moreover, their performance surpasses that of models generated by all previously proposed pruning schemes. arXiv:2006.07253v1 [cs.LG] 12 Jun 2020 2 RELATED WORK Previous works on obtaining pruned networks can (loosely) be divided into three main categories.Pruning after training. Training approaches to obtain sparse networks usually include a three stage pipeline-training of a dense model, one-shot pruning and fine-tuning-e.g., (Han et al.",
    "prev": "weight decay yields models suitable for averaging, sharing identical sparse connectivity by design.Averaging these models significantly enhances generalization and OOD performance over their individual counterparts.Building on this, we introduce SPARSE MODEL SOUPS (SMS), a novel method for merging sparse models by initiating each prune-retrain cycle with the averaged model from the previous phase.SMS preserves sparsity, exploits sparse network benefits, is modular and fully parallelizable, and substantially improves IMP's performance.We further demonstrate that SMS can be adapted to enhance state-of-the-art pruning-during-training approaches.",
    "curr": "INTRODUCTION\n\nState-of-the-art Neural Network architectures typically rely on extensive over-parameterization with millions or billions of parameters (Zhang et al., 2016).In consequence, these models have significant memory requirements and the training and inference process is computationally demanding.However, recent work (e.g.Han et al., 2015;Lin et al., 2020;Renda et al., 2020;Zimmer et al., 2022) has demonstrated that these resource demands can be significantly reduced by pruning the model, i.e., removing redundant structures such as individual parameters or groups thereof.The resulting sparse models demand considerably less storage and floating-point operations (FLOPs) during inference, while retaining performance comparable to dense models.",
    "next": "A different line of research has shown that the performance of a predictor can be significantly enhanced by leveraging multiple models, instead of selecting the best one on a hold-out validation dataset and discarding the rest.Such ensembles combine the predictions of m ∈ N individually trained models by averaging their output predictions (Ganaie et al., 2021;Mehrtash et al., 2020;Chandak et al., 2023;Fort et al., 2019).Prediction ensembles have been shown to improve the predictive performance and positively impact predictive uncertainty metrics such as calibration, out-of-distribution generalizatio",
    "query": "Are there any research papers on methods for merging sparse neural networks through averaging, dynamic pruning with feedback mechanisms, or improving model performance via ensembling pruned architectures?"
  },
  {
    "index": 350,
    "source_corpus_id": 249097923,
    "ref_id": "b16",
    "citation_corpus_id": 3526769,
    "start": 6705,
    "end": 6724,
    "title": "MITIGATING ADVERSARIAL EFFECTS THROUGH RAN- DOMIZATION",
    "abstract": "Convolutional neural networks have demonstrated their powerful ability on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. I.e., clean images, with imperceptible perturbations added, can easily cause convolutional neural networks to fail. In this paper, we propose to utilize randomization to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method also enjoys the following advantages: 1) no additional training or fine-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it achieves a normalized score of 0.924 (ranked No.2 among 107 defense teams) in the NIPS 2017 adversarial examples defense challenge, which is far better than using adversarial training alone with a normalized score of 0.773 (ranked No.56). The code is public available at https: /. R-fcn: Object detection via region-based fully convolutional networks.",
    "prev": "In addition to the attention terms, AoA and ATA also include the typical attack losses, e.g., logit output in their objective functions, and use a hyperparameter to balance the two terms.",
    "curr": "Adversarial Perturbations (TAP) (Zhou et al., 2018), Activation attack (AA) (Inkawhich et al., 2019) and Intermediate Level Attack (ILA) (Huang et al., 2019), which all directly maximize the distance between feature maps of benign images and adversarial examples, also belong to this category.",
    "next": "TAP and AA generate adversarial examples by employing multi-layer and single-layer feature maps respectively.",
    "query": "Are there any research papers on adversarial defense methods utilizing randomization techniques to mitigate effects of attacks targeting feature map distances in neural networks?"
  },
  {
    "index": 351,
    "source_corpus_id": 252668422,
    "ref_id": "b52",
    "citation_corpus_id": 244954585,
    "start": 2842,
    "end": 2868,
    "title": "REPRESENTATIONS OF THE HIPPOCAMPAL FORMATION",
    "abstract": "Many deep neural network architectures loosely based on brain networks have recently been shown to replicate neural firing patterns observed in the brain.One of the most exciting and promising novel architectures, the Transformer neural network, was developed without the brain in mind.In this work, we show that transformers, when equipped with recurrent position encodings, replicate the precisely tuned spatial representations of the hippocampal formation; most notably place and grid cells.Furthermore, we show that this result is no surprise since it is closely related to current hippocampal models from neuroscience.We additionally show the transformer version offers dramatic performance gains over the neuroscience version.This work continues to bind computations of artificial and brain networks, offers a novel understanding of the hippocampal-cortical interaction, and suggests how wider cortical areas may perform complex tasks beyond current neuroscience models such as language comprehension.",
    "prev": "Transformer, originally designed for natural language processing (Vaswani et al., 2017), has flourished for various tasks in computer vision, including image classification (Dosovitskiy et al., 2020;Yuan et al., 2021a), object detection (Carion et al., 2020;Zhu et al., 2020;, semantic segmentation Yuan et al., 2021b) and low-level image processing (Chen et al., 2021).",
    "curr": "Self-attention, the key part of Transformer, selectively focuses on information of interest, and is also an important feature of the human biological system (Whittington et al., 2022;Caucheteux & King, 2022).",
    "next": "Intuitively, it is intriguing to explore applying self-attention in SNNs for more advanced deep learning, considering the biological properties of the two mechanisms.",
    "query": "Are there any research papers on the application of self-attention mechanisms in Transformers for modeling hippocampal representations in neuroscience?"
  },
  {
    "index": 355,
    "source_corpus_id": 3525232,
    "ref_id": "b13",
    "citation_corpus_id": 12639289,
    "start": 2536,
    "end": 2566,
    "title": "Recurrent Continuous Translation Models",
    "abstract": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.",
    "prev": "Neural network architectures lie at the heart of a variety of applications.",
    "curr": "They are practically ubiquitous across vision tasks (LeCun et al., 1995;Krizhevsky et al., 2012;Simonyan & Zisserman, 2014) and natural language understanding, from machine translation (Kalchbrenner & Blunsom, 2013;Sutskever et al., 2014;Bahdanau et al., 2014) to textual entailment (Bowman et al., 2015;Rocktäschel et al., 2015) via sentiment analysis (Socher et al., 2013;Kalchbrenner et al., 2014) and reading comprehension Hill et al., 2015;Rajpurkar et al., 2016).",
    "next": "They have been used to synthesise programs (Ling et al., 2016;Parisotto et al., 2016;Devlin et al., 2017) or internalise algorithms (Graves et al., 2016;Joulin & Mikolov, 2015;Kaiser & Sutskever, 2015;Reed & De Freitas, 2015).",
    "query": "Are there any research papers on recurrent continuous translation models that use continuous representations instead of alignments for machine translation?"
  },
  {
    "index": 357,
    "source_corpus_id": 220280819,
    "ref_id": "b24",
    "citation_corpus_id": 14717992,
    "start": 1630,
    "end": 1633,
    "title": "REINFORCEMENT LEARNING WITH UNSUPERVISED AUXILIARY TASKS",
    "abstract": "Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-theart on Atari, averaging 880% expert human performance, and a challenging suite of first-person, three-dimensional Labyrinth tasks leading to a mean speedup in learning of 10× and averaging 87% expert human performance on Labyrinth.Natural and artificial agents live in a stream of sensorimotor data. At each time step t, the agent receives observations o t and executes actions a t . These actions influence the future course of the sensorimotor stream. In this paper we develop agents that learn to predict and control this stream, by solving a host of reinforcement learning problems, each focusing on a distinct feature of the sensorimotor stream. Our hypothesis is that an agent that can flexibly control its future experiences will also be able to achieve any goal with which it is presented, such as maximising its future rewards.The classic reinforcement learning paradigm focuses on the maximisation of extrinsic reward. However, in many interesting domains, extrinsic rewards are only rarely observed. This raises questions of what and how to learn in their absence. Even if extrinsic rewards are frequent, the sensorimotor stream contains an abundance of other possible learning targets. Traditionally, unsupervised learning attempts to reconstruct these targets, such as the pixels in the current or subsequent frame. It is typically used to accelerate the acquisition of a useful representation. In contrast, our learning objective is to predict and control features of the sensorimotor stream, by treating them as pseudorewards for reinforcement learning. Intuitively, this set of tasks is more closely matched with the agent's long-term goals, potentially leading to more useful representations.Consider a baby that learns to maximise the cumulative amount of red that it observes. To correctly predict the optimal value, the baby must understand how to increase \"redness\" by various means, including manipulation (bringing a red object closer to the eyes); locomotion (moving in front of a red object); and communication (crying until the parents bring a red object). These behaviours are likely to recur for many other goals that the baby may subsequently encounter. No understanding of these behaviours is required to simply reconstruct the redness of current or subsequent images.Our architecture uses reinforcement learning to approximate both the optimal policy and optimal value function for many different pseudo-rewards. It also makes other auxiliary predictions that serve to focus the agent on important aspects of the task. These include the long-term goal of predicting cumulative extrinsic reward as well as short-term predictions of extrinsic reward. To learn more efficiently, our agents use an experience replay mechanism to provide additional updates * Joint first authors. Ordered alphabetically by first name.",
    "prev": "Introduction\n\nReinforcement learning (RL) provides a general solution for an agent to solve a given task by learning from its own experiences of interacting with the environment.",
    "curr": "In various domains of careful task designs and informative rewards, such as game playing [3,25,37] and robotics [1,11], RL algorithms have achieved significant progress on solving challenging sequential decision making problems.",
    "next": "However, the effectiveness of such a trial-and-error learning paradigm relies on the agent's ability of exploring the task environment and collecting informative experiences.",
    "query": "Are there any research papers on reinforcement learning with unsupervised auxiliary tasks or pseudo-rewards for improving learning efficiency and performance in complex environments?"
  },
  {
    "index": 359,
    "source_corpus_id": 3529936,
    "ref_id": "b28",
    "citation_corpus_id": 11758569,
    "start": 2445,
    "end": 2449,
    "title": "UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks -demonstrating their applicability as general image representations.arXiv:1511.06434v2 [cs.LG] 7 Jan 2016Under review as a conference paper at ICLR 2016• We show that the generators have interesting vector arithmetic properties allowing for easy manipulation of many semantic qualities of generated samples.",
    "prev": "These are omnipresent in scientific and engineering research involving data analysis, for instance ecology, climate science and geography, where simulators are used to fit real-world observations to produce forecasting results.",
    "curr": "Within the machine learning community there is a recent interest in a specific type of implicit models, generative adversarial networks (GANs) [10], which has been shown to be one of the most successful approaches to image and text generation [29,49,2,5].",
    "next": "Very recently, implicit distributions have also been considered as approximate posterior distributions for Bayesian inference, e.g.",
    "query": "Are there any research papers on unsupervised representation learning using deep convolutional generative adversarial networks (DCGANs) for image datasets?"
  },
  {
    "index": 360,
    "source_corpus_id": 232478335,
    "ref_id": "b28",
    "citation_corpus_id": 67856276,
    "start": 1815,
    "end": 1832,
    "title": "MULTILINGUAL NEURAL MACHINE TRANSLATION WITH KNOWLEDGE DISTILLATION",
    "abstract": "Multilingual machine translation, which translates multiple languages with a single model, has attracted much attention due to its efficiency of offline training and online serving. However, traditional multilingual translation usually yields inferior accuracy compared with the counterpart using individual models for each language pair, due to language diversity and model capacity limitations. In this paper, we propose a distillation-based approach to boost the accuracy of multilingual machine translation. Specifically, individual models are first trained and regarded as teachers, and then the multilingual model is trained to fit the training data and match the outputs of individual models simultaneously through knowledge distillation. Experiments on IWSLT, WMT and Ted talk translation datasets demonstrate the effectiveness of our method. Particularly, we show that one model is enough to handle multiple languages (up to 44 languages in our experiment), with comparable or even better accuracy than individual models. * Authors contribute equally to this work.",
    "prev": "Finally, we broadly discuss several circumstances wherein label smoothing will indeed lose its effectiveness.",
    "curr": "INTRODUCTION\n\nLabel smoothing (Szegedy et al., 2016) and knowledge distillation (Hinton et al., 2015) are two commonly recognized techniques in training deep neural networks and have been applied in many state-of-the-art models, such as language translation (Vaswani et al., 2017;Tan et al., 2019;Zhou et al., 2020), image classification  and speech recognition (Chiu et al., 2018;Pereyra et al., 2017;Chorowski & Jaitly, 2017).",
    "next": "Recently a large body of studies is focusing on exploring the underlying relationships between these two methods, for instance, Müller et al.",
    "query": "Are there any research papers on knowledge distillation methods for improving multilingual machine translation accuracy?"
  },
  {
    "index": 361,
    "source_corpus_id": 227254803,
    "ref_id": "b33",
    "citation_corpus_id": 3502463,
    "start": 4467,
    "end": 4489,
    "title": "Published as a conference paper at ICLR 2018 DEEP BAYESIAN BANDITS SHOWDOWN AN EMPIRICAL COMPARISON OF BAYESIAN DEEP NETWORKS FOR THOMPSON SAMPLING",
    "abstract": "Recent advances in deep reinforcement learning have made significant strides in performance on applications such as Go and Atari games. However, developing practical methods to balance exploration and exploitation in complex domains remains largely unsolved. Thompson Sampling and its extension to reinforcement learning provide an elegant approach to exploration that only requires access to posterior samples of the model. At the same time, advances in approximate Bayesian methods have made posterior approximation for flexible neural network models practical. Thus, it is attractive to consider approximate Bayesian neural networks in a Thompson Sampling framework. To understand the impact of using an approximate posterior on Thompson Sampling, we benchmark well-established and recently developed methods for approximate posterior sampling combined with Thompson Sampling over a series of contextual bandit problems. We found that many approaches that have been successful in the supervised learning setting underperformed in the sequential decision-making scenario. In particular, we highlight the challenge of adapting slowly converging uncertainty estimates to the online setting. * Google AI Resident Published as a conference paper at ICLR 2018 1: Input: Prior distribution over models, π 0 : θ ∈ Θ → [0, 1]. 2: for time t = 0, . . . , N do 3:Observe context X t ∈ R d .4:Sample model θ t ∼ π t .5:Compute a t = BestAction(X t , θ t ).6:Select action a t and observe reward r t .7:Update posterior distribution π t+1 with (X t , a t , r t ).In the following sections we rely on the idea that, if we had access to the actual posterior π t given the observed data at all times t, then choosing actions using Thompson Sampling would lead to near-optimal cumulative regret or, more informally, to good performance. It is important to remark that in some problems this is not necessarily the case; for example, when actions that have no chance of being optimal still convey useful information about other actions. Thompson Sampling (or UCB approaches) would never select such actions, even if they are worth their cost (Russo & Van Roy,  2014). In addition, Thompson Sampling does not take into account the time horizon where the process ends, and if known, exploration efforts should be tuned accordingly (Russo et al., 2017). Nonetheless, under the assumption that very accurate posterior approximations lead to efficient decisions, the question is: what happens when the approximations are not so accurate? In some cases, the mismatch in posteriors may not hurt in terms of decision making, and we will still end up with good decisions. Unfortunately, in other cases, this mismatch together with its induced feedback loop will degenerate in a significant loss of performance. We would like to understand the main aspects that determine which way it goes. This is an important practical question as, in large and complex systems, computational sacrifices and statistical assumptions are made to favor simplicity and tractability. But, what is their impact?",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any studies that compare Bayesian deep networks for Thompson sampling in contextual bandit problems, focusing on challenges in sequential decision-making and uncertainty estimation in online settings?"
  },
  {
    "index": 362,
    "source_corpus_id": 208857488,
    "ref_id": "b45",
    "citation_corpus_id": 3531730,
    "start": 17563,
    "end": 17588,
    "title": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks",
    "abstract": "We present a generalization bound for feedforward neural networks in terms of the product of the spectral norms of the layers and the Frobenius norm of the weights. The generalization bound is derived using a PAC-Bayes analysis.",
    "prev": "Experiments\n\n\nOverparamterized LQR\n\nWe first analyze the case of the LQR as a surrogate for what may occur in deep RL, which has been done before for various topics such as sample complexity (Dean et al., 2019) and model-based RL .",
    "curr": "This is analogous to analyzing linear/logistic regression (Kakade et al., 2008;McAllester, 2003) as a surrogate to understanding extensions to deep SL techniques (Neyshabur et al., 2018a;Bartlett et al., 2017).",
    "next": "In particular, this has numerous benefits -the cost (negative of reward) function is deterministic, and allows exact gradient descent (i.e.",
    "query": "Are there any research papers on PAC-Bayesian generalization bounds for neural networks using spectral norms and Frobenius norms?"
  },
  {
    "index": 364,
    "source_corpus_id": 245117682,
    "ref_id": "b22",
    "citation_corpus_id": 28202810,
    "start": 2632,
    "end": 2655,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "INTRODUCTION\n\nReinforcement learning (RL) has suffered for years from a curse of poor sample complexity.",
    "curr": "State-ofthe-art model-free reinforcement learning algorithms routinely take tens of thousands of sampled transitions to solve very simple tasks and millions to solve moderately complex ones (Haarnoja et al., 2018;Lillicrap et al., 2015).",
    "next": "The current best model-based reinforcement learning (MBRL) algorithms are better, requiring thousands of samples for simple problems and hundreds of thousands of samples for harder ones (Chua et al., 2018).",
    "query": "Are there any research papers on off-policy maximum entropy reinforcement learning methods that address high sample complexity and improve stability in model-free algorithms?"
  },
  {
    "index": 365,
    "source_corpus_id": 256390058,
    "ref_id": "b10",
    "citation_corpus_id": 3484654,
    "start": 21628,
    "end": 21647,
    "title": "RECASTING GRADIENT-BASED META-LEARNING AS HIERARCHICAL BAYES",
    "abstract": "Meta-learning allows an intelligent agent to leverage prior learning episodes as a basis for quickly improving performance on a novel task. Bayesian hierarchical modeling provides a theoretical framework for formalizing meta-learning as inference for a set of parameters that are shared across tasks. Here, we reformulate the model-agnostic meta-learning algorithm (MAML) of Finn et al.(2017)as a method for probabilistic inference in a hierarchical Bayesian model. In contrast to prior methods for meta-learning via hierarchical Bayes, MAML is naturally applicable to complex function approximators through its use of a scalable gradient descent procedure for posterior inference. Furthermore, the identification of MAML as hierarchical Bayes provides a way to understand the algorithm's operation as a meta-learning procedure, as well as an opportunity to make use of computational strategies for efficient inference. We use this opportunity to propose an improvement to the MAML algorithm that makes use of techniques from approximate inference and curvature estimation.",
    "prev": "Meta learning is a learning framework that aims to adapt or generalize well on new tasks.",
    "curr": "There are three approaches in meta learning: metric-based (Koch et al., 2015;Vinyals et al., 2016;Sung et al., 2018;Snell et al., 2017), model-based (Santoro et al., 2016Munkhdalai & Yu, 2017;Grant et al., 2018) and optimization-based (Finn et al., 2017;Nichol et al., 2018).",
    "next": "Neural processes (NPs) is the model-based meta learning with stochasticity.",
    "query": "Are there any research papers on gradient-based meta-learning approaches and their connection to hierarchical Bayesian modeling?"
  },
  {
    "index": 366,
    "source_corpus_id": 67856276,
    "ref_id": "b26",
    "citation_corpus_id": 1998416,
    "start": 1685,
    "end": 1705,
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "abstract": "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1",
    "prev": "* Authors contribute equally to this work.",
    "curr": "INTRODUCTION\n\nNeural Machine Translation (NMT) has witnessed rapid development in recent years (Bahdanau et al., 2015;Luong et al., 2015b;Wu et al., 2016;Gehring et al., 2017;Vaswani et al., 2017;Guo et al., 2018;Shen et al., 2018), including advanced model structures (Gehring et al., 2017;Vaswani et al., 2017; and human parity achievements .",
    "next": "While conventional NMT can well handle single pair translation, training a separate model for each language pair is resource consuming, considering there are thousands of languages in the world 1 .",
    "query": "Are there any studies that explore different attention architectures for neural machine translation and their effectiveness in improving translation performance?"
  },
  {
    "index": 367,
    "source_corpus_id": 235293986,
    "ref_id": "b8",
    "citation_corpus_id": 56657912,
    "start": 26709,
    "end": 26739,
    "title": "BENCHMARKING NEURAL NETWORK ROBUSTNESS TO COMMON CORRUPTIONS AND PERTURBATIONS",
    "abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, IMAGENET-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called IMAGENET-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.",
    "prev": "Response to gradual domain shift.",
    "curr": "In order to assess how well models can cope with a gradual transition from their native domain, we evaluate their ECE performance on data perturbed by 19 types of corruption (Hendrycks & Dietterich, 2019) at five different severity levels.",
    "next": "Figure 3 depicts the performance of models averaged across all corruptions.",
    "query": "Are there any research papers on benchmarking neural network robustness to common corruptions and perturbations using standardized datasets and evaluation metrics?"
  },
  {
    "index": 371,
    "source_corpus_id": 235417023,
    "ref_id": "b37",
    "citation_corpus_id": 604334,
    "start": 28490,
    "end": 28511,
    "title": "Intriguing properties of neural networks",
    "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
    "prev": "Before that we review adversarial attacks widely used for evaluating and improving robustness.",
    "curr": "Adversarial attack\n\nSince the realization of the adversarial example phenomenon (Biggio et al., 2013;Szegedy et al., 2014), tons of adversarial attacks have been proposed (Moosavi-Dezfooli et al., 2016;Goodfellow et al., 2014;Carlini & Wagner, 2017;Dong et al., 2018;Tu et al., 2019;Madry et al., 2017).",
    "next": "Among these attacks, PGD attack (Madry et al., 2017) is one of the most commonly used attacks.",
    "query": "Are there any studies that investigate the properties of adversarial attacks and their implications for the robustness of neural networks?"
  },
  {
    "index": 372,
    "source_corpus_id": 49473438,
    "ref_id": "b25",
    "citation_corpus_id": 11758569,
    "start": 5993,
    "end": 5996,
    "title": "UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks -demonstrating their applicability as general image representations.arXiv:1511.06434v2 [cs.LG] 7 Jan 2016Under review as a conference paper at ICLR 2016• We show that the generators have interesting vector arithmetic properties allowing for easy manipulation of many semantic qualities of generated samples.",
    "prev": "Various ideas have been proposed to further improve the quality of the learned distributions and the stability of the training.",
    "curr": "(See e.g., [1,24,15,26,33,28,16,9,38] and the reference therein.)",
    "next": "However, understanding of GANs is still in its infancy.",
    "query": "Are there any research papers on unsupervised representation learning using deep convolutional generative adversarial networks (DCGANs) and their applications in image tasks?"
  },
  {
    "index": 373,
    "source_corpus_id": 54101493,
    "ref_id": "b1",
    "citation_corpus_id": 68222714,
    "start": 5709,
    "end": 5732,
    "title": "APPROXIMATING CNNS WITH BAG-OF-LOCAL- FEATURES MODELS WORKS SURPRISINGLY WELL ON IMAGENET",
    "abstract": "Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difficult to understand how they reach their decisions. We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain. Our model, a simple variant of the ResNet-50 architecture called BagNet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering. This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet (87.6% top-5 for 33 × 33 px features and Alexnet performance for 17 × 17 px features). The constraint on local features makes it straight-forward to analyse how exactly each part of the image influences the classification. Furthermore, the BagNets behave similar to state-of-the art deep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of feature sensitivity, error distribution and interactions between image parts. This suggests that the improvements of DNNs over previous bag-of-feature classifiers in the last few years is mostly achieved by better fine-tuning rather than by qualitatively different decision strategies.",
    "prev": "On the other hand, some rather disconnected findings point to an important role of object textures for CNN object recognition.",
    "curr": "CNNs can still classify texturised images perfectly well, even if the global shape structure is completely destroyed Brendel & Bethge, 2019).",
    "next": "Conversely, standard CNNs are bad at recognising object sketches where object shapes are preserved yet all texture cues are missing (Ballester & de Araújo, 2016).",
    "query": "Are there any research papers on the role of texture versus shape in CNN object recognition and the effectiveness of bag-of-features models in approximating CNNs?"
  },
  {
    "index": 376,
    "source_corpus_id": 252816031,
    "ref_id": "b28",
    "citation_corpus_id": 240288835,
    "start": 7385,
    "end": 7403,
    "title": "MetaICL: Learning to Learn In Context",
    "abstract": "We introduce MetaICL (Meta-training for In-Context Learning), a new meta-training framework for few-shot learning where a pretrained language model is tuned to do in-context learning on a large set of training tasks. This metatraining enables the model to more effectively learn a new task in context at test time, by simply conditioning on a few training examples with no parameter updates or task-specific templates. We experiment on a large, diverse collection of tasks consisting of 142 NLP datasets including classification, question answering, natural language inference, paraphrase detection and more, across seven different metatraining/target splits. MetaICL outperforms a range of baselines including in-context learning without meta-training and multi-task learning followed by zero-shot transfer. We find that the gains are particularly significant for target tasks that have domain shifts from the meta-training tasks, and that using a diverse set of the meta-training tasks is key to improvements. We also show that MetaICL approaches (and sometimes beats) the performance of models fully finetuned on the target task, and outperforms much bigger models with nearly 8x parameters. Finally, we show that MetaICL is complementary to human-written instructions, and the best performance can be achieved by combining both approaches. Tom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21). .2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In EMNLP.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on meta-training frameworks for in-context learning in few-shot natural language processing tasks?"
  },
  {
    "index": 379,
    "source_corpus_id": 236912505,
    "ref_id": "b34",
    "citation_corpus_id": 3075448,
    "start": 2712,
    "end": 2715,
    "title": "HIGH-DIMENSIONAL CONTINUOUS CONTROL USING GENERALIZED ADVANTAGE ESTIMATION",
    "abstract": "Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(λ). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.Published as a conference paper at ICLR 2016 λ ∈ [0, 1], the generalized advantage estimator (GAE). Related methods have been proposed in the context of online actor-critic methods(Kimura & Kobayashi, 1998;Wawrzyński, 2009). We provide a more general analysis, which is applicable in both the online and batch settings, and discuss an interpretation of our method as an instance of reward shaping(Ng et al., 1999), where the approximate value function is used to shape the reward.We present experimental results on a number of highly challenging 3D locomotion tasks, where we show that our approach can learn complex gaits using high-dimensional, general purpose neural network function approximators for both the policy and the value function, each with over 10 4 parameters. The policies perform torque-level control of simulated 3D robots with up to 33 state dimensions and 10 actuators.The contributions of this paper are summarized as follows:1. We provide justification and intuition for an effective variance reduction scheme for policy gradients, which we call generalized advantage estimation (GAE). While the formula has been proposed in prior work(Kimura & Kobayashi, 1998;Wawrzyński, 2009), our analysis is novel and enables GAE to be applied with a more general set of algorithms, including the batch trust-region algorithm we use for our experiments. 2. We propose the use of a trust region optimization method for the value function, which we find is a robust and efficient way to train neural network value functions with thousands of parameters. 3. By combining(1)and(2)above, we obtain an algorithm that empirically is effective at learning neural network policies for challenging control tasks. The results extend the state of the art in using reinforcement learning for high-dimensional continuous control. Videos are available at https://sites.google.com/site/gaepapersupp.",
    "prev": "The value function is learned using experience of the agent acting in the environment via temporal difference (TD) methods [39], which regress the value function to a target based on a combination of groundtruth returns achieved in the environment and the approximate value function itself.",
    "curr": "The need to bootstrap learning of the value function on its own estimates is known to lead to difficulties in practice, where one must achieve a careful balance between bias and variance [15,49,35,23].",
    "next": "If a slight imbalance arises, the consequences can be disastrous for learning [45,46,41].",
    "query": "Are there any research papers on reinforcement learning methods for high-dimensional continuous control using generalized advantage estimation and trust region optimization?"
  },
  {
    "index": 380,
    "source_corpus_id": 261823054,
    "ref_id": "b54",
    "citation_corpus_id": 204509033,
    "start": 23353,
    "end": 23357,
    "title": "UNDERSTANDING THE LIMITATIONS OF VARIATIONAL MUTUAL INFORMATION ESTIMATORS",
    "abstract": "Variational approaches based on neural networks are showing promise for estimating mutual information (MI) between high dimensional variables. However, they can be difficult to use in practice due to poorly understood bias/variance tradeoffs. We theoretically show that, under some conditions, estimators such as MINE exhibit variance that could grow exponentially with the true amount of underlying MI. We also empirically demonstrate that existing estimators fail to satisfy basic self-consistency properties of MI, such as data processing and additivity under independence. Based on a unified perspective of variational approaches, we develop a new estimator that focuses on variance reduction. Empirical results on standard benchmark tasks demonstrate that our proposed estimator exhibits improved biasvariance trade-offs on standard benchmark tasks.",
    "prev": "Models\n\nWe evaluate representations obtained using correlation maximization methods based either on linear projections (TICA) (43) or non-linear encoders (VAMPNet) (39)  in Appendix E.1.We use a conditional Flow++ architecture (25) to model the variational transition distribution q ϕ (z t |z t−τ ).This is because of the modeling flexibility, the tractability of the likelihood, and the possibility of directly sampling to unfold latent simulations.Multi-layer perceptrons (MLPs) are used to model q ψ (y t |z t ), mapping the representations z t into the logits of a categorical distribution over the target y t .For all objectives, we use the same encoder, transition, and predictive architectures.",
    "curr": "Quantitative evaluation We estimate the autoinformation of the representations AI([z t ] T t=s ; τ ) at several lag time τ using SMILE (54) and we measure the amount of information that the representations contain about the targets of interest I(z t ; y t ) by difference of discrete entropies: H(y t ) − H(y t |z t ) (47; 54; 41).These measurements aim to validate the theory presented in Section 2 and better characterize the representations obtained with the different models.",
    "next": "Following the graphical model in Figure 2, given an initial system state x s of a test trajectory [x t ] T t=s and the sequence of corresponding targets [y t ] T t=s , we use the trained encoder, transition, and prediction models to unfold trajectories [ỹ s+kτ ] K k=1 ∼ q LS ([y s+kτ ] K k=1 |x s ) that cover the same temporal span as the test trajectory (K = ⌊(T − s)/τ ⌋).Similarly to previous work (4), for evaluation purposes, we consider only discrete targets y t so that we can estimate the marginal and transition probabilities for the ground truth and unfolded trajectories by counting the frequency of each state and the corresponding transition m",
    "query": "Are there any studies that investigate the limitations of variational mutual information estimators, particularly their bias-variance tradeoffs and challenges in ensuring self-consistency properties like data processing and additivity?"
  },
  {
    "index": 381,
    "source_corpus_id": 225067567,
    "ref_id": "b23",
    "citation_corpus_id": 40100965,
    "start": 2198,
    "end": 2220,
    "title": "Universal Language Model Fine-tuning for Text Classification",
    "abstract": "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100× more data. We opensource our pretrained models and code 1 .",
    "prev": "2  We focus on encoder-only models, and do not consider encoder-decoder models like T5 (Raffel et al.,  2020)  where none of the embedding matrices are discarded after pre-training.",
    "curr": "Output embeddings may also be\n\nINTRODUCTION\n\nThe performance of models in natural language processing (NLP) has dramatically improved in recent years, mainly driven by advances in transfer learning from large amounts of unlabeled data (Howard & Ruder, 2018;Devlin et al., 2019).",
    "next": "The most successful paradigm consists of pre-training a large Transformer (Vaswani et al., 2017) model with a self-supervised loss and fine-tuning it on data of a downstream task (Ruder et al., 2019).",
    "query": "Are there any research papers on effective transfer learning methods for NLP tasks using pre-trained language models with fine-tuning techniques?"
  },
  {
    "index": 382,
    "source_corpus_id": 256358895,
    "ref_id": "b1",
    "citation_corpus_id": 222398658,
    "start": 6650,
    "end": 6668,
    "title": null,
    "abstract": "Explainability in AI is crucial for model development, compliance with regulation, and providing operational nuance to predictions.The Shapley framework for explainability attributes a model's predictions to its input features in a mathematically principled and model-agnostic way.However, general implementations of Shapley explainability make an untenable assumption: that the model's features are uncorrelated.In this work, we demonstrate unambiguous drawbacks of this assumption and develop two solutions to Shapley explainability that respect the data manifold.One solution, based on generative modelling, provides flexible access to data imputations; the other directly learns the Shapley value-function, providing performance and stability at the cost of flexibility.While \"off-manifold\" Shapley values can (i) give rise to incorrect explanations, (ii) hide implicit model dependence on sensitive attributes, and (iii) lead to unintelligible explanations in higher-dimensional data, on-manifold explainability overcomes these problems.",
    "prev": "However, we find that most existing masking methods are not satisfactory from this perspective (see Section 2.1 and Table 1).",
    "curr": "The masking method based on conditional distribution (Covert et al., 2020b;Frye et al., 2021) performs a bit better but still has problems.",
    "next": "In particular, we notice that Shapley values can be derived from effects of causal patterns in theory, i.e., the causal patterns are proven to be elementary effects of Shapley values.",
    "query": "Are there any studies that address the limitations of Shapley explainability in AI due to the assumption of uncorrelated features, and propose solutions such as generative modeling or direct learning of the Shapley value-function?"
  },
  {
    "index": 383,
    "source_corpus_id": 259108558,
    "ref_id": "b35",
    "citation_corpus_id": 174802916,
    "start": 2625,
    "end": 2629,
    "title": "Published as a conference paper at ICLR 2020 SCALING AUTOREGRESSIVE VIDEO MODELS",
    "abstract": "Due to the statistical complexity of video, the high degree of inherent stochasticity, and the sheer amount of data, generating natural video remains a challenging task. State-of-the-art video generation models often attempt to address these issues by combining sometimes complex, usually video-specific neural network architectures, latent variable models, adversarial training and a range of other methods. Despite their often high complexity, these approaches still fall short of generating high quality video continuations outside of narrow domains and often struggle with fidelity. In contrast, we show that conceptually simple autoregressive video generation models based on a three-dimensional self-attention mechanism achieve competitive results across multiple metrics on popular benchmark datasets, for which they produce continuations of high fidelity and realism. We also present results from training our models on Kinetics, a large scale action recognition dataset comprised of YouTube videos exhibiting phenomena such as camera movement, complex object interactions and diverse human movement. While modeling these phenomena consistently remains elusive, we hope that our results, which include occasional realistic continuations encourage further research on comparatively complex, large scale datasets such as Kinetics.Published as a conference paper at ICLR 2020We obtain strong results on popular benchmarks (Section 4.2, Appendix A) and produce high fidelity video continuations on the BAIR robot pushing dataset(Ebert et al., 2017)exhibiting plausible object interactions. Furthermore, our model achieves an almost 50% reduction in perplexity compared to prior work on autoregressive models on another robot pushing dataset.Finally, we apply our models to down-sampled videos from the Kinetics-600 dataset(Section 4.3). While modeling the full range of Kinetics-600 videos still poses a major challenge, we see encouraging video continuations for a more limited subset, namely cooking videos. These feature camera movement, complex object interactions and still cover diverse subjects.We hope that these initial results will encourage future video generation work to evaluate models on more challenging datasets such as Kinetics.",
    "prev": "These include generating the same token or series of token repeatedly, or generating gibberish outputs.",
    "curr": "This phenomenon of degeneration for longer continuations has also been observed in autoregressive models for video [36].",
    "next": "This problem of neural text degeneration has been linked to the training objective for LLMs, which trains a conditional distribution for the next token given a (partial) sentence [10].",
    "query": "Are there any studies that explore techniques to address degeneration in long video continuations using autoregressive models and alternative training objectives?"
  },
  {
    "index": 384,
    "source_corpus_id": 211003696,
    "ref_id": "b14",
    "citation_corpus_id": 8393918,
    "start": 3806,
    "end": 3822,
    "title": "Published as a conference paper at ICLR 2016 GATED GRAPH SEQUENCE NEURAL NETWORKS",
    "abstract": "Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks(Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be described as abstract data structures.Published as a conference paper at ICLR 2016 to learn features on the graph that encode the partial output sequence that has already been produced (e.g., the path so far if outputting a path) and that still needs to be produced (e.g., the remaining path). We will show how the GNN framework can be adapted to these settings, leading to a novel graph-based neural network model that we call Gated Graph Sequence Neural Networks (GGS-NNs).We illustrate aspects of this general model in experiments on bAbI tasks  and graph algorithm learning tasks that illustrate the capabilities of the model. We then present an application to the verification of computer programs. When attempting to prove properties such as memory safety (i.e., that there are no null pointer dereferences in a program), a core problem is to find mathematical descriptions of the data structures used in a program. FollowingBrockschmidt et al. (2015), we have phrased this as a machine learning problem where we will learn to map from a set of input graphs, representing the state of memory, to a logical description of the data structures that have been instantiated. Whereas Brockschmidt et al.(2015)relied on a large amount of hand-engineering of features, we show that the system can be replaced with a GGS-NN at no cost in accuracy.",
    "prev": "Also, logic rules can only cover a small part of the possible combinations of knowledge graph relations, hence limiting the application of models that are purely based on logic rules.",
    "curr": "Graph neural networks (GNNs) have recently gained increasing popularity for addressing many graph related problems effectively (Dai et al., 2016;Li et al., 2016;Kipf & Welling, 2017;Schlichtkrull et al., 2018).",
    "next": "GNN-based methods typically require sufficient labeled instances on specific end tasks to achieve good performance, however, knowledge graphs have the long-tail nature (Xiong et al., 2018), i.e., a large portion the relations in only are a few triples.",
    "query": "Are there any research papers on integrating graph neural networks with sequence modeling techniques to address challenges in knowledge graph reasoning, particularly for handling long-tail relations and limited labeled data?"
  },
  {
    "index": 386,
    "source_corpus_id": 252967887,
    "ref_id": "b8",
    "citation_corpus_id": 28202810,
    "start": 6326,
    "end": 6349,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "PRELIMINARIES\n\nSoft Actor-Critic.",
    "curr": "In this paper, our approach is built on Soft Actor-Critic (SAC) (Haarnoja et al., 2018).",
    "next": "SAC is a stable off-policy actor-critic algorithm based on the maximum entropy reinforcement learning framework, in which the actor maximizes both the returns and the entropy.",
    "query": "Are there any research papers on off-policy maximum entropy deep reinforcement learning algorithms with stochastic actors that address stability and sample efficiency challenges?"
  },
  {
    "index": 387,
    "source_corpus_id": 249538510,
    "ref_id": "b2",
    "citation_corpus_id": 202749904,
    "start": 5114,
    "end": 5137,
    "title": "HIGH FIDELITY SPEECH SYNTHESIS WITH ADVERSARIAL NETWORKS",
    "abstract": "Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention, and autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech. Our architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced. To measure the performance of GAN-TTS, we employ both subjective human evaluation (MOS -Mean Opinion Score), as well as novel quantitative metrics (Fréchet DeepSpeech Distance and Kernel DeepSpeech Distance), which we find to be well correlated with MOS. We show that GAN-TTS is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to GAN-TTS reading this abstract at https://storage.googleapis.com/ deepmind-media/research/abstract.wav.",
    "prev": "INTRODUCTION\n\nDeep generative models have demonstrated noticeable successes for modeling raw audio.",
    "curr": "The successful methods include, autoregressive models (van den Oord et al., 2016;Mehri et al., 2017;Kalchbrenner et al., 2018), flow-based models (van den Oord et al., 2018;Ping et al., 2019;Prenger et al., 2019;Kim et al., 2019;Ping et al., 2020;Lee et al., 2020), GAN-based models Kumar et al., 2019;Bińkowski et al., 2020;Yamamoto et al., 2020;Kong et al., 2020), and diffusion models (Kong et al., 2021;Chen et al., 2021;Lee et al., 2022).",
    "next": "Among these methods, GAN-based vocoders (e.g., Kong et al., 2020) can generate high-fidelity raw audio conditioned on mel spectrogram, while synthesizing hundreds of times faster than real-time on a single GPU.",
    "query": "Are there any research papers on GAN-based models for high-fidelity speech synthesis and text-to-speech generation using adversarial networks?"
  },
  {
    "index": 388,
    "source_corpus_id": 258331519,
    "ref_id": "b12",
    "citation_corpus_id": 28202810,
    "start": 12124,
    "end": 12147,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "We do not include HalfCheetah-v3 since the simulator does not terminate even when the robot has already fallen over, so there is no standard way to determine catastrophic failures.",
    "curr": "We first select three popular algorithms, including Soft Actor-Critic (SAC) (Haarnoja et al., 2018), Twin Delayed DDPG Table 1: Failure rates (%) of relay-evaluation using states generated by stranger agents trained with 4 algorithms, reported in the 4 rows for each environment.",
    "next": "The \"Reference\" column shows the failure rate of the stranger agents, serving as the baseline failure rate for these controllable states.",
    "query": "Are there any research papers on the evaluation of off-policy maximum entropy deep reinforcement learning algorithms, focusing on failure rates and stability in continuous control tasks?"
  },
  {
    "index": 391,
    "source_corpus_id": 220302636,
    "ref_id": "b6",
    "citation_corpus_id": 52055130,
    "start": 6736,
    "end": 6756,
    "title": "Learning deep representations by mutual information estimation and maximization",
    "abstract": "In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals.IntroductionOne core objective of deep learning is to discover \"good\" representations, and the simple idea explored here is to train a representation-learning function (i.e., an encoder) to maximize the mutual information (MI) between its inputs and outputs. MI is notoriously difficult to compute, particularly in continuous and high-dimensional settings. Fortunately, recent advances enable effective computation of MI between high dimensional input/output pairs of deep neural networks(Belghazi et al., 2018). We leverage MI estimation for representation learning, but we will also show that, depending on the downstream task, maximizing MI between the complete input and the encoder output (i.e., global MI) is often not sufficient for learning useful representations. Rather, structure matters: maximizing the average MI between the representation and local regions of the input can greatly improve the representation's quality for, e.g., classification tasks, while global MI plays a stronger role in the ability to reconstruct the full input given the representation.",
    "prev": "Related Work\n\nContrastive loss Our work aims to encourage data-efficient multimodal generative-model learning using a popular representation learning metric-contrastive loss (Hadsell et al., 2006;Weinberger and Saul, 2009).",
    "curr": "Although learning robust generalisable representations have been an important desideratum in generative-model learning (Bengio et al., 2013;Tenenbaum and Freeman, 2000), most applications of contrastive methods skip the generative component, and directly learn representations for specific tasks, as seen in contrastive predictive coding for time series data (van den Oord et al., 2018), image classification (Hénaff et al., 2019), noise contrastive estimation for vector embeddings of words (Mnih and Kavukcuoglu, 2013), as well as a range of frameworks such as DIM (Hjelm et al., 2019), MoCo (He et al., 2019), SimCLR (Chen et al., 2020) for more general visual-representation learning.",
    "next": "These approaches, while effective for the particular task at hand, fall short in learning representations that are generalisable to different tasks.",
    "query": "Are there any research papers on unsupervised representation learning that use mutual information maximization to improve generalizability across tasks by incorporating input structure and locality?"
  },
  {
    "index": 392,
    "source_corpus_id": 252716080,
    "ref_id": "b9",
    "citation_corpus_id": 56895453,
    "start": 1430,
    "end": 1449,
    "title": "LEARNING WHEN TO COMMUNICATE AT SCALE IN MULTIAGENT COOPERATIVE AND COMPETITIVE TASKS",
    "abstract": "Learning when to communicate and doing that effectively is essential in multi-agent tasks. Recent works show that continuous communication allows efficient training with back-propagation in multiagent scenarios, but have been restricted to fullycooperative tasks. In this paper, we present Individualized Controlled Continuous Communication Model (IC3Net) which has better training efficiency than simple continuous communication model, and can be applied to semi-cooperative and competitive settings along with the cooperative settings. IC3Net controls continuous communication with a gating mechanism and uses individualized rewards for each agent to gain better performance and scalability while fixing credit assignment issues. Using variety of tasks including StarCraft BroodWars TM explore and combat scenarios, we show that our network yields improved performance and convergence rates than the baselines as the scale increases. Our results convey that IC3Net agents learn when to communicate based on the scenario and profitability.Published as a conference paper at ICLR 2019Teaching agents how to communicate makes it is unnecessary to hand code the communication protocol with expert knowledge(Sukhbaatar et al., 2016)(Kottur et al., 2017. While the content of communication is important, it is also important to know when to communicate either to increase scalability and performance or to increase competitive edge. For example, a prey needs to learn when to communicate to avoid communicating its location with predators.Sukhbaatar et al. (2016)showed that agents communicating through a continuous vector are easier to train and have a higher information throughput than communication based on discrete symbols. Their continuous communication is differentiable, so it can be trained efficiently with back-propagation. However, their model assumes full-cooperation between agents and uses average global rewards. This restricts the model from being used in mixed or competitive scenarios as full-cooperation involves sharing hidden states to everyone; exposing everything and leading to poor performance by all agents as shown by our results. Furthermore, the average global reward for all agents makes the credit assignment problem even harder and difficult to scale as agents don't know their individual contributions in mixed or competitive scenarios where they want themselves to succeed before others.To solve above mentioned issues, we make the following contributions:",
    "prev": "Self-play (Samuel, 1967;Tesauro, 1995) has not only demonstrated the ability to abstract highdimensional state spaces as typified by AlphaGo (Silver et al., 2017), but also improved exploration coverage in partially observable environments.",
    "curr": "Communication (Sukhbaatar et al., 2016;Singh et al., 2019) exchanges their internal representations such as explored observation and hidden state in RNNs.",
    "next": "Evolutionary learning is expected to be a general framework for creating superhuman AIs as such learning can generate a high-level abstract representation without any bias in supervision.",
    "query": "Are there any research papers on methods for learning communication strategies in multi-agent systems that handle both cooperative and competitive tasks, addressing scalability and credit assignment challenges?"
  },
  {
    "index": 393,
    "source_corpus_id": 257279944,
    "ref_id": "b4",
    "citation_corpus_id": 52967399,
    "start": 2120,
    "end": 2140,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on pre-training of deep bidirectional transformers for language understanding?"
  },
  {
    "index": 394,
    "source_corpus_id": 237353080,
    "ref_id": "b6",
    "citation_corpus_id": 215814169,
    "start": 2957,
    "end": 2974,
    "title": "Training with Quantization Noise for Extreme Model Compression",
    "abstract": "We tackle the problem of producing compact models, maximizing their accuracy for a given model size. A standard solution is to train networks with Quantization Aware Training [1], where the weights are quantized during training and the gradients approximated with the Straight-Through Estimator[2]. In this paper, we extend this approach to work with extreme compression methods where the approximations introduced by STE are severe. Our proposal is to only quantize a different random subset of weights during each forward, allowing for unbiased gradients to flow through the other weights. Controlling the amount of noise and its form allows for extreme compression rates while maintaining the performance of the original model. As a result we establish new state-of-the-art compromises between accuracy and model size both in natural language processing and image classification. For example, applying our method to state-of-the-art Transformer and ConvNet architectures, we can achieve 82.5% accuracy on MNLI by compressing RoBERTa to 14 MB and 80.0% top-1 accuracy on ImageNet by compressing an EfficientNet-B3 to 3.3 MB. * Equal",
    "prev": "Another solution is to compress a model with small accuracy degradation so that it takes less storage and reduces System-on-Chip (SoC) memory bandwidth utilization, which can minimize power-consumption and latency.",
    "curr": "To this end, various DNN compression techniques have been proposed (Wang et al., 2019b;Dong et al., 2020;Park et al., 2018;Rastegari et al., 2016;Fan et al., 2021;Stock et al., 2020;Zhou et al., 2019;Park et al., 2019;Yu et al., 2018;Polino et al., 2018).",
    "next": "Among them, weight-clustering/sharing (Han et al., 2016;Wu et al., 2018;Ullrich et al., 2017;Stock et al., 2020) has shown a high DNN compression ratio where weights are clustered into a few shareable weight values (or centroids) based on k-means clustering.",
    "query": "Are there any research papers on extreme model compression techniques using quantization aware training and weight clustering to maintain high accuracy in natural language processing and image classification tasks?"
  },
  {
    "index": 395,
    "source_corpus_id": 204008396,
    "ref_id": "b40",
    "citation_corpus_id": 1957433,
    "start": 2782,
    "end": 2806,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "1 Published as a conference paper at ICLR 2020The joint vocabulary is composed of three disjoint sets:is the shared vocabulary set and V i J is the set of tokens that appear in the ith language only.",
    "curr": "Note that a key difference of existing supervised joint training methods is that embeddings corresponding to V s J are not shared between E L1 and E L2 , meaning that they are disjoint, as in alignment methods.DISCUSSIONWhile alignment methods have had great success, there are still some critical downsides, among which we stress the following points:\n\nINTRODUCTION\n\nContinuous word representations (Mikolov et al., 2013a;Pennington et al., 2014;Bojanowski et al., 2017) have become ubiquitous across a wide range of NLP tasks.",
    "next": "In particular, methods for crosslingual word embeddings (CLWE) have proven a powerful tool for cross-lingual transfer for downstream tasks, such as text classification (Klementiev et al., 2012), dependency parsing (Ahmad et al., 2019), named entity recognition (NER) (Xie et al., 2018;Chen et al., 2019), natural language inference , language modeling (Adams et al., 2017), and machine translation (MT) (Zou et al., 2013;Artetxe et al., 2018b;.",
    "query": "Are there any studies on cross-lingual word embeddings methods that compare supervised joint training and alignment approaches for handling shared and language-specific vocabularies?"
  },
  {
    "index": 396,
    "source_corpus_id": 219708931,
    "ref_id": "b0",
    "citation_corpus_id": 53388625,
    "start": 1969,
    "end": 1994,
    "title": "THE LOTTERY TICKET HYPOTHESIS: FINDING SPARSE, TRAINABLE NEURAL NETWORKS",
    "abstract": "Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard technique for pruning weights naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the lottery ticket hypothesis: dense, randomly-initialized feed-forward networks contain subnetworks (winning tickets) that-when trained in isolation-arrive at comparable test accuracy in a comparable number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Furthermore, the winning tickets we find above that size learn faster than the original network and exhibit higher test accuracy. arXiv:1803.03635v4 [cs.LG] 27 Nov 2018 1. Randomly initialize a neural network f (x; θ 0 ) (where θ 0 ∼ D θ ). 2. Train the network for j iterations, reaching parameters θ j . 3. Prune s% of the parameters, creating a mask m where P m = (100 − s)%. 4. To extract the winning ticket, reset the remaining parameters to their values in θ 0 , creating the untrained network f (x; m θ 0 ).If dense networks contain winning tickets and pruning reveals them, then the network f (x; m θ 0 )when trained for j iterations-will reach similar accuracy to f (x; θ j ) at least as quickly, and m will be too sparse for a randomly-reinitialized or randomly-sparsified network to do the same.Results. We identify winning tickets in a fully-connected architecture for MNIST and convolutional architectures for CIFAR10 across several optimization strategies (SGD, momentum, and Adam) with techniques like dropout, weight decay, and batchnorm. In deeper networks, our pruning-based strategy for finding winning tickets is sensitive to the learning rate: it requires warmup to find winning tickets at higher learning rates. The winning tickets we find are 10-20% (or less) of the size of the Anonymous. Gradient descent provably optimizes over-parameterized neural networks. Dally. Dsd: Regularizing deep neural networks with dense-sparse-dense training flow. Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016.",
    "prev": "Introduction\n\nThe majority of pruning algorithms for Deep Neural Networks require training the dense models and often fine-tuning sparse sub-networks in order to obtain their pruned counterparts.",
    "curr": "In Frankle and Carbin [2019], the authors provide empirical evidence that there exist sparse sub-networks that can be trained from scratch to achieve similar performance as their dense counterparts.",
    "next": "However, their method to find such sub-networks requires training the full-sized model and intermediate sub-networks, making the process much more expensive.",
    "query": "Are there any research papers on methods for identifying sparse, trainable subnetworks in deep neural networks through pruning, particularly those related to the lottery ticket hypothesis and training from scratch?"
  },
  {
    "index": 397,
    "source_corpus_id": 249191769,
    "ref_id": "b7",
    "citation_corpus_id": 203641746,
    "start": 3128,
    "end": 3130,
    "title": "Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks",
    "abstract": "Recent theoretical work has established connections between over-parametrized neural networks and linearized models governed by the Neural Tangent Kernels (NTKs). NTK theory leads to concrete convergence and generalization results, yet the empirical performance of neural networks are observed to exceed their linearized models, suggesting insufficiency of this theory.Towards closing this gap, we investigate the training of over-parametrized neural networks that are beyond the NTK regime yet still governed by the Taylor expansion of the network. We bring forward the idea of randomizing the neural networks, which allows them to escape their NTK and couple with quadratic models. We show that the optimization landscape of randomized two-layer networks are nice and amenable to escaping-saddle algorithms. We prove concrete generalization and expressivity results on these randomized networks, which leads to sample complexity bounds (of learning certain simple functions) that match the NTK and can in addition be better by a dimension factor when mild distributional assumptions are present. We demonstrate that our randomization technique can be generalized systematically beyond the quadratic case, by using it to find networks that are coupled with higher-order terms in their Taylor series. * Salesforce Research. yu.bai@salesforce.com † Princeton University. jasonlee@princeton.edu design. arXiv preprint arXiv:1711.00501, 2017. . Characterizing implicit bias in terms of optimization geometry. arXiv preprint arXiv:1802.08246, 2018a.Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on linear convolutional networks. In Advances in Neural Information Processing Systems, pages 9461-9471, 2018b.Benjamin D Haeffele and René Vidal. Global optimality in tensor factorization, deep learning, and beyond. arXiv preprint arXiv:1506.07540, 2015.Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231, 2016. . Kernel and deep regimes in overparametrized models. arXiv preprint arXiv:1906.05827, 2019.Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding neural networks. arXiv preprint arXiv:1904.00687, 2019. gradient descent optimizes over-parameterized deep ReLU networks. arXiv preprint arXiv:1811.08888, 2018.",
    "prev": "To alleviate the non-convexity of neural network optimization, recent studies focus on over-parameterization as one of the promising approaches.",
    "curr": "Indeed, it is fully exploited by (i) Neural Tangent Kernel (NTK) [27,5,6,18,53, 60] and (ii) mean field analysis [36,15,31,50,14,46].",
    "next": "In the setting of NTK, a relatively large-scale initialization is considered.",
    "query": "Are there any research papers on methods for analyzing over-parameterized neural networks beyond the Neural Tangent Kernel (NTK) regime, using quadratic or higher-order approximations and randomization techniques to improve generalization?"
  },
  {
    "index": 398,
    "source_corpus_id": 256598122,
    "ref_id": "b0",
    "citation_corpus_id": 15978939,
    "start": 6162,
    "end": 6184,
    "title": "Domain Adaptation with Structural Correspondence Learning",
    "abstract": "Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are drawn from the same distribution. For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent. In such cases, we seek to adapt existing models from a resourcerich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger.",
    "prev": "Domain Adaptation Typically domain adaptations consider data distributions and deviations to search for mappings aligning domains.",
    "curr": "Early literature suggested that assuming data drawn from certain probabilities (Blitzer et al., 2006) can be used to model and compensate for the domain mismatch.",
    "next": "Some studies then looked for theoretical arguments when a successful adaptation can be yielded.",
    "query": "Are there any research papers on domain adaptation methods in natural language processing that utilize structural correspondence learning to address domain mismatch?"
  },
  {
    "index": 399,
    "source_corpus_id": 203592088,
    "ref_id": "b17",
    "citation_corpus_id": 3366315,
    "start": 4120,
    "end": 4140,
    "title": "Published as a conference paper at ICLR 2018 SPECTRAL NORMALIZATION FOR GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. The code with Chainer (Tokui et al., 2015), generated images and pretrained models are available at https://github.com/pfnet-research/sngan_ projection.Published as a conference paper at ICLR 2018• Lipschitz constant is the only hyper-parameter to be tuned, and the algorithm does not require intensive tuning of the only hyper-parameter for satisfactory performance. • Implementation is simple and the additional computational cost is small.",
    "prev": "As such, the distribution of training images can be estimated precisely without supervision.",
    "curr": "Recent interests in generative models pertain to their training stability Gulrajani et al., 2017;Miyato et al., 2018) and improvement in quality and diversity (Karras et al., 2018;Brock et al., 2019;Karras et al., 2019).",
    "next": "Furthermore, methods to learn 3D representation from 2D images by constructing a generative model conditioned on camera parameters have been proposed (Shen et al., 2018;Sitzmann et al., 2019;Nguyen-Phuoc et al., 2019).",
    "query": "Are there any research papers on techniques for stabilizing generative adversarial networks during training, such as spectral normalization, and their impact on image quality and diversity?"
  },
  {
    "index": 401,
    "source_corpus_id": 243847413,
    "ref_id": "b6",
    "citation_corpus_id": 6628106,
    "start": 15985,
    "end": 16004,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "We then approximate the integral over the δ-ball via Monte Carlo integration by sampling k points x 1 , ..., x k uniformly in the ball to obtain the objective:\n1 k k i=1 log p(g|x i ) + log p(x i ).",
    "curr": "(3)\nFinally, as the objective is differentiable, we can use gradient-based optimizer such as Adam (Kingma & Ba, 2015), and obtain the attack in Algorithm 1.",
    "next": "Fig.",
    "query": "Are there any research papers on Adam as a method for stochastic optimization in machine learning?"
  },
  {
    "index": 403,
    "source_corpus_id": 56657849,
    "ref_id": "b17",
    "citation_corpus_id": 6628106,
    "start": 37305,
    "end": 37326,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "For external competitor methods, we used the hyperparameters from the respective publications where applicable and otherwise the default parameters from their packages.",
    "curr": "The models were implemented in TensorFlow (Abadi et al., 2016) and optimized using Adam (Kingma and Ba, 2015).",
    "next": "C CLUSTERING PERFORMANCE MEASURES\n\nGiven that one of our most interesting tasks at hand is the clustering of data, we need some performance measures to objectively compare the quality of this clustering with other methods.",
    "query": "Are there any research papers on the application of the Adam optimization algorithm in clustering tasks and the evaluation of clustering performance measures?"
  },
  {
    "index": 404,
    "source_corpus_id": 49876500,
    "ref_id": "b13",
    "citation_corpus_id": 8968704,
    "start": 3164,
    "end": 3167,
    "title": "A Neural Representation of Sketch Drawings",
    "abstract": "We present sketch-rnn, a recurrent neural network (RNN) able to construct stroke-based drawings of common objects. The model is trained on a dataset of human-drawn images representing many different classes. We outline a framework for conditional and unconditional sketch generation, and describe new robust training methods for generating coherent sketch drawings in a vector format.",
    "prev": "From another point of view, it also indicates that the autoencoder has uncovered some structure about the data and has captured it in its latent space.",
    "curr": "These characteristics have led interpolations to be a commonly reported experimental result in studies about autoencoders [5,11,14,26,27,32] and latent-variable generative models in general [10,30,38].",
    "next": "The connection between interpolation and a \"flat\" data manifold has also been explored in the context of unsupervised representation learning [3] and regularization [40].",
    "query": "Are there any studies that explore interpolation in autoencoders and latent-variable generative models, and their relationship to data manifolds or regularization techniques?"
  },
  {
    "index": 408,
    "source_corpus_id": 263909446,
    "ref_id": "b40",
    "citation_corpus_id": 227209335,
    "start": 1900,
    "end": 1918,
    "title": "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024ˆ1024 images for the first time from a score-based generative model.",
    "prev": "s generalize DDIM, mapping the reverse diffusion dynamics to a more amenable space for sampling.In contrast, splitting-based integrators, commonly used in molecular dynamics, reduce the numerical simulation error by cleverly alternating between numerical updates involving the data and auxiliary variables.After extensively studying these methods empirically and theoretically, we present a hybrid method that leads to the best-reported performance for diffusion models in augmented spaces.Applied to Phase Space Langevin Diffusion[Pandey & Mandt, 2023]on CIFAR-10, our deterministic and stochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network function evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing baselines, respectively.Our code and model checkpoints will be made publicly available at https://github.com/mandt-lab/PSLD.",
    "curr": "INTRODUCTION\n\nScore-based Generative models (or Diffusion models) (Sohl-Dickstein et al., 2015;Song & Ermon, 2019;Ho et al., 2020;Song et al., 2020) have demonstrated impressive performance on various tasks, such as image and video synthesis (Dhariwal & Nichol, 2021;Ho et al., 2022a;Rombach et al., 2022;Ramesh et al., 2022;Saharia et al., 2022a;Yang et al., 2022;Ho et al., 2022b;Harvey et al., 2022), image super-resolution (Saharia et al., 2022b), and audio and speech synthesis (Chen et al., 2021;Lam et al., 2021).",
    "next": "However, high-quality sample generation in standard diffusion models requires hundreds to thousands of expensive score function evaluations.While there have been recent advances in improving the sampling efficiency (Song et al., 2021;Lu et al., 2022;Zhang & Chen, 2023), most of these efforts have been focused towards a specific family of models that perform diffusion in the data space (Song et al., 2020;Karras et al., 2022).Interestingly, recent work (Dockhorn et al., 2022b;Pandey & Mandt, 2023;Singhal et al., 2023) indicates that performing diffusion in a joint space, where the data space is augmented with auxiliary v",
    "query": "Are there any research papers on score-based generative models using stochastic differential equations and methods for improving sampling efficiency in diffusion models?"
  },
  {
    "index": 410,
    "source_corpus_id": 264825556,
    "ref_id": "b34",
    "citation_corpus_id": 246823323,
    "start": 4366,
    "end": 4387,
    "title": "Published as a conference paper at ICLR 2022 LEARNING FAST SAMPLERS FOR DIFFUSION MODELS BY DIFFERENTIATING THROUGH SAMPLE QUALITY",
    "abstract": "Diffusion models have emerged as an expressive family of generative models rivaling GANs in sample quality and autoregressive models in likelihood scores. Standard diffusion models typically require hundreds of forward passes through the model to generate a single high-fidelity sample. We introduce Differentiable Diffusion Sampler Search (DDSS): a method that optimizes fast samplers for any pre-trained diffusion model by differentiating through sample quality scores. We present Generalized Gaussian Diffusion Models (GGDM), a family of flexible non-Markovian samplers for diffusion models. We show that optimizing the degrees of freedom of GGDM samplers by maximizing sample quality scores via gradient descent leads to improved sample quality. Our optimization procedure backpropagates through the sampling process using the reparametrization trick and gradient rematerialization. DDSS achieves strong results on unconditional image generation across various datasets (e.g., FID scores on LSUN church 128x128 of 11.6 with only 10 inference steps, and 4.82 with 20 steps, compared to 51.1 and 14.9 with strongest DDPM/DDIM baselines). Our method is compatible with any pre-trained diffusion model without fine-tuning or re-training required. * Work done as part of the Google AI Residency.",
    "prev": "n training, it is shown to empirically generate samples of comparable quality to the original model (Salimans & Ho, 2022;Meng et al., 2023).Unfortunately, the GPU time required to distill a model is comparable to the training time of the original model Salimans & Ho (2022), which is often considerable.",
    "curr": "(ii) Dedicated solvers: where the specific structure of the ODE is used to design a more efficient solver (Song et al., 2020a;Lu et al., 2022a;b) and/or employ a suitable solver family from the literature of numerical analysis (Zhang & Chen, 2022;Zhang et al., 2023).The main benefit of this approach is two-fold: First, it is consistent, i.e., as the number of steps (NFE) increases, the samples converge to those of the pre-trained model.Second, it does not require further training/fine-tuning of the pre-trained model, consequently avoiding long additional training times and access to training data.Related to our approach, some works have tried to learn an ODE solver within a certain class (Watson et al., 2021;Duan et al., 2023); however, they do not guarantee consistency and usually introduce moderate improvements over generic dedicated solvers.In this paper, we introduce Bespoke solvers, a framework for learning consistent ODE solvers custom-tailored to pre-trained flow models.The main motivation for Bespoke solvers is that different models exhibit sampling paths with different characteristics, leading to local truncation errors that are specific to each instance of a trained model.A key observation of this paper is that optimizing a solver for a particular model can significantly improve quality of samples for low NFE compared to existing dedicated solvers.Furthermore, Bespoke solvers use a very small number of learnable parameters and consequently are efficient to train.For example, we have trained n ∈ {5, 8, 10} steps Bespoke solvers for a pretrained ImageNet-64×64 flow model with {40, 64, 80} learnable parameters (resp.",
    "next": ")producing images with Fréchet Inception Distances (FID) of 2.2, 1.",
    "query": "Are there any research papers on methods for learning fast samplers for diffusion models by differentiating through sample quality scores?"
  },
  {
    "index": 412,
    "source_corpus_id": 256827824,
    "ref_id": "b53",
    "citation_corpus_id": 5834589,
    "start": 25088,
    "end": 25092,
    "title": "ON LARGE-BATCH TRAINING FOR DEEP LEARNING: GENERALIZATION GAP AND SHARP MINIMA",
    "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say 32-512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions-and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap. Published as a conference paper at ICLR 2017 These methods minimize the objective function f by iteratively taking steps of the form: J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. . Weak sharp minima and penalty functions in mathematical programming. PhD thesis, University of Cambridge, 1988.Michael P Friedlander and Mark Schmidt. Hybrid deterministic-stochastic methods for data fitting.",
    "prev": "Figure 4 shows that for some rules, i.e.Trimmed Mean, the presence of even a single Byzantine worker has a catastrophic impact.For other rules, as the number of Byzantine workers increases, filtering out the outliers becomes more challenging because the amount of noise increases.Regardless, FA remains more robust compared to other approaches.",
    "curr": "Marginal utility of larger batch sizes under a fixed noise level:\n\nWe empirically verified the batch size required to identify our optimal Y * -the FA matrix at each iteration.In particular, we fixed the noise level to f = 3 Byzantine workers and varied batch sizes.We show the results in Figure 5.Our results indicate that, in cases where a larger batch size is a training requirement, FA achieves a significantly better accuracy compared to the existing state of the art aggregators.This may be useful in some large scale vision applications, see [54,55] for more details.Empirically, we can already see that our spectral relaxation to identify gradient subspace is effective in practice in all our experiments.",
    "next": "Tolerance to communication loss:\n\nTo analyze the effect of unreliable communication channels between the workers and the parameter server on convergence, we design an experiment where the physical link between some of the workers and the parameter server randomly drops a percentage of packets.Here, we set the loss rate of three links to 10% i.e., there are 3 Byzantine workers in our setting.The loss is introduced using the netem queuing discipline in Linux designed to emulate the properties of wide area networks [56].The two main takeaways in Figure 6a are:\n\n1.",
    "query": "Are there any research papers on the generalization gap in large-batch training for deep learning and strategies to mitigate it?"
  },
  {
    "index": 413,
    "source_corpus_id": 259375820,
    "ref_id": "b11",
    "citation_corpus_id": 241035330,
    "start": 2653,
    "end": 2671,
    "title": "An Explanation of In-context Learning as Implicit Bayesian Inference",
    "abstract": "Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning 1 . Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.",
    "prev": "Introduction\n\nLarge language models (LLMs) demonstrate the surprising ability of in-context learning, where an LLM \"learns\" to solve a task by conditioning on a prompt containing input-output exemplars [Brown et al., 2020, Lieber et al., 2021, Radford et al., 2019, Wang and Komatsuzaki, 2021.",
    "curr": "Recent works have advanced the understanding of in-context learning via empirical analysis [Min et al., 2022, Akyürek et al., 2023, von Oswald et al., 2022, Dai et al., 2023, but theoretical analysis remains limited [Xie et al., 2022].",
    "next": "A recent line of work [Garg et al., 2022, Akyürek et al., 2023, von Oswald et al., 2022, Dai et al., 2023 empirically finds that transformers can be trained to implement algorithms that solve linear regression problems in-context.",
    "query": "Are there any research papers on theoretical explanations of in-context learning in large language models, particularly those framing it as implicit Bayesian inference?"
  },
  {
    "index": 414,
    "source_corpus_id": 246485738,
    "ref_id": "b12",
    "citation_corpus_id": 235293778,
    "start": 30919,
    "end": 30944,
    "title": "Published as a conference paper at ICLR 2021 UNSUPERVISED REPRESENTATION LEARNING FOR TIME SERIES WITH TEMPORAL NEIGHBORHOOD CODING",
    "abstract": "Time series are often complex and rich in information but sparsely labeled and therefore challenging to model. In this paper, we propose a self-supervised framework for learning generalizable representations for non-stationary time series. Our approach, called Temporal Neighborhood Coding (TNC), takes advantage of the local smoothness of a signal's generative process to define neighborhoods in time with stationary properties. Using a debiased contrastive objective, our framework learns time series representations by ensuring that in the encoding space, the distribution of signals from within a neighborhood is distinguishable from the distribution of non-neighboring signals. Our motivation stems from the medical field, where the ability to model the dynamic nature of time series data is especially valuable for identifying, tracking, and predicting the underlying patients' latent states in settings where labeling data is practically impossible. We compare our method to recently developed unsupervised representation learning approaches and demonstrate superior performance on clustering and classification tasks for multiple datasets. *",
    "prev": "We ran the code from their open source repository 4 as is, hyper-parameters used are all defaults as suggested in their paper.",
    "curr": "TNC (Tonekaboni et al., 2021) TNC proposes a self-supervised framework for learning generalizable representations for non-stationary time series.",
    "next": "They make use of the augmented Dickey-Fuller test for stationarity to ensure positive samples come from the a neighborhood of similar signals.",
    "query": "Are there any research papers on self-supervised representation learning for non-stationary time series using contrastive objectives and temporal neighborhood coding?"
  },
  {
    "index": 417,
    "source_corpus_id": 263909387,
    "ref_id": "b19",
    "citation_corpus_id": 215768182,
    "start": 23887,
    "end": 23909,
    "title": null,
    "abstract": null,
    "prev": "Following Leviathan et al.",
    "curr": "(2023), we evaluate two model types: 1) GPTlike decoder-only Transformer models trained using the standard autoregressive objective on LM1B task (Chelba et al., 2013), where the target and draft models have 234M and 33M parameters, respectively; and 2) Standard encoder-decoder T5 v1.1 models (Raffel et al., 2020) supervised fine-tuned on four different tasks, with T5-XL (3B) and T5-Small (77M) serving as the target and draft models, respectively.As for the four datasets, we utilize two datasets form the T5 paper, namely WMT EnDe (Bojar et al., 2014) and CNN/DM (Hermann et al., 2015) which deal with translation and text summarization, respectively.The remaining two tasks used to test T5 models are XSum (Narayan et al., 2018) and GSM8K (Cobbe et al., 2021), which deal with abstractive summarization and arithmetic reasoning, respectively.See Appendix C for more details.",
    "next": "Training data for KD.We study five KD algorithms outlined in Table 1.However, for Se-qKD (Kim & Rush, 2016) and f -Distill (Wen et al., 2023), we opt for an online data generation approach from the teacher instead of a conventional fixed offline teacher-generated dataset.This approach, while computationally expensive, yields a more diverse dataset.For GKD, we solely rely on the data generated by the online student model, excluding the static ground truth data.All data generated by either the teacher or the student is based on temperature sampling with a temperature of 1.0 (see Appendix D.1.3for an ablation study on sampling temperature).",
    "query": "Are there any research papers on knowledge distillation methods comparing decoder-only and encoder-decoder models for tasks like translation, summarization, and arithmetic reasoning, using online data generation and temperature sampling approaches?"
  },
  {
    "index": 418,
    "source_corpus_id": 53034786,
    "ref_id": "b42",
    "citation_corpus_id": 990233,
    "start": 2951,
    "end": 2972,
    "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    "abstract": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.",
    "prev": "RNNs explicitly imposes a chain structure on the data.",
    "curr": "This chain structure may seem at odds with the latent non-sequential structure of language and poses several difficulties for the processing of natural language data with deep learning methods, such as capturing long-term dependencies (Bengio et al., 2009), achieving good generalization (Bowman et al., 2015), handling negation (Socher et al., 2013), etc.",
    "next": "Meanwhile, some evidence exists that an RNN with sufficient capacity has the potential to encode such a tree structure implicitly .",
    "query": "Are there any studies that explore recursive or tree-based models for semantic compositionality in sentiment analysis, particularly using the Sentiment Treebank and addressing challenges like negation handling?"
  },
  {
    "index": 419,
    "source_corpus_id": 256827026,
    "ref_id": "b7",
    "citation_corpus_id": 52967399,
    "start": 2468,
    "end": 2471,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Notably, on the MSRVTT retrieval task, UniAdapter achieves 49.7% recall@1 with only 2.2% tunable model parameters, outperforming the latest competitors by 2.0%.",
    "curr": "The code and models are available at https\n\nIntroduction\n\nThe pretrain-finetune paradigm has achieved great success in natural language processing (NLP) [8,4], computer vision (CV) [41], and multimodal modeling [37,22], where models are first pre-trained with large-scale data, and then fully fine-tuned for each downstream task.",
    "next": "Recent research further finds that fine-tuning/adapting a foundation model to a new modality by introducing additional trainable modules significantly outperforms previous works, such as temporal modeling modules [13,23,33] for image-to-video transferring (see Figure 1 (a)).",
    "query": "Are there any research papers on efficient adaptation of pre-trained models for multimodal tasks with minimal tunable parameters, such as video retrieval?"
  },
  {
    "index": 421,
    "source_corpus_id": 252715598,
    "ref_id": "b3",
    "citation_corpus_id": 11212020,
    "start": 2344,
    "end": 2367,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "Code is publicly available.",
    "curr": "INTRODUCTION\n\nThe vision community has witnessed the prevalence of self-attention (Bahdanau et al., 2015) and Transformers (Vaswani et al., 2017).",
    "next": "The success of Transformers in natural language processing motivates the creation of their variants for vision recognition.",
    "query": "Are there any research papers on applying Transformer-based models to vision recognition tasks, building upon their success in natural language processing?"
  },
  {
    "index": 423,
    "source_corpus_id": 237372712,
    "ref_id": "b18",
    "citation_corpus_id": 5034059,
    "start": 1429,
    "end": 1448,
    "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    "abstract": "For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusively tailored to a specific task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating and analyzing the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all our tasks yields better results than training a separate model for each task. However, the low absolute performance of our best model indicates the need for improved general NLU systems. son. 2013. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint 1312.3005.",
    "prev": "INTRODUCTION\n\nShared benchmarks and datasets have historically played a crucial role in driving advances in largescale applications of deep learning, e.g.",
    "curr": "in computer vision (Deng et al., 2009) and natural language processing (Wang et al., 2019;Rajpurkar et al., 2016;Paperno et al., 2016).",
    "next": "Neural theorem proving is a rapidly developing area which aims to apply techniques from deep learning to interactive theorem proving.",
    "query": "Are there any research papers on multi-task benchmarks or platforms for evaluating natural language understanding across diverse tasks?"
  },
  {
    "index": 424,
    "source_corpus_id": 246294898,
    "ref_id": "b35",
    "citation_corpus_id": 220249726,
    "start": 3316,
    "end": 3334,
    "title": "BERTology Meets Biology: Interpreting Attention in Protein Language Models",
    "abstract": "Transformer architectures have proven to learn useful representations for protein classification and generation tasks. However, these representations present challenges in interpretability. Through the lens of attention, we analyze the inner workings of the Transformer and explore how the model discerns structural and functional properties of proteins. We show that attention (1) captures the folding structure of proteins, connecting amino acids that are far apart in the underlying sequence, but spatially close in the three-dimensional structure, (2) targets binding sites, a key functional component of proteins, and (3) focuses on progressively more complex biophysical properties with increasing layer depth. We also present a three-dimensional visualization of the interaction between attention and protein structure. Our findings align with known biological processes and provide a tool to aid discovery in protein engineering and synthetic biology. The code for visualization and analysis is available at https://github.com/salesforce/provis.Preprint. Under review.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any studies on interpreting attention mechanisms in protein language models to understand structural and functional properties of proteins?"
  },
  {
    "index": 425,
    "source_corpus_id": 259187750,
    "ref_id": "b11",
    "citation_corpus_id": 232185279,
    "start": 12657,
    "end": 12661,
    "title": "Published as a conference paper at ICLR 2021 FAIR MIXUP: FAIRNESS VIA INTERPOLATION",
    "abstract": "Training classifiers under fairness constraints such as group fairness, regularizes the disparities of predictions between the groups. Nevertheless, even though the constraints are satisfied during training, they might not generalize at evaluation time. To improve the generalizability of fair classifiers, we propose fair mixup, a new data augmentation strategy for imposing the fairness constraint. In particular, we show that fairness can be achieved by regularizing the models on paths of interpolated samples between the groups. We use mixup, a powerful data augmentation strategy to generate these interpolates. We analyze fair mixup and empirically show that it ensures a better generalization for both accuracy and fairness measurement in tabular, vision, and language benchmarks. The code is available at https://github.com/chingyaoc/fair-mixup. * Work done during an internship at IBM Research AI",
    "prev": "In particular, we consider the following three types of in-processing methods.",
    "curr": "Gap Regularization [12] simplifies optimization by offering a smooth approximation to real loss functions, which are often non-convex or difficult to optimize directly.",
    "next": "This approach includes DiffDP, DiffEodd, and DiffEopp.",
    "query": "Are there any research papers on data augmentation techniques for enforcing fairness in machine learning classifiers through interpolation methods like fair mixup?"
  },
  {
    "index": 429,
    "source_corpus_id": 238419003,
    "ref_id": "b2",
    "citation_corpus_id": 52156206,
    "start": 8524,
    "end": 8556,
    "title": "Gromov-Wasserstein Alignment of Word Embedding Spaces",
    "abstract": "Cross-lingual or cross-domain correspondences play key roles in tasks ranging from machine translation to transfer learning. Recently, purely unsupervised methods operating on monolingual embeddings have become effective alignment tools. Current state-of-theart methods, however, involve multiple steps, including heuristic post-hoc refinement strategies. In this paper, we cast the correspondence problem directly as an optimal transport (OT) problem, building on the idea that word embeddings arise from metric recovery algorithms. Indeed, we exploit the Gromov-Wasserstein distance that measures how similarities between pairs of words relate across languages. We show that our OT objective can be estimated efficiently, requires little or no tuning, and results in performance comparable with the state-of-the-art in various unsupervised word translation tasks.",
    "prev": "Finally, other domain adaptation and transfer learning settings use Gromov-Wasserstein variants, e.g.",
    "curr": "for transfer between word embedding spaces (Alvarez-Melis & Jaakkola, 2018) and image spaces (Vayer et al., 2020b).",
    "next": "PRELIMINARIES\n\nMetric Markov Decision Process.",
    "query": "Are there any research papers on using Gromov-Wasserstein methods for aligning word embedding spaces in cross-lingual or cross-domain settings?"
  },
  {
    "index": 432,
    "source_corpus_id": 254877694,
    "ref_id": "b8",
    "citation_corpus_id": 220055784,
    "start": 4552,
    "end": 4574,
    "title": "SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness",
    "abstract": "In this paper, we cast fair machine learning as invariant machine learning. We first formulate a version of individual fairness that enforces invariance on certain sensitive sets. We then design a transport-based regularizer that enforces this version of individual fairness and develop an algorithm to minimize the regularizer efficiently. Our theoretical results guarantee the proposed approach trains certifiably fair ML models. Finally, in the experimental studies we demonstrate improved fairness metrics in comparison to several recent fair training procedures on three ML tasks that are susceptible to algorithmic bias.",
    "prev": "Then, we use active learning and crowdworker judgments to identify pairs that deserve similar treatment according to human intuition.",
    "curr": "In the context of text classification, most existing works have cast similarity in terms of word replacement (Dixon et al., 2018;Garg et al., 2019;Yurochkin & Sun, 2021;Liang et al., 2020).",
    "next": "Given a sentence s, a similar sentence s is generated by replacing each word in s, that belongs to a list of words A j indicative of a demographic group j, by a word from list A j , indicative of another demographic group j = j.",
    "query": "Are there any research papers on methods for enforcing individual fairness in machine learning through invariance on sensitive sets using transport-based regularizers?"
  },
  {
    "index": 435,
    "source_corpus_id": 253098972,
    "ref_id": "b5",
    "citation_corpus_id": 165163607,
    "start": 2447,
    "end": 2473,
    "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions",
    "abstract": "In this paper we study yes/no questions that are naturally occurring -meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, BoolQ, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid information, and require difficult entailment-like inference to solve. We also explore the effectiveness of a range of transfer learning baselines. We find that transferring from entailment data is more effective than transferring from paraphrase or extractive QA data, and that it, surprisingly, continues to be very beneficial even when starting from massive pre-trained language models such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on our train set. It achieves 80.4% accuracy compared to 90% accuracy of human annotators (and 62% majority-baseline), leaving a significant gap for future work.",
    "prev": "We conduct experiments across a diverse set of eight NLP tasks using models of different scales (T5-{base, large, XL}) and find that SESoM consistently outperforms the existing models of the same as well as larger parametric scale by a large margin.",
    "curr": "INTRODUCTION\n\nRecent few years have witnessed the great success of large pre-trained language models (PLM) (Kenton & Toutanova, 2019;Liu et al., 2019;Radford et al., 2019;Raffel et al., 2020;Brown et al., 2020).",
    "next": "The size of pre-trained models which can easily go to billions of parameters (Brown et al., 2020;Raffel et al., 2020), however, hinder their real-world deployments and applications.",
    "query": "Are there any research papers on datasets for natural yes/no questions and their effectiveness in transfer learning using pre-trained language models?"
  },
  {
    "index": 436,
    "source_corpus_id": 253237991,
    "ref_id": "b5",
    "citation_corpus_id": 239998683,
    "start": 3426,
    "end": 3445,
    "title": "Learning-Augmented k-means Clustering",
    "abstract": "k-means clustering is a well-studied problem due to its wide applicability. Unfortunately, there exist strong theoretical limits on the performance of any algorithm for the k-means problem on worst-case inputs. To overcome this barrier, we consider a scenario where \"advice\" is provided to help perform clustering. Specifically, we consider the k-means problem augmented with a predictor that, given any point, returns its cluster label in an approximately optimal clustering up to some, possibly adversarial, error. We present an algorithm whose performance improves along with the accuracy of the predictor, even though naïvely following the accurate predictor can still lead to a high clustering cost. Thus if the predictor is sufficiently accurate, we can retrieve a close to optimal clustering with nearly optimal runtime, breaking known computational barriers for algorithms that do not have access to such advice. We evaluate our algorithms on real datasets and show significant improvements in the quality of clustering. *",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on learning-augmented approaches for improving k-means clustering using predictor-based advice?"
  },
  {
    "index": 437,
    "source_corpus_id": 254535963,
    "ref_id": "b8",
    "citation_corpus_id": 21529792,
    "start": 7416,
    "end": 7433,
    "title": "Published as a conference paper at ICLR 2018 LEARNING ROBUST REWARDS WITH ADVERSARIAL INVERSE REINFORCEMENT LEARNING",
    "abstract": "Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose AIRL, a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that AIRL greatly outperforms prior methods in these transfer settings.",
    "prev": "te that the same finding holds even with real humans.Overall, our results suggest an optimistic perspective on the framework of reward learning, and that efforts in improving human models will further enhance the quality of the inferred rewards.",
    "curr": "RELATED WORK\n\nInverse reinforcement learning (IRL) aims to use expert demonstrations, often from a human, to infer a reward function (Ng & Russel, 2000;Ziebart et al., 2008).Maximum-entropy (MaxEnt) IRL is a popular IRL framework that models the demonstrator as noisily optimal, maximizing reward while also randomising actions as much as possible (Ziebart et al., 2008;2010).This is equivalent to modeling humans as Boltzmann rational.MaxEnt IRL is preferred in practice over Bayesian IRL (Ramachandran & Amir, 2007), which learns a posterior over reward functions rather than a point estimate, due to better scaling in high-dimensional environments (Wulfmeier et al., 2015).More recently, Guided Cost Learning (Finn et al., 2016) and Adversarial IRL (Fu et al., 2018) learn reward functions more robust to environment changes, but build off similar modeling assumptions as MaxEnt IRL.Gleave & Toyer (2022) connected MaxEnt IRL to maximum likelihood estimation (MLE), which is the framework that we consider in this work.One of the challenges with IRL is that rewards are not always uniquely identified from expert demonstrations (Cao et al., 2021;Kim et al., 2021).Since identifiability is orthogonal to the main message of our work-sensitivity to misspecified human models-we assume that the dataset avoids this ambiguity.",
    "next": "Recent IRL algorithms attempt to account for possible irrationalities in the expert (Evans et al., 2016;Reddy et al., 2018;Shah et al., 2019).Reddy et al.",
    "query": "Are there any research papers on adversarial inverse reinforcement learning methods for robust reward inference in high-dimensional environments?"
  },
  {
    "index": 440,
    "source_corpus_id": 258865444,
    "ref_id": "b14",
    "citation_corpus_id": 248987078,
    "start": 3398,
    "end": 3420,
    "title": "Context Matters for Image Descriptions for Accessibility: Challenges for Referenceless Evaluation Metrics",
    "abstract": "Few images on the Web receive alt-text descriptions that would make them accessible to blind and low vision (BLV) users. Imagebased NLG systems have progressed to the point where they can begin to address this persistent societal problem, but these systems will not be fully successful unless we evaluate them on metrics that guide their development correctly. Here, we argue against current referenceless metrics -those that don't rely on human-generated ground-truth descriptions -on the grounds that they do not align with the needs of BLV users. The fundamental shortcoming of these metrics is that they do not take context into account, whereas contextual information is highly valued by BLV users. To substantiate these claims, we present a study with BLV participants who rated descriptions along a variety of dimensions. An in-depth analysis reveals that the lack of context-awareness makes current referenceless metrics inadequate for advancing image accessibility. As a proof-of-concept, we provide a contextual version of the referenceless metric CLIPScoreSculpture GazeboImage descriptionA freestanding, open, hexagonal gazebo with a dome-like roof in an idyllic park area.",
    "prev": "onto the users, who frequently either for convenience or lack of awareness simply do not choose to do so (Gleason et al., 2019).We find that as many as 98% of images uploaded to Twitter even after the widespread feature rollout do not have alt-text, to say nothing of the quality of those that do.When a screen reader encounters such an image, it will simply say \"Image\", leaving the user with no meaningful information about what the image is actually of.Even when images on Twitter do have accompanying user-written alt-text, the quality is inconsistent as not all users are well informed regarding best practices.",
    "curr": "Note that while there is a long line of existing research on the broader task of captioning images more generally, alt-text generation for social media is a special case of this task, which in turn comes with its own challenges.Well written alt-text is generally more explicitly descriptive than a high level caption and may emphasize specific details in the image based on context (Kreiss et al., 2022a).See Figure 1 for an example.Furthermore, the distribution of image types on Twitter differs substantially from those found in traditional captioning datasets, and may contain digital artwork, promotional graphics, or screenshots containing text.An additional challenge is that Twitter users are not well trained in the practice of writing alt-text, and therefore native \"gold\" examples can vary  ()   Alt-text: Delicious pink margarita sits on a table full of salsa and chips\n\nCaption: having cocktail at person who wants to join ?",
    "next": "Our Dataset Conceptual Captions\n\nFigure 1: Left: An image that requires textual context to write accurate alt-text for.Without conditioning on the tweet text, the election flyers are indistinguishable from books to a traditional captioning system.Right: Two similar images from our dataset and Conceptual Captions with their gold labels.The alt-text for the first image is literally descriptive while the second is more colloquial.",
    "query": "Are there any studies that investigate challenges in generating context-aware alt-text for social media images and the impact of context on accessibility evaluation metrics?"
  },
  {
    "index": 441,
    "source_corpus_id": 257687492,
    "ref_id": "b26",
    "citation_corpus_id": 52908393,
    "start": 27139,
    "end": 27164,
    "title": "CEM-RL: Combining evolutionary and gradient-based methods for policy search",
    "abstract": "Deep neuroevolution and deep reinforcement learning (deep RL) algorithms are two popular approaches to policy search. The former is widely applicable and rather stable, but suffers from low sample efficiency. By contrast, the latter is more sample efficient, but the most sample efficient variants are also rather unstable and highly sensitive to hyper-parameter setting. So far, these families of methods have mostly been compared as competing tools. However, an emerging approach consists in combining them so as to get the best of both worlds. Two previously existing combinations use either an ad hoc evolutionary algorithm or a goal exploration process together with the Deep Deterministic Policy Gradient (ddpg) algorithm, a sample efficient off-policy deep RL algorithm. In this paper, we propose a different combination scheme using the simple cross-entropy method (cem) and Twin Delayed Deep Deterministic policy gradient (td3), another off-policy deep RL algorithm which improves over ddpg. We evaluate the resulting method, cem-rl, on a set of benchmarks classically used in deep RL. We show that cem-rl benefits from several advantages over its competitors and offers a satisfactory trade-off between performance and sample efficiency.",
    "prev": "While PBT-MAP-ELITES and PBT use similar strategies to update the population of agents, PBT only seeks the highest-performing agent by extracting the best one from the final population while PBT-MAP-ELITES aims to find a diverse collection of high-performing agents.",
    "curr": "Several methods such as CERL, ERL, and CEM-RL (Pourchot & Sigaud, 2019;Khadka & Tumer, 2018;Khadka et al., 2019) combine ES algorithms with PBRL methods to improve the asymptotic performance and sample efficiency of standard RL methods.",
    "next": "Other methods, such as DvD (Parker-Holder et al., 2020) and P3S-TD3 (Jung et al., 2020), train populations of agents and add terms in their loss functions to encourage the agents to explore different regions of the state-action space but always with the end goal of maximizing the performance of the best agent in the population.",
    "query": "Are there any research papers on combining evolutionary algorithms with reinforcement learning methods for policy search to enhance sample efficiency and performance?"
  },
  {
    "index": 442,
    "source_corpus_id": 3497822,
    "ref_id": "b4",
    "citation_corpus_id": 51559,
    "start": 3308,
    "end": 3311,
    "title": "QUASI-RECURRENT NEURAL NETWORKS",
    "abstract": "Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's computation on the previous timestep's output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks. * Equal contribution",
    "prev": "Bradbury et al.",
    "curr": "[5], Kalchbrenner et al.",
    "next": "[12], Gehring et al.",
    "query": "Are there any research papers on quasi-recurrent neural networks that use convolutional layers and recurrent pooling to improve parallelism in sequence modeling, with applications in language modeling and machine translation?"
  },
  {
    "index": 444,
    "source_corpus_id": 27494814,
    "ref_id": "b24",
    "citation_corpus_id": 2973141,
    "start": 6807,
    "end": 6824,
    "title": "Compression of Neural Machine Translation Models via Pruning",
    "abstract": "Neural Machine Translation (NMT), like many other deep learning domains, typically suffers from over-parameterization, resulting in large storage sizes. This paper examines three simple magnitude-based pruning schemes to compress NMT models, namely class-blind, class-uniform, and class-distribution, which differ in terms of how pruning thresholds are computed for the different classes of weights in the NMT architecture. We demonstrate the efficacy of weight pruning as a compression technique for a state-of-the-art NMT system. We show that an NMT model with over 200 million parameters can be pruned by 40% with very little performance loss as measured on the WMT'14 English-German translation task. This sheds light on the distribution of redundancy in the NMT architecture. Our main result is that with retraining, we can recover and even surpass the original performance with an 80%-pruned model.",
    "prev": "In Optimal Brain Surgeon (Hassibi et al., 1993), the saliency for each parameter was computed using the inverse Hessian matrix, and the low-saliency weights are pruned and all other weights in the network are updated using the second-order information.",
    "curr": "More recently, magnitude-based weight pruning methods have become popular techniques for network pruning (Han et al., 2015b,a;See et al., 2016;Narang et al., 2017).",
    "next": "Magnitude-based weight pruning techniques are computationally efficient, scaling to large networks and datasets.",
    "query": "Are there any studies that investigate the effectiveness of magnitude-based pruning methods for compressing neural machine translation models and their impact on model performance with retraining?"
  },
  {
    "index": 445,
    "source_corpus_id": 3340951,
    "ref_id": "b14",
    "citation_corpus_id": 2134321,
    "start": 7814,
    "end": 7831,
    "title": "Published as a conference paper at ICLR 2016 DEEP COMPRESSION: COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING",
    "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35× to 49× without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9× to 13×; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35×, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49× from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3× to 4× layerwise speedup and 3× to 7× better energy efficiency.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on techniques for compressing deep neural networks using pruning, trained quantization, and Huffman coding?"
  },
  {
    "index": 448,
    "source_corpus_id": 235417313,
    "ref_id": "b0",
    "citation_corpus_id": 235614315,
    "start": 4834,
    "end": 4852,
    "title": "Published as a conference paper at ICLR 2021 FEDERATED LEARNING BASED ON DYNAMIC REGULARIZATION",
    "abstract": "We propose a novel federated learning method for distributively training neural network models, where the server orchestrates cooperation between a subset of randomly chosen devices in each round. We view Federated Learning problem primarily from a communication perspective and allow more device level computations to save transmission costs. We point out a fundamental dilemma, in that the minima of the local-device level empirical loss are inconsistent with those of the global empirical loss. Different from recent prior works, that either attempt inexact minimization or utilize devices for parallelizing gradient computation, we propose a dynamic regularizer for each device at each round, so that in the limit the global and device solutions are aligned. We demonstrate both through empirical results on real and synthetic data as well as analytical results that our scheme leads to efficient training, in both convex and non-convex settings, while being fully agnostic to device heterogeneity and robust to large number of devices, partial participation and unbalanced data.",
    "prev": "arXiv preprint arXiv:\n\nINTRODUCTION\n\nFederated learning (FL) (McMahan et al., 2017), which is a distributed learning framework with personalized data, has become an attractive field of research.",
    "curr": "From the early days of this field, improving a single global model across devices has been the main objective (Zhao et al., 2018;Duan et al., 2019;Li et al., 2018;Acar et al., 2021), where the global model suffers from data heterogeneity.",
    "next": "Many researchers have recently focused on multiple personalized models by leveraging data heterogeneity across devices as a blessing in disguise (Chen et al., 2018;Dinh et al., 2021;Zhang et al., 2020;Fallah et al., 2020;Shamsian et al., 2021;Smith et al., 2017).",
    "query": "Are there any research papers on federated learning approaches that use dynamic regularization techniques to address data heterogeneity and improve alignment between global and local model solutions?"
  },
  {
    "index": 449,
    "source_corpus_id": 220249871,
    "ref_id": "b3",
    "citation_corpus_id": 213152193,
    "start": 9504,
    "end": 9524,
    "title": "ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS",
    "abstract": "Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK]  and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.",
    "prev": "(2017), a hand-craft positional encoding based on sinusoid function is proposed.",
    "curr": "But learnable positional encoding, i.e., treating p i as parameters, is often used in the recent works Devlin et al., 2018;Clark et al., 2019b).",
    "next": "Relative Positional Encoding.",
    "query": "Are there any research papers on pre-training text encoders as discriminators rather than generators using replaced token detection?"
  },
  {
    "index": 450,
    "source_corpus_id": 60440615,
    "ref_id": "b24",
    "citation_corpus_id": 16631020,
    "start": 1779,
    "end": 1798,
    "title": "Transfer Learning for Low-Resource Neural Machine Translation",
    "abstract": "The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that significantly improves BLEU scores across a range of low-resource languages. Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 BLEU on four low-resource language pairs. Ensembling and unknown word replacement add another 2 BLEU which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair. Additionally, using the transfer learning model for re-scoring, we can improve the SBMT system by an average of 1.3 BLEU, improving the state-of-the-art on low-resource machine translation.",
    "prev": "Experiments on a standard dataset of four low-resource languages show consistent improvements over strong multilingual NMT baselines, with gains of up to 2 BLEU on one of the tested languages, achieving the new state-of-the-art on all four language pairs 1 .",
    "curr": "INTRODUCTION\n\nMultilingual Neural Machine Translation (NMT) has shown great potential both in creating parameter-efficient MT systems for many languages (Johnson et al., 2016), and in improving translation quality of low-resource languages (Zoph et al., 2016;Firat et al., 2016;Gu et al., 2018;Neubig & Hu, 2018;Nguyen & Chiang, 2018).",
    "next": "Despite the success of multilingual NMT, it remains a research question how to represent the words from multiple languages in a way that is both parameter efficient and conducive to cross-lingual generalization.",
    "query": "Are there any research papers on transfer learning methods for improving low-resource neural machine translation?"
  },
  {
    "index": 451,
    "source_corpus_id": 252762329,
    "ref_id": "b14",
    "citation_corpus_id": 238418995,
    "start": 7526,
    "end": 7544,
    "title": "LARGE LEARNING RATE TAMES HOMOGENEITY: CONVERGENCE AND BALANCING EFFECT",
    "abstract": "Recent empirical advances show that training deep models with large learning rate often improves generalization performance. However, theoretical justifications on the benefits of large learning rate are highly limited, due to challenges in analysis. In this paper, we consider using Gradient Descent (GD) with a large learning rate on a homogeneous matrix factorization problem, i.e., min X,Y A − XY 2 F . We prove a convergence theory for constant large learning rates well beyond 2/L, where L is the largest eigenvalue of Hessian at the initialization. Moreover, we rigorously establish an implicit bias of GD induced by such a large learning rate, termed 'balancing', meaning that magnitudes of X and Y at the limit of GD iterations will be close even if their initialization is significantly unbalanced. Numerical experiments are provided to support our theory.Published as a conference paper at ICLR 2022 Problem (1) possesses several intriguing properties. Firstly, the objective function is non-convex, and critical points are either global minima or saddles (see e.g.,Baldi & Hornik (1989); Li et al. (2019b); Valavi et al. (2020a); Chen et al. (2018)). Secondly, problem (1) is homogeneous in X and Y , meaning that rescaling X, Y to aX, a −1 Y for any a = 0 will not change the objective's value. This property is shared by commonly used ReLU neural networks. A direct consequence of homogeneity is that global minima of (1) are non-isolated and can be unbounded. The curvatures at these global minima are highly dependent on the magnitudes of X, Y . When X, Y have comparable magnitudes, the largest eigenvalue of Hessian is small, and this corresponds to a flat minimum; on the contrary, unbalanced X and Y give a sharp minimum. Last but not the least, the homogeneity impairs smoothness conditions of (1), rendering the gradient being not Lipschitz continuous unless X, Y are bounded. See a formal discussion in Section 2.Existing approaches for solving (1) often uses explicit regularization(Ge et al., 2017;Tu et al., 2016;Cabral et al., 2013;Li et al., 2019a), or infinitesimal (or diminishing) learning rates for controlling the magnitudes of X, Y (Du et al., 2018; Ye & Du, 2021). In this paper, we go beyond the scope of aforementioned works, and analyze GD with a large learning rate for solving (1). In particular, we allow the learning rate h to be as large as approximately 4/L (see more explanation in Section 2), where L denotes the largest eigenvalue of Hessian at GD initialization. In connection to empirical observations, we provide positive answers to the following two questions:Does GD with large learning rate converge at least for some cases of (1)? Does larger learning rate biases toward flatter minima (i.e., X, Y with comparable magnitudes)?We remark that having a learning rate h ≈ 4/L is far beyond the commonly analyzed regime in optimization. Even for globally L-smooth objective, traditional theory requires h < 2/L for GD convergence and h = 1/L is optimal for convex functions(Boyd et al., 2004), not to mention that our problem (1) is never globally L-smooth due to homogeneity. Modified equation provides a tool for probing intermediate learning rates (see Hairer et al. (2006, Chapter 9) for a general review, and Kong & Tao (2020, Appendix A) for the specific setup of GD), but the learning rate here is too large for modified equation to work (see Appendix C). In fact, besides blowing up, GD with large learning rate may have a zoology of limiting behaviors (see e.g., Appendix B for convergence to periodic orbits under our setup, and Kong & Tao (2020) for convergence to chaotic attractors).Our analyses (of convergence and balancing) leverage various mathematical tools, including a proper partition of state space and its dynamical transition (specifically invented for this problem), stability theory of discrete time dynamical systems(Alligood et al., 1996), and geometric measure theory(Federer, 2014).The rest of the paper is organized as: Section 2 provides the background of studying (1) and discusses related works; Section 3 presents convergence and balancing results for scalar factorization problems; Section 4 generalizes the theory to rank-1 matrix approximation; Section 5 studies problem (1) with arbitrary A and its arbitrary-rank approximation; Section 6 summarizes the paper and discusses broadly related topics and future directions.",
    "prev": "They show that the loss can non-monotonically decrease even when the sharpness λ > 2/η.",
    "curr": "The non-monotone property of the loss has also been observed in many other settings (Jastrzebski et al., 2020;Xing et al., 2018;Lewkowycz et al., 2020;Wang et al., 2022;Arora et al., 2018;Li et al., 2022a).",
    "next": "Recently several works try to understand the mechanism behind EoS with different loss functions under various assumptions (Ahn et al., 2022;Ma et al., 2022;Lyu et al., 2022;Li et al., 2022b).",
    "query": "Are there any research papers on the convergence analysis of gradient descent with large learning rates in homogeneous matrix factorization problems and the implicit balancing effect?"
  },
  {
    "index": 452,
    "source_corpus_id": 253098739,
    "ref_id": "b33",
    "citation_corpus_id": 68137503,
    "start": 3821,
    "end": 3838,
    "title": "FUNCTIONAL VARIATIONAL BAYESIAN NEURAL NETWORKS",
    "abstract": "Variational Bayesian neural networks (BNNs) perform variational inference over weights, but it is difficult to specify meaningful priors and approximate posteriors in a high-dimensional weight space. We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions. We prove that the KL divergence between stochastic processes equals the supremum of marginal KL divergences over all finite sets of inputs. Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator. With fBNNs, we can specify priors entailing rich structures, including Gaussian processes and implicit stochastic processes. Empirically, we find fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and scale to large datasets. * Equal contribution.Theorem 7. For two stochastic processes P, M on a cylindrical measurable space (Ω T , F T ), the KL divergence of P with respect to M satisfies,where the supremum is over all finite indices subsets T d ⊆ T , and P T d , M T d represent the canonical projection maps π T →T d of P, M , respectively.Proof. Recall that stochastic processes are defined over a cylindrical σ-algebra F T . By Lemma 6, for every set H ∈ F T , the restricted index set τ (H) is countable. Our proof proceeds in two steps:",
    "prev": "To address these shortcomings, we take a new approach to formulating the meta-learning problem and represent prior knowledge in a novel way.",
    "curr": "We build on recent advances in functional approximate inference for BNNs that perform Bayesian inference in the function space rather than in the parameter space of neural networks (Wang et al., 2018;Sun et al., 2019).",
    "next": "When viewing the BNN prior and posterior as stochastic processes, the perfect Bayesian prior is the (true) data-generating stochastic process itself.",
    "query": "Are there any research papers on Bayesian neural networks that perform inference in function space using stochastic processes?"
  },
  {
    "index": 453,
    "source_corpus_id": 251710555,
    "ref_id": "b6",
    "citation_corpus_id": 52967399,
    "start": 2072,
    "end": 2093,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Compared to Linformer, the accuracy can be as much as 12% higher while using similar FLOPs in the attention layer.",
    "curr": "INTRODUCTION\n\nSelf attention layer is the key component of Transformers (Vaswani et al., 2017), enabling them to achieve state of the art performance across tasks in Natural Language Processing (Devlin et al., 2019;Raffel et al., 2019) and Computer Vision (Dosovitskiy et al., 2021).",
    "next": "Attention computation scales quadratically (n 2 ) with the input sequence length (n), making it a key bottleneck in scaling Transformers to long inputs.",
    "query": "Are there any research papers on methods to improve the scalability of self-attention mechanisms in Transformers for natural language processing and computer vision tasks?"
  },
  {
    "index": 457,
    "source_corpus_id": 248266388,
    "ref_id": "b9",
    "citation_corpus_id": 14717992,
    "start": 1555,
    "end": 1575,
    "title": "REINFORCEMENT LEARNING WITH UNSUPERVISED AUXILIARY TASKS",
    "abstract": "Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-theart on Atari, averaging 880% expert human performance, and a challenging suite of first-person, three-dimensional Labyrinth tasks leading to a mean speedup in learning of 10× and averaging 87% expert human performance on Labyrinth.Natural and artificial agents live in a stream of sensorimotor data. At each time step t, the agent receives observations o t and executes actions a t . These actions influence the future course of the sensorimotor stream. In this paper we develop agents that learn to predict and control this stream, by solving a host of reinforcement learning problems, each focusing on a distinct feature of the sensorimotor stream. Our hypothesis is that an agent that can flexibly control its future experiences will also be able to achieve any goal with which it is presented, such as maximising its future rewards.The classic reinforcement learning paradigm focuses on the maximisation of extrinsic reward. However, in many interesting domains, extrinsic rewards are only rarely observed. This raises questions of what and how to learn in their absence. Even if extrinsic rewards are frequent, the sensorimotor stream contains an abundance of other possible learning targets. Traditionally, unsupervised learning attempts to reconstruct these targets, such as the pixels in the current or subsequent frame. It is typically used to accelerate the acquisition of a useful representation. In contrast, our learning objective is to predict and control features of the sensorimotor stream, by treating them as pseudorewards for reinforcement learning. Intuitively, this set of tasks is more closely matched with the agent's long-term goals, potentially leading to more useful representations.Consider a baby that learns to maximise the cumulative amount of red that it observes. To correctly predict the optimal value, the baby must understand how to increase \"redness\" by various means, including manipulation (bringing a red object closer to the eyes); locomotion (moving in front of a red object); and communication (crying until the parents bring a red object). These behaviours are likely to recur for many other goals that the baby may subsequently encounter. No understanding of these behaviours is required to simply reconstruct the redness of current or subsequent images.Our architecture uses reinforcement learning to approximate both the optimal policy and optimal value function for many different pseudo-rewards. It also makes other auxiliary predictions that serve to focus the agent on important aspects of the task. These include the long-term goal of predicting cumulative extrinsic reward as well as short-term predictions of extrinsic reward. To learn more efficiently, our agents use an experience replay mechanism to provide additional updates * Joint first authors. Ordered alphabetically by first name.",
    "prev": "* Correspondence to clare.",
    "curr": "INTRODUCTION\n\nDeep reinforcement learning has achieved remarkable successes in a variety of tasks (Mnih et al., 2015;Moravčík et al., 2017;Silver et al., 2017;Abreu et al., 2019), but its impressive performance is mirrored by its brittleness and sensitivity to seemingly innocuous design choices (Henderson et al., 2018).",
    "next": "In sparse-reward environments in particular, even different random seeds of the same algorithm can attain dramatically different performance outcomes.",
    "query": "Are there any research papers on reinforcement learning methods that incorporate unsupervised auxiliary tasks and pseudo-rewards for improving performance in sparse-reward environments?"
  },
  {
    "index": 458,
    "source_corpus_id": 221447287,
    "ref_id": "b36",
    "citation_corpus_id": 49882757,
    "start": 2654,
    "end": 2672,
    "title": "ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech",
    "abstract": "In this work, we propose an alternative solution for parallel wave generation by WaveNet. In contrast to parallel WaveNet (Oord et al., 2018), we distill a Gaussian inverse autoregressive flow from the autoregressive WaveNet by minimizing a novel regularized KL divergence between their highly-peaked output distributions. Our method computes the KL divergence in closed-form, which simplifies the training algorithm and provides very efficient distillation. In addition, we propose the first text-to-wave neural architecture for speech synthesis, which is fully convolutional and enables fast end-to-end training from scratch. It significantly outperforms the previous pipeline that connects a text-to-spectrogram model to a separately trained WaveNet (Ping et al., 2018). We also successfully distill a parallel waveform synthesizer conditioned on the hidden representation in this end-to-end model. 2 * These authors contributed equally to this work. Correspondence to <weiping.thu@gmail.com>. Our method is named after the musical instrument clarinet, whose sound resembles human voice.2 Audio samples are in https://clarinet-demo.github.io/ arXiv:1807.07281v2 [cs.CL] 30 Jul 2018",
    "prev": "However, autoregressive models require a large number of sequential computational steps to generate one audio sample, which makes it challenging to deploy these models in real-world production applications, such as digital voice assistants on smart speakers, even using specialized hardware designed for neural networks.",
    "curr": "There has been a plethora of research into non-autoregressive models for audio generation, including normalizing flow (NF) models like inverse autoregressive flow (IAF) Ping et al., 2019), generative flow (Glow) (Prenger et al., 2019;Kim et al., 2019), and continuous normalizing flow (CNF) Wu & Ling, 2020), implicit generative models like generative adversarial network (GAN) (Donahue et al., 2018;Engel et al., 2019;Yamamoto et al., 2020;Bikowski et al., 2020;McCarthy & Ahmed, 2020) and energy score (Gritsenko et al., 2020), variational models like variational auto-encoder (VAE) (Peng et al., 2020), models inspired by digital signal processing (Ai & Ling, 2020;Engel et al., 2020), and those by the speech production mechanism (Juvela et al., 2019;Wang et al., 2020).",
    "next": "Although these models improve inference speed thanks to their architectures offering less sequential operations, they often yield lower quality samples than the autoregressive models.",
    "query": "Are there any research papers on parallel audio generation methods for text-to-speech that address the trade-off between inference speed and sample quality using distillation techniques or non-autoregressive architectures?"
  },
  {
    "index": 460,
    "source_corpus_id": 48361056,
    "ref_id": "b7",
    "citation_corpus_id": 848112,
    "start": 4514,
    "end": 4539,
    "title": "TRAINING GANS WITH OPTIMISM",
    "abstract": "We address the issue of limit cycling behavior in training Generative Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for training Wasserstein GANs. Recent theoretical results have shown that optimistic mirror decent (OMD) can enjoy faster regret rates in the context of zero-sum games. WGANs is exactly a context of solving a zero-sum game with simultaneous noregret dynamics. Moreover, we show that optimistic mirror decent addresses the limit cycling problem in training WGANs. We formally show that in the case of bi-linear zero-sum games the last iterate of OMD dynamics converges to an equilibrium, in contrast to GD dynamics which are bound to cycle. We also portray the huge qualitative difference between GD and OMD dynamics with toy examples, even when GD is modified with many adaptations proposed in the recent literature, such as gradient penalty or momentum. We apply OMD WGAN training to a bioinformatics problem of generating DNA sequences. We observe that models trained with OMD achieve consistently smaller KL divergence with respect to the true underlying distribution, than models trained with GD variants. Finally, we introduce a new algorithm, Optimistic Adam, which is an optimistic variant of Adam. We apply it to WGAN training on CIFAR10 and observe improved performance in terms of inception score as compared to Adam.",
    "prev": "Searching for a more stable function family (Radford et al., 2015) and utilizing better objectives Gulrajani et al., 2017) have been explored for training and instability issues.",
    "curr": "For tackling non-convergence, regularizing objective functions (Mescheder et al., 2017;Salimans et al., 2016), two time-scale techniques (Heusel et al., 2017) and extra-gradient (optimistic) methods (Daskalakis et al., 2018;Mertikopoulos et al., 2019) have been studied.",
    "next": "One reason for non-convergence is cycling around an optimal solution (Mertikopoulos et al., 2018;, or even slow outward spiraling (Bailey & Piliouras, 2018).",
    "query": "Are there any studies that explore optimistic optimization techniques for stabilizing GAN training and addressing non-convergence issues such as limit cycling in Wasserstein GANs?"
  },
  {
    "index": 462,
    "source_corpus_id": 1803861,
    "ref_id": "b4",
    "citation_corpus_id": 3130692,
    "start": 36851,
    "end": 36872,
    "title": "Learning to Compose Neural Networks for Question Answering",
    "abstract": "We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural module network, achieves state-of-theart results on benchmark datasets in both visual and structured domains.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on dynamic neural module networks for question answering using reinforcement learning and composable modules in both visual and structured domains?"
  },
  {
    "index": 463,
    "source_corpus_id": 231719730,
    "ref_id": "b5",
    "citation_corpus_id": 213152193,
    "start": 20388,
    "end": 20408,
    "title": "ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS",
    "abstract": "Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK]  and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.",
    "prev": "Unlike the training datasets, the evaluation document was translated to plain L A T E X manually using the PDF as a reference, in order to avoid possible spurious patterns in automatically expanded sT E X.",
    "curr": "S T E X-ANNOTATING WITH MACHINE LEARNING AS AN NMT TASK\n\nIn the course of our experiments, we considered our disambiguation task as a machine translation (NMT) problem, the models for which have been proven to be quite effective even beyond natural language translations (Clark et al., 2020).",
    "next": "In fact, the autoformalization projects mentiond in Section 3, which are spiritually closest to our task, all used NMT models with positive results.",
    "query": "Are there any research papers on using machine translation (NMT) models for disambiguation tasks in STeX annotation?"
  },
  {
    "index": 465,
    "source_corpus_id": 58014184,
    "ref_id": "b2",
    "citation_corpus_id": 11212020,
    "start": 10881,
    "end": 10904,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "The queried values are invariant to the ordering of the key-value pairs; this permutation invariance property of attention is key in its application to NPs.",
    "curr": "The idea of using a differentiable addressing mechanism that can be learned from the data has been applied successfully in various areas of Deep Learning, namely handwriting generation and recognition (Graves, 2012) and neural machine translation (Bahdanau et al., 2015).",
    "next": "More recently, there has been work employing self-attention (where keys and queries are identical) to give expressive sequence-to-sequence mappings in natural language processing (Vaswani et al., 2017) and image modelling (Parmar et al., 2018).",
    "query": "Are there any research papers on attention mechanisms in neural machine translation that employ differentiable addressing and soft alignment strategies?"
  },
  {
    "index": 470,
    "source_corpus_id": 264406180,
    "ref_id": "b6",
    "citation_corpus_id": 229923720,
    "start": 17860,
    "end": 17880,
    "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
    "abstract": "Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformerbased language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution.",
    "prev": "Mean ablating the 1st or 2nd head slightly increased the average loss for BA questions from 0.05 to 0.08, whereas ablating the 3rd head substantially increased the loss to 3.7, confirming that the 3rd head is doing the BA task.",
    "curr": "The MLP can be thought of as a \"key-value pair\" memory [Meng et al., 2022, Geva et al., 2021] that can hold many bigrams and trigrams.We claim our MLP pulls together the two-state 1st head result, the tri-state 2nd head result and the ten-state 3rd head result value, treating them as a trigram with 60 (2 x 3 x 10) possible keys.For each digit, the MLP has memorised the mapping of these 60 keys to the 60 correct digit answers (0 to 9).We haven't proven this experimentally.Our MLP is sufficiently large to store this many mappings with zero interference between mappings [Elhage et al., 2022].",
    "next": "Despite being feasible, the model does not calculate the task MC1 in layers 7 to 11.Instead it completes each digit calculation in 1 layer, possibly because there are training optimisation benefits in generating a \"compact\" algorithm.",
    "query": "Are there any studies that investigate the role of transformer feed-forward layers as key-value memory structures for storing and processing textual patterns in language models?"
  },
  {
    "index": 471,
    "source_corpus_id": 59413817,
    "ref_id": "b12",
    "citation_corpus_id": 3286670,
    "start": 1973,
    "end": 1995,
    "title": "EXPRESSIVE POWER OF RECURRENT NEURAL NET- WORKS",
    "abstract": "Deep neural networks are surprisingly efficient at solving practical tasks, but the theory behind this phenomenon is only starting to catch up with the practice. Numerous works show that depth is the key to this efficiency. A certain class of deep convolutional networks -namely those that correspond to the Hierarchical Tucker (HT) tensor decomposition -has been proven to have exponentially higher expressive power than shallow networks. I.e. a shallow network of exponential width is required to realize the same score function as computed by the deep architecture. In this paper, we prove the expressive power theorem (an exponential lower bound on the width of the equivalent shallow network) for a class of recurrent neural networks -ones that correspond to the Tensor Train (TT) decomposition. This means that even processing an image patch by patch with an RNN can be exponentially more efficient than a (shallow) convolutional network with one hidden layer. Using theoretical results on the relation between the tensor decompositions we compare expressive powers of the HT-and TT-Networks. We also implement the recurrent TT-Networks and provide numerical evidence of their expressivity.",
    "prev": "The ability of these neural networks to efficiently represent a rich class of functions with a relatively small number of parameters is often referred to as depth efficiency, and the theory behind this phenomenon is not yet fully understood.",
    "curr": "A recent line of work Khrulkov et al., 2018;Cohen et al., 2018) focuses on comparing various deep learning architectures in terms of their expressive power.",
    "next": "It was shown in  that ConvNets with product pooling are exponentially more expressive than shallow networks, that is there exist functions realized by ConvNets which require an exponentially large number of parameters in order to be realized by shallow nets.",
    "query": "Are there any studies that compare the expressive power of recurrent neural networks with other deep learning architectures, such as convolutional networks, in terms of parameter efficiency?"
  },
  {
    "index": 473,
    "source_corpus_id": 238419359,
    "ref_id": "b5",
    "citation_corpus_id": 189762527,
    "start": 2107,
    "end": 2129,
    "title": "COMET : Commonsense Transformers for Automatic Knowledge Graph Construction",
    "abstract": "We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et al., 2019) and Con-ceptNet (Speer et al., 2017). Contrary to many conventional KBs that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET ) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality, with up to 77.5% (ATOMIC) and 91.7% (ConceptNet) precision at top 1, which approaches human performance for these resources. Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods.",
    "prev": "Such knowledge can be implicitly encoded or explicitly stored in structured knowledge graphs (KGs).",
    "curr": "Large pre-trained language models (Devlin et al., 2018;Radford et al., 2018;Brown et al., 2020) are found to be effective in learning broad and rich implicit knowledge (Petroni et al., 2019;Bosselut et al., 2019;Talmor et al., 2020) and thus demonstrate much success for QA tasks.",
    "next": "Nevertheless, pretrained LMs struggle a lot with structured reasoning such as handling negation (Ribeiro et al., 2020;Yasunaga et al., 2021).",
    "query": "Are there any research papers on generative models for automatic commonsense knowledge graph construction using pre-trained language models?"
  },
  {
    "index": 474,
    "source_corpus_id": 264306063,
    "ref_id": "b28",
    "citation_corpus_id": 1957433,
    "start": 7123,
    "end": 7147,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "In summary, our primary contributions are a) the emulated fine-tuning framework; b) clear experimental justification for the claim that scaling pre-training leads to improved factual knowledge while scaling finetuning leads to improved task adherence; and c) the technique of model up-scaling, which enables a small fine-tuned model and large base model to approximate the compute-intensive result of fine-tuning a large base model.",
    "curr": "Related Work\n\nThe benefits of unsupervised pre-training in neural networks was first identified in deep belief networks (Hinton et al., 2006) and stacked autoencoders (Bengio et al., 2007), with early analyses noting persistent effects of pre-training even when fine-tuning data is not limited (Erhan et al., 2010).In natural language processing, pre-trained representations of individual words (Mikolov et al., 2013;Pennington et al., 2014) or entire passages (Devlin et al., 2019;Peters et al., 2018) demonstrated the ability for task-agnostic pre-training to learn representations useful for a wide variety of downstream linguistic tasks such as question-answering, natural language inference, and translation (Devlin et al., 2019;Raffel et al., 2020).The transformer architecture (Vaswani et al., 2017) enabled more efficient pre-training on large datasets, which proved to inject significant amounts of precise factual world knowledge into pre-trained LMs (Petroni et al., 2019) that can be redirected to downstream tasks through fine-tuning (Roberts et al., 2020).Most recently, various works have shown that language models pre-trained with unsupervised generative modeling can be fine-tuned to engage in general-purpose dialogue, producing a model that can perform a variety of complex tasks specified in natural language (Thoppilan et al., 2022;Ouyang et al., 2022;Bai et al., 2022;Bubeck et al., 2023;Touvron et al., 2023b).Due to",
    "next": null,
    "query": "Are there any research papers on word representation models that combine global matrix factorization with local context window methods?"
  },
  {
    "index": 475,
    "source_corpus_id": 244714829,
    "ref_id": "b10",
    "citation_corpus_id": 85501317,
    "start": 27512,
    "end": 27529,
    "title": "Cyclical Annealing Schedule: A Simple Approach to Mitigating KL Vanishing",
    "abstract": "Variational autoencoders (VAEs) with an autoregressive decoder have been applied for many natural language processing (NLP) tasks. The VAE objective consists of two terms, (i) reconstruction and (ii) KL regularization, balanced by a weighting hyper-parameter β. One notorious training difficulty is that the KL term tends to vanish. In this paper we study scheduling schemes for β, and show that KL vanishing is caused by the lack of good latent codes in training the decoder at the beginning of optimization. To remedy this, we propose a cyclical annealing schedule, which repeats the process of increasing β multiple times. This new procedure allows the progressive learning of more meaningful latent codes, by leveraging the informative representations of previous cycles as warm re-starts. The effectiveness of cyclical annealing is validated on a broad range of NLP tasks, including language modeling, dialog response generation and unsupervised language pre-training.",
    "prev": "EXPERIMENTS\n\n\nEXPERIMENTAL SETTINGS\n\nWe train the conditional VAE described in Section 4.2 and all implicit functions using the Adam solver (Kingma & Ba, 2014) with a learning rate of 5 × 10 −5 .",
    "curr": "To balance the KL-divergence and the reconstruction objectives in the VAE, we adopt the Cyclical Annealing Schedule introduced in (Fu et al., 2019).",
    "next": "We evaluate the proposed environment field on agent navigation in 2D mazes (Section 5.2) and compare with the VIN (Tamar et al., 2016).",
    "query": "Are there any studies that explore cyclical annealing schedules for balancing reconstruction and KL divergence objectives in variational autoencoders?"
  },
  {
    "index": 476,
    "source_corpus_id": 253510295,
    "ref_id": "b2",
    "citation_corpus_id": 53729760,
    "start": 3778,
    "end": 3780,
    "title": "GAN DISSECTION: VISUALIZING AND UNDERSTANDING GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, they have not been well visualized or understood. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. We examine the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in a scene. We provide open source interpretation tools to help researchers and practitioners better understand their GAN models * . * Interactive demos, video, code, and data are available at GitHub and gandissect.",
    "prev": "6\n\nINTRODUCTION\n\nSemantic image synthesis refers to generating photo-realistic images conditioned on pixel-level semantic labels.",
    "curr": "This task has a wide range of applications such as image editing and content generation (Chen & Koltun, 2017;Isola et al., 2017;Guo et al., 2022;Gu et al., 2019;Bau et al., 2019a;b;Liu et al., 2019;Qi et al., 2018;Jiang et al., 2020).",
    "next": "Although existing methods conducted interesting explorations, we still observe unsatisfactory aspects, mainly in the generated local structures and details, as well as small-scale objects, which we believe are mainly due to three reasons: 1) Conventional methods (Park et al., 2019;Wang et al., 2018;Liu et al., 2019) generally take the semantic label map as input directly.",
    "query": "Are there any studies that explore techniques for visualizing and understanding Generative Adversarial Networks, including their internal representations, causes of artifacts, and the impact of architectural choices on model performance?"
  },
  {
    "index": 477,
    "source_corpus_id": 252873224,
    "ref_id": "b22",
    "citation_corpus_id": 189898036,
    "start": 2669,
    "end": 2675,
    "title": "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks",
    "abstract": "Recent works on implicit regularization have shown that gradient descent converges to the max-margin direction for logistic regression with one-layer or multi-layer linear networks. In this paper, we generalize this result to homogeneous neural networks, including fully-connected and convolutional neural networks with ReLU or LeakyReLU activations. In particular, we study the gradient flow (gradient descent with infinitesimal step size) optimizing the logistic loss or cross-entropy loss of any homogeneous model (possibly non-smooth), and show that if the training loss decreases below a certain threshold, then we can define a smoothed version of the normalized margin which increases over time. We also formulate a natural constrained optimization problem related to margin maximization, and prove that both the normalized margin and its smoothed version converge to the objective value at a KKT point of the optimization problem. Furthermore, we extend the above results to a large family of loss functions. We conduct several experiments to justify our theoretical finding on MNIST and CIFAR-10 datasets. For gradient descent with constant learning rate, we observe that the normalized margin indeed keeps increasing after the dataset is fitted, but the speed is very slow. However, if we schedule the learning rate more carefully, we can observe a more rapid growth of the normalized margin. Finally, as margin is closely related to robustness, we discuss potential benefits of training longer for improving the robustness of the model. model fits the training data perfectly). For example, given a Convolutional Neural Network (CNN) that has achieved 100% training accuracy, one can easily make the cross-entropy loss arbitrarily small by scaling up the weight and bias parameters (W , b) at the last layer, i.e., transforming (W , b) to (cW , cb) for large enough c > 0. This means that, similar to linear logistic regression, CNNs also have some parameters whose scale does not matter, and hence a promising and meaningful research direction is to study whether their convergent direction maximizes the margin. In general, we observe that the following three properties that are usually satisfied by modern deep neural networks:1. Partial Homogeneity. The output of the neural network is (positively) homogeneous with respect to a part of its parameters (e.g., the parameters at the last linear layer);2. Separability. The training set is separable by the neural network for some set of parameters, i.e., the neural network has sufficient representation power to achieve 100% training accuracy (this is true for state-of-the-art CNNs for image classification, and many of them even have enough capacity to fit randomly labeled data easily [Zhang et al., 2017]);3. No finite minima on the loss function. The loss function used to measure the similarity between the network output and ground-truth is lower bounded by a constant (e.g., 0) but it does not have finite minima (e.g., exponential loss, logistic loss, cross-entropy loss).Simplifications. For simplicity and ease of presentation, we make the following simplifications. First, as the most prominent examples of homogeneous neural networks are all non-smooth (e.g., ReLU networks), we turn to analyze the case of training neural networks by gradient flow (more precisely, subgradient flow in Clarke's sense).Second, we ensure Separability as follows: we assume that after time t 0 , the training loss is smaller than a threshold, and the threshold here is chosen to be so small that the training accuracy is guaranteed to be 100% (e.g., for the logistic loss and cross-entropy loss, the threshold can be set to ln 2). In this paper, we focus on analyzing the behavior of the network after t 0 .",
    "prev": "For gradient flow, we consider the standard leaky ReLU activation, φ(z) = max(γz, z).",
    "curr": "Our starting point in this setting is recent work by Lyu and Li [LL20] and Ji and Telgarsky [JT20] that show that, provided the network interpolates the training data at some time, gradient flow on homogeneous networks, such as two-layer leaky ReLU networks, converges (in direction) to a network that satisfies the Karush-Kuhn-Tucker (KKT) conditions for the margin-maximization problem,\nmin W 1 2 W 2 F s.t.",
    "next": "∀i ∈ [n], y i f (x i ; W ) ≥ 1 .",
    "query": "Are there any research papers on gradient descent maximizing the margin in homogeneous neural networks with ReLU or LeakyReLU activations?"
  },
  {
    "index": 478,
    "source_corpus_id": 14298291,
    "ref_id": "b21",
    "citation_corpus_id": 806709,
    "start": 3895,
    "end": 3916,
    "title": "Semantic Compositionality through Recursive Matrix-Vector Spaces",
    "abstract": "Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.",
    "prev": "(2014), who focus on learning expression representations to aid the search for computationally efficient identities.",
    "curr": "They use recursive neural networks (TREENN) 1 (Socher et al., 2012) for modeling homogenous, single-variable polynomial expressions.",
    "next": "While they present impressive results, we find that the TREENN model fails when applied to more complex symbolic polynomial and boolean expressions.",
    "query": "Are there any studies that address the limitations of recursive neural networks in handling complex symbolic and boolean expressions for semantic compositionality?"
  },
  {
    "index": 479,
    "source_corpus_id": 228705808,
    "ref_id": "b12",
    "citation_corpus_id": 208248131,
    "start": 2018,
    "end": 2037,
    "title": "Published as a conference paper at ICLR 2020 ECONOMY STATISTICAL RECURRENT UNITS FOR INFERRING NONLINEAR GRANGER CAUSALITY",
    "abstract": "Granger causality is a widely-used criterion for analyzing interactions in largescale networks. As most physical interactions are inherently nonlinear, we consider the problem of inferring the existence of pairwise Granger causality between nonlinearly interacting stochastic processes from their time series measurements. Our proposed approach relies on modeling the embedded nonlinearities in the measurements using a component-wise time series prediction model based on Statistical Recurrent Units (SRUs). We make a case that the network topology of Granger causal relations is directly inferrable from a structured sparse estimate of the internal parameters of the SRU networks trained to predict the processes' time series measurements. We propose a variant of SRU, called economy-SRU, which, by design has considerably fewer trainable parameters, and therefore less prone to overfitting. The economy-SRU computes a low-dimensional sketch of its high-dimensional hidden state in the form of random projections to generate the feedback for its recurrent processing. Additionally, the internal weight parameters of the economy-SRU are strategically regularized in a group-wise manner to facilitate the proposed network in extracting meaningful predictive features that are highly time-localized to mimic real-world causal events. Extensive experiments are carried out to demonstrate that the proposed economy-SRU based time series prediction model outperforms the MLP, LSTM and attention-gated CNN-based time series models considered previously for inferring Granger causality. . Towards a rigorous assessment of systems biology models: The DREAM3 challenges. PLOS ONE, 5(2):1-18, Feb 2010. . Earth system modeling 2.0: A blueprint for models that learn from observations and targeted high-resolution simulations. . Scalable matrix-valued kernel learning for high-dimensional nonlinear multivariate regression and Granger causality. In Pan. Estimating brain connectivity with varying-length time lags using a recurrent neural network.",
    "prev": "INTRODUCTION\n\nGranger causality (GC) (Granger, 1969) is a popular practical approach for the analysis of multivariate time series and has become instrumental in exploratory analysis (McCracken, 2016) in various disciplines, such as neuroscience (Roebroeck et al., 2005), economics (Appiah, 2018), and climatology (Charakopoulos et al., 2018).",
    "curr": "Recently, the focus of the methodological research has been on inferring GC under nonlinear dynamics (Tank et al., 2018;Nauta et al., 2019;Wu et al., 2020;Khanna & Tan, 2020;Löwe et al., 2020), causal structures varying across replicates (Löwe et al., 2020), and unobserved confounding (Nauta et al., 2019;Löwe et al., 2020).",
    "next": "To the best of our knowledge, the latest powerful techniques for inferring GC do not target the effect sign detection (see Section 2.1 for a formal definition) or exploration of effect variability with time and, thus, have limited interpretability.",
    "query": "Are there any studies that explore methods for inferring nonlinear Granger causality using recurrent neural networks, particularly focusing on detecting effect signs and exploring variability in causal relationships over time?"
  },
  {
    "index": 481,
    "source_corpus_id": 256846551,
    "ref_id": "b19",
    "citation_corpus_id": 67855860,
    "start": 2396,
    "end": 2418,
    "title": "Attention is not Explanation",
    "abstract": "Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful \"explanations\" for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code to reproduce all experiments is available at https://github.com/successar/ AttentionExplanation.",
    "prev": "Some recent works in the field of question-answering (QA) have demonstrated that language models can bypass some of these issues and learn to reason directly over natural language , allowing for more flexible and adaptable reasoning capabilities.",
    "curr": "Another advantage of performing multi-step reasoning over natural language is that it allows for more inspectable outputs, improving the explainability of models that are otherwise regarded as black box systems (Jain & Wallace, 2019;Rajani et al., 2019a;Danilevsky et al., 2020).",
    "next": "Despite the recent progress, we notice that there is still a gap in resources for training and evaluating general reasoning capabilities over natural language.",
    "query": "Are there any research papers on the relationship between attention mechanisms and model explainability in natural language processing tasks?"
  },
  {
    "index": 482,
    "source_corpus_id": 252683429,
    "ref_id": "b0",
    "citation_corpus_id": 245334722,
    "start": 3939,
    "end": 3958,
    "title": "TRANSFORMERS CAN DO BAYESIAN INFERENCE",
    "abstract": "Currently, it is hard to reap the benefits of deep learning for Bayesian methods, which allow the explicit specification of prior knowledge and accurately capture model uncertainty. We present Prior-Data Fitted Networks (PFNs). PFNs leverage large-scale machine learning techniques to approximate a large set of posteriors. The only requirement for PFNs to work is the ability to sample from a prior distribution over supervised learning tasks (or functions). Our method restates the objective of posterior approximation as a supervised classification problem with a set-valued input: it repeatedly draws a task (or function) from the prior, draws a set of data points and their labels from it, masks one of the labels and learns to make probabilistic predictions for it based on the set-valued input of the rest of the data points. Presented with a set of samples from a new supervised learning task as input, PFNs make probabilistic predictions for arbitrary other data points in a single forward propagation, having learned to approximate Bayesian inference. We demonstrate that PFNs can near-perfectly mimic Gaussian processes and also enable efficient Bayesian inference for intractable problems, with over 200-fold speedups in multiple setups compared to current methods. We obtain strong results in very diverse areas such as Gaussian process regression, Bayesian neural networks, classification for small tabular data sets, and few-shot image classification, demonstrating the generality of PFNs. Code and trained PFNs are released at https://github. com/automl/TransformersCanDoBayesianInference.",
    "prev": "Instead, we replace this step by performing a single forward pass with a large Transformer that has been pre-trained to solve artificially generated classification tasks from a tabular dataset prior.",
    "curr": "Our method builds on Prior-Data Fitted Networks (PFNs; Müller et al., 2022; see Section 2), which learn the training and prediction algorithm itself.",
    "next": "PFNs approximate Bayesian inference given any prior one can sample from and approximate the posterior predictive distribution (PPD) directly.",
    "query": "Are there any studies that explore the use of Transformers for Bayesian inference through Prior-Data Fitted Networks (PFNs) or similar approaches?"
  },
  {
    "index": 483,
    "source_corpus_id": 53094405,
    "ref_id": "b15",
    "citation_corpus_id": 11445252,
    "start": 8928,
    "end": 8952,
    "title": "LEARNING FEATURES OF MUSIC FROM SCRATCH",
    "abstract": "We introduce a new large-scale music dataset, MusicNet, to serve as a source of supervision and evaluation of machine learning methods for music research. Mu-sicNet consists of hundreds of freely-licensed classical music recordings by 10 composers, written for 11 instruments, together with instrument/note annotations resulting in over 1 million temporal labels on 34 hours of chamber music performances under various studio and microphone conditions. We define a multi-label classification task to predict notes in musical recordings, along with an evaluation protocol. We benchmark several machine learning architectures for this task: i) learning from \"hand-crafted\" spectrogram features; ii) end-to-end learning with a neural net; iii) end-to-end learning with a convolutional neural net. We show that several end-to-end learning proposals outperform approaches based on learning from hand-crafted audio features.",
    "prev": "Repertoire is mostly classical, including composers from the 17 th to early 20 th century.",
    "curr": "MusicNet (Thickstun et al., 2017) contains recordings of human performances, but separatelysourced scores.",
    "next": "As discussed in , the alignment between audio and score is not fully accurate.",
    "query": "Are there any studies that explore the use of the MusicNet dataset for learning musical features from scratch, particularly addressing challenges in aligning audio recordings with musical scores?"
  },
  {
    "index": 485,
    "source_corpus_id": 252693111,
    "ref_id": "b26",
    "citation_corpus_id": 6628106,
    "start": 17840,
    "end": 17858,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "We resize the training batch to 32 for image SR, and 8 for image denoising and JPEG CAR in order to make a fair comparison.",
    "curr": "We choose ADAM Kingma & Ba (2015) to optimize our ART model with β 1 = 0.9, β 2 = 0.999, and zero weight decay.",
    "next": "The initial learning rate is set as 2×10 −4 and is reduced by half as the training iteration reaches a certain number.",
    "query": "Are there any research papers on adaptive optimization methods for stochastic objectives, particularly focusing on the Adam algorithm and its hyperparameter configurations?"
  },
  {
    "index": 490,
    "source_corpus_id": 254044229,
    "ref_id": "b14",
    "citation_corpus_id": 225039882,
    "start": 3225,
    "end": 3251,
    "title": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.RELATED WORKTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.",
    "prev": "Recently, Saunshi et al.",
    "curr": "(2022) argue that the pre-training loss is not the only contributor to the performance of self-supervised learning, and that previous works which view neural networks as a black box cannot tell apart the differences in downstream performance between architectures (e.g., ResNet (He et al., 2015) vs vision transformers (Dosovitskiy et al., 2020)).",
    "next": "Furthermore, selfsupervised learning with an appropriate architecture can possibly work under more general condi-tions and/or with fewer pre-training data than predicted by these results on general architecture.",
    "query": "Are there any studies on the application of transformers in image recognition compared to convolutional networks, focusing on architecture differences and pre-training data requirements?"
  },
  {
    "index": 494,
    "source_corpus_id": 231648391,
    "ref_id": "b11",
    "citation_corpus_id": 13123084,
    "start": 8985,
    "end": 9005,
    "title": "Published as a conference paper at ICLR 2017 TEMPORAL ENSEMBLING FOR SEMI-SUPERVISED LEARNING",
    "abstract": "In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44% to 7.05% in SVHN with 500 labels and from 18.63% to 16.55% in CIFAR-10 with 4000 labels, and further to 5.12% and 12.16% by enabling the standard augmentations. We additionally obtain a clear improvement in CIFAR-100 classification accuracy by using random images from the Tiny Images dataset as unlabeled extra inputs during training. Finally, we demonstrate good tolerance to incorrect labels.",
    "prev": "Another technique is a consistency regularization which states that realistic perturbations of input examples from unlabeled dataset should not significantly change the output of a neural network.",
    "curr": "Consistency regularization is used in Π-model (Laine & Aila, 2017) and further improved by Temporal Ensembling (Laine & Aila, 2017) which maintains an exponential moving average prediction for each training example and Mean Teacher (Tarvainen & Valpola, 2017) that averages model weights instead of model predictions.",
    "next": "Recent methods UDA (Xie et al., 2019), ReMixMatch  and FixMatch ) use a combination of consistency loss, pseudo-labeling and advanced augmentation techniques in addition to color perturbations and spatial transformations.",
    "query": "Are there any research papers on semi-supervised learning methods that employ consistency regularization, ensembling techniques, and data augmentation strategies for improving model performance?"
  },
  {
    "index": 496,
    "source_corpus_id": 207878944,
    "ref_id": "b39",
    "citation_corpus_id": 8217340,
    "start": 3663,
    "end": 3682,
    "title": "OBJECT DETECTORS EMERGE IN DEEP SCENE CNNS",
    "abstract": "With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures.Here we show that object detectors emerge from training CNNs to perform scene classification. As scenes are composed of objects, the CNN for scene classification automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having explicitly learned the notion of objects.",
    "prev": null,
    "curr": "tle differences between interpretability and explanation (Turner, 2016).While the former mainly focuses on building or approximating models that are locally or globally interpretable (Ribeiro et al., 2016), the latter aims at explaining a predictor aposteriori.The explanation approach does not compromise the prediction performance.However, a rigorous definition for what is a good explanation is elusive.Some researchers focused on providing feature importance (e.g., in the form of a heatmap (Selvaraju et al., 2017)) that influence the outcome of the predictor.In some applications (e.g., diagnosis with medical images) the causal changes are spread out across a large number of features (i.e., large portions of the image are impacted by a disease).Therefore, a heatmap may not be informative or useful, as almost all image features are highlighted.Furthermore, those methods do not explain why a predictor returns an outcome.Others have introduced local occlusion or perturbations to the input (Zhou et al., 2014;Fong & Vedaldi, 2017) by assessing which manipulations have the largest impact on the predictors.There is also recent interest in generating counterfactual inputs that would change the black box classification decision with respect to the query inputs (Goyal et al., 2019;Liu et al., 2019).Local perturbations of a query are not guaranteed to generate realistic or plausible inputs, which diminishes the usefulness of the explanation, especially for end users (e.g., physicians).We argue that the explanation should depend not only on the predictor function but also on the data.Therefore, it is reasonable to train a model that learns from data as well as the black-box classifier (e.g., (Chang et al., 2019;Dabkowski & Gal, 2017;Fong & Vedaldi, 2017)).",
    "next": "Our proposed method falls into the local explanation paradigm.Our approach is model agnostic and only requires access to the predictor values and its gradient with respect to the input.Given a query input to a black-box, we aim at explaining th",
    "query": "Are there any research papers on explanation methods for object detectors in deep scene convolutional neural networks?"
  },
  {
    "index": 497,
    "source_corpus_id": 252683376,
    "ref_id": "b0",
    "citation_corpus_id": 49310753,
    "start": 24596,
    "end": 24621,
    "title": "Published as a conference paper at ICLR 2018 MAXIMUM A POSTERIORI POLICY OPTIMISATION",
    "abstract": "We introduce a new algorithm for reinforcement learning called Maximum aposteriori Policy Optimisation (MPO) based on coordinate ascent on a relativeentropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings. , et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.",
    "prev": "(Ahmed et al., 2018), and stronger robustness (Eysenbach & Levine, 2022).",
    "curr": "Generally, policies optimizing the MaxEnt RL objective sample actions that are proportional to the exponentiated reward, and alternatively can be viewed as a noise injection procedure for better exploration (Attias, 2003;Ziebart, 2010;Haarnoja et al., 2017;Nachum et al., 2017;Levine, 2018;Abdolmaleki et al., 2018;Haarnoja et al., 2018b;Vieillard et al., 2020b;Pan et al., 2022;2023;Lahlou et al., 2023).",
    "next": "However, this noise injection is commonly done directly in action space, leading to only local perturbations, whereas we inject noise through a nonlinear mapping.",
    "query": "Are there any research papers on reinforcement learning algorithms that use maximum entropy objectives with alternative noise injection methods for exploration?"
  },
  {
    "index": 498,
    "source_corpus_id": 15816492,
    "ref_id": "b21",
    "citation_corpus_id": 12639289,
    "start": 1364,
    "end": 1394,
    "title": "Recurrent Continuous Translation Models",
    "abstract": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.",
    "prev": "Experimental results on abstractive sentence summarisation, morphological inflection, and machine translation show that noisy channel models outperform direct models, and that they significantly benefit from increased amounts of unpaired output data that direct models cannot easily use.",
    "curr": "INTRODUCTION\n\nRecurrent neural network sequence to sequence models (Kalchbrenner & Blunsom, 2013;Sutskever et al., 2014;Bahdanau et al., 2015) are excellent models of p(output sequence y | input sequence x), provided sufficient input-output (x, y) pairs are available for estimating their parameters.",
    "next": "However, in many domains, vastly more unpaired output examples are available than input-output pairs (e.g., transcribed speech is relatively rare although non-spoken texts are abundant; Swahili-English translations are rare although English texts are abundant; etc.).",
    "query": "Are there any studies that explore translation models based on continuous representations without alignments, which utilize unpaired output data to improve performance in machine translation?"
  },
  {
    "index": 499,
    "source_corpus_id": 265038424,
    "ref_id": "b14",
    "citation_corpus_id": 3366315,
    "start": 2403,
    "end": 2423,
    "title": "Published as a conference paper at ICLR 2018 SPECTRAL NORMALIZATION FOR GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. The code with Chainer (Tokui et al., 2015), generated images and pretrained models are available at https://github.com/pfnet-research/sngan_ projection.Published as a conference paper at ICLR 2018• Lipschitz constant is the only hyper-parameter to be tuned, and the algorithm does not require intensive tuning of the only hyper-parameter for satisfactory performance. • Implementation is simple and the additional computational cost is small.",
    "prev": "The original GAN is, however, highly unstable and often suffers from mode collapse.",
    "curr": "Much of recent researches has focused on improving the stability of GANs (Radford et al., 2015;Heusel et al., 2017;Miyato et al., 2018;Karras et al., 2018).",
    "next": "On the theoretical aspect, Nagarajan & Kolter (2017) proved that gradient based training of the original GAN is locally stable.",
    "query": "Are there any research papers on methods for stabilizing generative adversarial networks, particularly those using spectral normalization techniques?"
  },
  {
    "index": 500,
    "source_corpus_id": 220525352,
    "ref_id": "b5",
    "citation_corpus_id": 28202810,
    "start": 2288,
    "end": 2290,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "The success of various off-policy RL algorithms largely attributes to the use of experience replay [5-7, 15, 16].",
    "curr": "However, most off-policy RL algorithms usually adopt a random sampling [5,6,16], which treats all past experiences equally.",
    "next": "It is questionable whether this simple strategy would always choose the most effective samples for training.",
    "query": "Are there any studies that investigate alternative sampling strategies in off-policy reinforcement learning algorithms using experience replay to improve training efficiency and stability?"
  },
  {
    "index": 502,
    "source_corpus_id": 261214750,
    "ref_id": "b2",
    "citation_corpus_id": 215416146,
    "start": 2521,
    "end": 2524,
    "title": "S2ORC: The Semantic Scholar Open Research Corpus",
    "abstract": "We introduce S2ORC, 1 a large corpus of 81.1M English-language academic papers spanning many academic disciplines. The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text is annotated with automaticallydetected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. In S2ORC, we aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date. We hope this resource will facilitate research and development of tools and tasks for text mining over academic text. * denotes equal contribution 1 Instructions for access to the data and model are available at https://github.com/allenai/s2orc/.2 https://arxiv.org Figure 1: Inline citations and references to figures and tables are annotated in S2ORC's structured full text. Citations are linked to bibliography entries, which are linked to other papers in S2ORC. Figure and table references are linked to their captions.3 https://www.ncbi.nlm.nih.gov/pmc 4 https://citeseerx.ist.psu.edu 5 https://www.aclweb.org/anthology 6 http://aan.how/",
    "prev": "The information of millions of academic papers can not be fully accessed because they are locked behind an unreadable format.",
    "curr": "Existing corpora, such as the S2ORC dataset [3], capture the text of 12M 2 papers using GROBID [4], but are missing meaningful representations of the mathematical equations.",
    "next": "To this end, we introduce Nougat, a transformer based model that can convert images of document pages to formatted markup text.",
    "query": "Are there any research papers on tools or resources for extracting structured text and mathematical equations from academic document images?"
  },
  {
    "index": 503,
    "source_corpus_id": 247594694,
    "ref_id": "b6",
    "citation_corpus_id": 222272028,
    "start": 29905,
    "end": 29928,
    "title": "LINEAR MODE CONNECTIVITY IN MULTITASK AND CONTINUAL LEARNING",
    "abstract": "Continual (sequential) training and multitask (simultaneous) training are often attempting to solve the same overall objective: to find a solution that performs well on all considered tasks. The main difference is in the training regimes, where continual learning can only have access to one task at a time, which for neural networks typically leads to catastrophic forgetting. That is, the solution found for a subsequent task does not perform well on the previous ones anymore. However, the relationship between the different minima that the two training regimes arrive at is not well understood. What sets them apart? Is there a local structure that could explain the difference in performance achieved by the two different schemes? Motivated by recent work showing that different minima of the same task are typically connected by very simple curves of low error, we investigate whether multitask and continual solutions are similarly connected. We empirically find that indeed such connectivity can be reliably achieved and, more interestingly, it can be done by a linear path, conditioned on having the same initialization for both. We thoroughly analyze this observation and discuss its significance for the continual learning process. Furthermore, we exploit this finding to propose an effective algorithm that constrains the sequentially learned minima to behave as the multitask solution. We show that our method outperforms several state of the art continual learning algorithms on various vision benchmarks 1 . * Equal contribution 1 The code is available at: https://github.com/imirzadeh/MC-SGD arXiv:2010.04495v1 [cs.LG]",
    "prev": "More implementation details are in Appendix C.4.",
    "curr": "Moreover, we also conduct the visualization of relative loss change g over a linear path like (Mirzadeh et al., 2020) in Appendix C.4.",
    "next": "The loss change of SSL is slower than that of SL along the linear interpolation path, demonstrating the flatter minima of SSL.",
    "query": "Are there any research papers on linear mode connectivity in multitask and continual learning, focusing on loss dynamics analysis and comparisons between self-supervised and supervised learning approaches?"
  },
  {
    "index": 505,
    "source_corpus_id": 256105083,
    "ref_id": "b23",
    "citation_corpus_id": 218487034,
    "start": 1176,
    "end": 1197,
    "title": "On Faithfulness and Factuality in Abstractive Summarization",
    "abstract": "It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.",
    "prev": "Introduction\n\nLarge language models, often trained with billions of parameters, have achieved impressive performance in recent years (Raffel et al., 2019) and are used in a wide variety of natural language generation tasks.",
    "curr": "However, their output is sometimes undesirable, with hallucinated content (Maynez et al., 2020;Filippova, 2020), and much work remains to fully understand their properties.",
    "next": "In many applications, such as healthcare, question-answering systems, or customer service, incorrect predictions are particularly costly and must be avoided.",
    "query": "Are there any research papers on methods to improve faithfulness and factuality in abstractive summarization using large language models, particularly in addressing hallucinations and evaluating with textual entailment measures?"
  },
  {
    "index": 506,
    "source_corpus_id": 260316137,
    "ref_id": "b16",
    "citation_corpus_id": 53112107,
    "start": 11078,
    "end": 11099,
    "title": "QUASI-HYPERBOLIC MOMENTUM AND ADAM FOR DEEP LEARNING",
    "abstract": "Momentum-based acceleration of stochastic gradient descent (SGD) is widely used in deep learning. We propose the quasi-hyperbolic momentum algorithm (QHM) as an extremely simple alteration of momentum SGD, averaging a plain SGD step with a momentum step. We describe numerous connections to and identities with other algorithms, and we characterize the set of two-state optimization algorithms that QHM can recover. Finally, we propose a QH variant of Adam called QHAdam, and we empirically demonstrate that our algorithms lead to significantly improved training in a variety of settings, including a new state-of-theart result on WMT16 EN-DE. We hope that these empirical results, combined with the conceptual and practical simplicity of QHM and QHAdam, will spur interest from both practitioners and researchers. PyTorch code is immediately available. 1 . We also offer evidence that momentum often yields negligible improvement over plain SGD.We emphasize QHM and QHAdam's efficiency and conceptual simplicity. QHM has no extra overhead vs. Nesterov's accelerated gradient, and QHAdam has very little overhead vs. Adam. Also, both algorithms are easily understood as an interpolation between two other well-known algorithms, so they are accessible to practitioners and can be tuned starting with existing practical intuitions. We believe that this contributes strongly to the algorithms' practical promise.PRELIMINARIESWe begin with notation and a brief review of stochastic gradient descent (SGD) and momentum.Primitives In this paper, θ ∈ R p denotes a vector of model parameters. L(θ) : R p → R denotes a loss function to be minimized via θ.L(θ) : R p → R denotes an approximator of the loss function (e.g. over a minibatch). ∇L denotes the gradient of function L. Unless otherwise specified, all vector operations are element-wise. We use g, a, s, v, w ∈ R p as auxiliary buffers, and g is typically the \"momentum buffer\". θ,L(·), and all buffers are subscriptable by t, the optimization step.Optimization algorithms We consider optimization algorithms that perform a sequence of steps (indexed by t), updating θ at each step towards minimizing L(θ). For brevity, we write algorithms as \"update rules\", which describe the algorithm's behavior during a single step t, rather than as full pseudocode. Update rules take this basic form (optionally with one or more auxiliary steps):Plain SGD The SGD algorithm, parameterized by learning rate α ∈ R, uses the update rule:Momentum The momentum algorithm, parameterized by α ∈ R and β ∈ R, uses the update rule:",
    "prev": "They also argued that momentum helps generalization from the lens of stability analysis for finite number of iterates, when the loss function is Lipschitz.",
    "curr": "Under the formulation of quasi-hyperbolic momentum [Ma and Yarats, 2019], Gitman et al.",
    "next": "[2019] proposed another unified analysis for momentum methods, in terms of the asymptotic and local convergence as well as the stability.",
    "query": "Are there any research papers on quasi-hyperbolic momentum and Adam variants for deep learning, focusing on their convergence analysis, stability, and empirical performance improvements?"
  },
  {
    "index": 507,
    "source_corpus_id": 102481167,
    "ref_id": "b3",
    "citation_corpus_id": 1844940,
    "start": 1790,
    "end": 1807,
    "title": "MAKING NEURAL PROGRAMMING ARCHITECTURES GENERALIZE VIA RECURSION",
    "abstract": "Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system's behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion.Published as a conference paper at ICLR 2017 reduce the complexity of programs. We find that recursion makes it easier for the network to learn the right program and generalize to unknown situations. Recursion enables provable guarantees on neural programs' behavior without needing to exhaustively enumerate all possible inputs to the programs. This paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs. As an application, we incorporate recursion into the Neural Programmer-Interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort. Empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results. Given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly. This is the first time one can provide provable guarantees of perfect generalization for neural programs.",
    "prev": "INTRODUCTION\n\nAdvances in machine learning and the availability of large corpora of source code have led to growing interest in the development of neural representations of programs for performing program analyses.",
    "curr": "In particular, different representations based on token sequences (Gupta et al., 2017;Bhatia et al., 2018), program parse trees (Piech et al., 2015;Mou et al., 2016), program traces (Reed & de Freitas, 2015;Cai et al., 2017;Wang et al., 2018), and graphs (Allamanis et al., 2018) have been proposed for a variety of tasks including repair (Devlin et al., 2017b;Allamanis et al., 2018), optimization (Bunel et al., 2017), and synthesis (Parisotto et al., 2017;Devlin et al., 2017a).",
    "next": "In recent work, Allamanis et al.",
    "query": "Are there any research papers on neural programming architectures that use recursion to improve generalizability and provide provable guarantees for program tasks?"
  },
  {
    "index": 510,
    "source_corpus_id": 202750253,
    "ref_id": "b28",
    "citation_corpus_id": 53015027,
    "start": 7048,
    "end": 7051,
    "title": "ADAPTIVITY OF DEEP RELU NETWORK FOR LEARN- ING IN BESOV AND MIXED SMOOTH BESOV SPACES: OPTIMAL RATE AND CURSE OF DIMENSIONALITY",
    "abstract": "Deep learning has shown high performances in various types of tasks from visual recognition to natural language processing, which indicates superior flexibility and adaptivity of deep learning. To understand this phenomenon theoretically, we develop a new approximation and estimation error analysis of deep learning with the ReLU activation for functions in a Besov space and its variant with mixed smoothness. The Besov space is a considerably general function space including the Hölder space and Sobolev space, and especially can capture spatial inhomogeneity of smoothness. Through the analysis in the Besov space, it is shown that deep learning can achieve the minimax optimal rate and outperform any nonadaptive (linear) estimator such as kernel ridge regression, which shows that deep learning has higher adaptivity to the spatial inhomogeneity of the target function than other estimators such as linear ones. In addition to this, it is shown that deep learning can avoid the curse of dimensionality if the target function is in a mixed smooth Besov space. We also show that the dependency of the convergence rate on the dimensionality is tight due to its minimax optimality. These results support high adaptivity of deep learning and its superior ability as a feature extractor.",
    "prev": "Due to its significant importance in wide range of application areas, its theoretical analysis is also getting much important.",
    "curr": "For example, it has been known that the deep neural network has universal approximation capability [13,24,44] and its expressive power grows up in an exponential order against the number of layers [37,8,12,11,42,47].",
    "next": "However, theoretical understandings are still lacking in several important issues.",
    "query": "Are there any research papers on the theoretical analysis of deep ReLU networks in Besov spaces and their adaptivity in achieving optimal approximation rates and overcoming the curse of dimensionality?"
  },
  {
    "index": 512,
    "source_corpus_id": 247613305,
    "ref_id": "b52",
    "citation_corpus_id": 216562627,
    "start": 7606,
    "end": 7626,
    "title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels",
    "abstract": "We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC)[20], are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based[21,31,22]methods and recently proposed contrastive learning[42]. Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at https://sites.google.com/view/data-regularized-q.IntroductionSample-efficient deep reinforcement learning (RL) algorithms capable of directly training from image pixels would open up many real-world applications in control and robotics. However, simultaneously training a convolutional encoder alongside a policy network is challenging when given limited environment interaction, strong correlation between samples and a typically sparse reward signal. Naive attempts to use a large capacity encoder result in severe over-fitting (seeFigure 1a) and smaller encoders produce impoverished representations that limit task performance.Limited supervision is a common problem across AI and a number of approaches are adopted: (i) pretraining with self-supervised learning (SSL), followed by standard supervised learning; (ii) supervised learning with an additional auxiliary loss and (iii) supervised learning with data augmentation. SSL approaches are highly effective in the large data regime, e.g. in domains such as vision [7, 23] and NLP[12,13]where large (unlabeled) datasets are readily available. However, in RL this is not the case: an off-policy RL agent is trained on a replay buffer that grows as the agent interacts with the environment. But in a sample efficient regime the buffer may only hold 10 4 -10 5 transitions from a few hundred trajectories, limiting the effectiveness of SSL methods.A wide range of auxiliary loss functions have been proposed to augment supervised objectives, e.g. weight regularization, noise injection [25], or some form of online SSL objective. In RL, * Equal contribution. Author ordering determined by coin flip. Both authors are corresponding.Preprint. Under review.",
    "prev": "Data augmentation for RL.",
    "curr": "In the context of RL, data augmentation has been widely investigated for improving data-efficiency Yarats et al., 2021), or RL generalization (Cobbe et al., 2019;Lee et al., 2019).",
    "next": "For example, RAD  demonstrated that data augmentation, such as random crop, can improve both data-efficiency and generalization of RL algorithms.",
    "query": "Are there any research papers on the application of image augmentation techniques for improving data efficiency and generalization in deep reinforcement learning algorithms?"
  },
  {
    "index": 515,
    "source_corpus_id": 231879989,
    "ref_id": "b40",
    "citation_corpus_id": 231933751,
    "start": 8827,
    "end": 8844,
    "title": "VA-RED 2 : VIDEO ADAPTIVE REDUNDANCY REDUC- TION",
    "abstract": "Performing inference on deep learning models for videos remains a challenge due to the large amount of computational resources required to achieve robust recognition. An inherent property of real-world videos is the high correlation of information across frames which can translate into redundancy in either temporal or spatial feature maps of the models, or both. The type of redundant features depends on the dynamics and type of events in the video: static videos have more temporal redundancy while videos focusing on objects tend to have more channel redundancy. Here we present a redundancy reduction framework, termed VA-RED 2 , which is input-dependent. Specifically, our VA-RED 2 framework uses an input-dependent policy to decide how many features need to be computed for temporal and channel dimensions. To keep the capacity of the original model, after fully computing the necessary features, we reconstruct the remaining redundant features from those using cheap linear operations. We learn the adaptive policy jointly with the network weights in a differentiable way with a shared-weight mechanism, making it highly efficient. Extensive experiments on multiple video datasets and different visual tasks show that our framework achieves 20% − 40% reduction in computation (FLOPs) when compared to state-of-the-art methods without any performance loss.",
    "prev": "Conditional Computation.",
    "curr": "Many conditional computation methods have been recently proposed with the goal of improving computational efficiency (Bengio et al., 2015;Veit & Belongie, 2018;Wang et al., 2018b;Graves, 2016;Meng et al., 2020;Pan et al., 2021).",
    "next": "Several works have been Figure 1: A conceptual view for adaptive temporal fusion.",
    "query": "Are there any studies that propose adaptive frameworks for reducing temporal and channel redundancy in video processing models through input-dependent computational policies?"
  },
  {
    "index": 519,
    "source_corpus_id": 214222435,
    "ref_id": "b4",
    "citation_corpus_id": 29153681,
    "start": 23607,
    "end": 23631,
    "title": "META-LEARNING WITH DIFFERENTIABLE CLOSED-FORM SOLVERS",
    "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.Most work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent. Nonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently. In this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning. The main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data. This requires back-propagating errors through the solver steps. While normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage. We propose both closed-form and iterative solvers, based on ridge regression and logistic regression components. Our methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on meta-learning approaches that use differentiable closed-form solvers, such as ridge or logistic regression, for few-shot learning tasks?"
  },
  {
    "index": 520,
    "source_corpus_id": 245502568,
    "ref_id": "b22",
    "citation_corpus_id": 231807280,
    "start": 3252,
    "end": 3259,
    "title": "UNDERSTANDING THE ROLE OF IMPORTANCE WEIGHT- ING FOR DEEP LEARNING",
    "abstract": "The recent paper byByrd & Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.",
    "prev": "In particular, Byrd and Lipton [BL19] empirically showed that when no regularization is used, overparameterized linear and nonlinear models trained with the importance weighted cross-entropy loss ignore the importance weights.",
    "curr": "Xu, Ye, and Ruan [XYR20] followed up and provided a theoretical justification for this observation in overparameterized linear and non-linear models.",
    "next": "To build intuition about why importance weighting fails, consider linear classifiers as an example.",
    "query": "Are there any research papers on the theoretical analysis of importance weighting in overparameterized deep learning models, focusing on optimization dynamics and generalization performance?"
  },
  {
    "index": 522,
    "source_corpus_id": 249431648,
    "ref_id": "b61",
    "citation_corpus_id": 6212000,
    "start": 2505,
    "end": 2509,
    "title": "UNDERSTANDING DEEP LEARNING REQUIRES RE- THINKING GENERALIZATION",
    "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.",
    "prev": "While significant efforts have been devoted in understanding and correcting biases in classical models such as logistic regressions and supported vector machines (SVM), see, e.g., [1,29], those derived tools are far less effective on modern over-parameterized models such as neural networks (NN).",
    "curr": "Over-parameterization can lead to poor generalization, as extensive efforts in both theoretical and empirical studies have exhibited ( [62,50,4]).",
    "next": "Furthermore, in large models, it is also difficult for measures of fairness (such as equalized odds to be introduced shortly) to generalize, as shown in Fig.",
    "query": "Are there any studies that investigate the generalization of deep learning models in over-parameterized neural networks and the limitations of traditional regularization techniques in explaining this phenomenon?"
  },
  {
    "index": 523,
    "source_corpus_id": 1463401,
    "ref_id": "b5",
    "citation_corpus_id": 5590763,
    "start": 2354,
    "end": 2371,
    "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "prev": "The remarkable recent successes of the deep convolutional neural networks are particularly based on this ability to learn hierarchical representation for spatial data (Krizhevsky et al., 2012).",
    "curr": "For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances (Mikolov et al., 2010;Graves, 2013;Cho et al., 2014;Vinyals et al., 2015).",
    "next": "However, unlike the spatial data, learning both hierarchical and temporal representation has been among the long-standing challenges of RNNs in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data (Schmidhuber, 1991;Mozer, 1993;El Hihi & Bengio, 1995;Lin et al., 1996;Koutník et al., 2014).",
    "query": "Are there any studies that explore methods for integrating hierarchical representations with recurrent neural networks to model temporal data in tasks like machine translation?"
  },
  {
    "index": 524,
    "source_corpus_id": 3535069,
    "ref_id": "b3",
    "citation_corpus_id": 19435386,
    "start": 3234,
    "end": 3236,
    "title": "A Corpus of Natural Language for Visual Reasoning",
    "abstract": "We present a new visual reasoning language dataset, containing 92,244 pairs of examples of natural statements grounded in synthetic images with 3,962 unique sentences. We describe a method of crowdsourcing linguistically-diverse data, and present an analysis of our data. The data demonstrates a broad set of linguistic phenomena, requiring visual and set-theoretic reasoning. We experiment with various models, and show the data presents a strong challenge for future research.",
    "prev": "Machine understanding of this structured visual information could assist human analysts in extracting knowledge from the vast documentation produced by modern science.",
    "curr": "Thus motivated, and inspired by recent research in Visual Question Answering (VQA) [1,2] and relational reasoning [3,4], we introduce FigureQA.",
    "next": "FigureQA is a corpus of over one million question-answer pairs grounded in over 100, 000 figures, devised to study aspects of comprehension and reasoning in machines.",
    "query": "Are there any studies that explore the creation of natural language corpora for visual reasoning, leveraging synthetic images and crowdsourcing methods to analyze linguistic phenomena requiring visual and set-theoretic comprehension?"
  },
  {
    "index": 525,
    "source_corpus_id": 3703428,
    "ref_id": "b22",
    "citation_corpus_id": 6628106,
    "start": 15185,
    "end": 15203,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "We even show that the use of particular representation learning algorithms such as VAEs in the IMGEP-UGL architecture can produce exploration dynamics that match the one using engineered representations.",
    "curr": "Secondary contributions of this article:\n\n• We show that the IMGEP-UGL architecture can be successfully implemented (in terms of exploration efficiency) using various unsupervised learning algorithms for the goal space learning component: AutoEncoders (AEs) (Bourlard & Kamp, 1988), Variational AE (VAE) (Rezende et al., 2014;Kingma & Ba, 2015), VAE with Normalizing Flow (Rezende & Mohamed, 2015), Isomap (Tenenbaum et al., 2000), PCA (Pearson, 1901), and we quantitatively compare their performances in terms of exploration dynamics of the associated IMGEP-UGL architecture.",
    "next": "• We show that specifying more embedding dimensions than needed to capture the phenomenon manifold does not deteriorate the performance of these unsupervised learning algorithms.",
    "query": "Are there any studies that explore the application of different unsupervised learning algorithms in the IMGEP-UGL architecture for exploration dynamics and their performance comparison?"
  },
  {
    "index": 526,
    "source_corpus_id": 257255036,
    "ref_id": "b51",
    "citation_corpus_id": 227209335,
    "start": 2032,
    "end": 2052,
    "title": "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024ˆ1024 images for the first time from a score-based generative model.",
    "prev": "INTRODUCTION\n\nDiffusion probabilistic modeling has quickly become a central approach for learning data distributions, obtaining impressive empirical results across multiple domains like images (Nichol & Dhariwal, 2021), videos (Ho et al., 2022) or even 3D geometry (Luo & Hu, 2021).",
    "curr": "In particular, Denoising Diffusion Probabilistic Models (often referred to as DDPMs or diffusion generative models) (Ho et al., 2020;Nichol & Dhariwal, 2021) and their continuous-time extension (Song et al., 2021b) both present a training objective that is more stable than precursors like generative adversarial nets (GANs) (Goodfellow et al., 2014) or energy-based models (EBMs) (Du et al., 2020).",
    "next": "In addition, diffusion generative models have shown to empirically outperform GANs in the image domain (Dhariwal & Nichol, 2021) and to suffer less from mode-seeking pathologies during training (Kodali et al., 2017).",
    "query": "Are there any research papers on score-based generative modeling using stochastic differential equations for image generation, including methods like predictor-corrector frameworks and neural ODEs?"
  },
  {
    "index": 527,
    "source_corpus_id": 7902640,
    "ref_id": "b21",
    "citation_corpus_id": 16299141,
    "start": 15676,
    "end": 15698,
    "title": "Pointer Sentinel Mixture Models",
    "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus. 1",
    "prev": "For machine translation, we use the extended DSL and construct candidate architectures incrementally using the RL generator without a ranking function.",
    "curr": "LANGUAGE MODELING USING RANDOM SEARCH WITH A RANKING FUNCTION\n\nFor evaluating architectures found during architecture search, we use the WikiText-2 dataset (Merity et al., 2017b).",
    "next": "When evaluating a proposed novel RNN cell c, we construct a two layer c-RNN with a 200 unit hidden size.",
    "query": "Are there any studies that explore language modeling using random search with a ranking function for architecture evaluation?"
  },
  {
    "index": 528,
    "source_corpus_id": 263909107,
    "ref_id": "b66",
    "citation_corpus_id": 3693334,
    "start": 1863,
    "end": 1880,
    "title": "Visualizing the Loss Landscape of Neural Nets",
    "abstract": "Neural network training relies on our ability to find \"good\" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and wellchosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple \"filter normalization\" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.",
    "prev": " analyze the connection between posterior quality and uncertainty quantification, delve into the impact of modes on the posterior, and explore methods for visualizing the posterior.Moreover, we uncover weight-space symmetries as a critical aspect for understanding the posterior.To this extent, we develop an in-depth assessment of the impact of both permutation and scaling symmetries that tend to obfuscate the Bayesian posterior.While the first type of transformation is known for duplicating modes, we explore the relationship between the latter and L2 regularization, challenging previous misconceptions.Finally, to help the community improve our understanding of the Bayesian posterior, we will release shortly the first large-scale checkpoint dataset, including thousands of real-world models, along with our codes.",
    "curr": "INTRODUCTION\n\nDespite substantial advancements in deep learning, Deep Neural Networks (DNNs) remain black box models.Various studies have sought to explore DNN loss landscapes (Li et al., 2018;Fort & Jastrzebski, 2019;Fort & Scherlis, 2019;Liu et al., 2022) to achieve a deeper understanding of these models.Recent works have, for instance, unveiled the interconnection of the modes obtained with Stochastic Gradient Descent (SGD) via narrow pathways that link pairs of modes, or through tunnels that connect multiple modes simultaneously (Garipov et al., 2018;Draxler et al., 2018).This mode connectivity primarily arises from scale and permutation invariances, which imply that numerous weights can represent the same exact function (e.g., Entezari et al.",
    "next": "(2022)).Several studies have delved into the relationship between these symmetries and the characteristics of the loss landscape (Entezari et al., 2022;Neyshabur et al., 2015;Brea et al., 2019).Our work investigates the connections between these symmetries and the distribution of DNN weights, a crucial aspect for uncertainty quantification.As shown in Figure 1, it is apparent that these symmetries also exert influence on DNN poster",
    "query": "Are there any research papers on methods for visualizing the loss landscapes of neural networks and analyzing the impact of network architecture and training parameters on these landscapes?"
  },
  {
    "index": 531,
    "source_corpus_id": 209316097,
    "ref_id": "b1",
    "citation_corpus_id": 7771402,
    "start": 2049,
    "end": 2073,
    "title": "Abstract Meaning Representation for Sembanking",
    "abstract": "We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are writing down the meanings of thousands of English sentences. We hope that a sembank of simple, whole-sentence semantic structures will spur new work in statistical natural language understanding and generation, like the Penn Treebank encouraged work on statistical parsing. This paper gives an overview of AMR and tools associated with it.",
    "prev": "So far, most of the existing methods focus on building statistical associations between textual inputs and semantic representations, e.g.",
    "curr": "using first-order logic (Manning et al., 1999) or other types of representations such as abstract meaning representation (Banarescu et al., 2013).",
    "next": "Recently, grounded language learning has gradually attracted attention in various domains, inspired by the hypothesis that early language learning was focused on problemsolving (Kirby & Hurford, 2002).",
    "query": "Are there any research papers on the use of Abstract Meaning Representation (AMR) for semantic banking and its applications in natural language understanding and generation?"
  },
  {
    "index": 532,
    "source_corpus_id": 222133372,
    "ref_id": "b50",
    "citation_corpus_id": 3366315,
    "start": 16630,
    "end": 16634,
    "title": "Published as a conference paper at ICLR 2018 SPECTRAL NORMALIZATION FOR GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. The code with Chainer (Tokui et al., 2015), generated images and pretrained models are available at https://github.com/pfnet-research/sngan_ projection.Published as a conference paper at ICLR 2018• Lipschitz constant is the only hyper-parameter to be tuned, and the algorithm does not require intensive tuning of the only hyper-parameter for satisfactory performance. • Implementation is simple and the additional computational cost is small.",
    "prev": "Compared with the conditions stated in Section 2 (namely, that it is locally homogeneous with a symmetric Jacobian), our requirement on the denoiser is milder.",
    "curr": "One can train a nonexpansive D σ by constraining the Lipschitz constant of D σ via the spectral normalization, which is an active area of research in deep learning [51][52][53].",
    "next": "We can now state the theorems on ASYNC-RED.",
    "query": "Are there any research papers on spectral normalization techniques for stabilizing generative adversarial networks during training?"
  },
  {
    "index": 534,
    "source_corpus_id": 240419913,
    "ref_id": "b47",
    "citation_corpus_id": 56475856,
    "start": 2418,
    "end": 2421,
    "title": "LEARNING TO ADAPT IN DYNAMIC, REAL-WORLD ENVIRONMENTS THROUGH META-REINFORCEMENT LEARNING",
    "abstract": "Although reinforcement learning methods can achieve impressive results in simulation, the real world presents two major challenges: generating samples is exceedingly expensive, and unexpected perturbations or unseen situations cause proficient but specialized policies to fail at test time. Given that it is impractical to train separate policies to accommodate all situations the agent may see in the real world, this work proposes to learn how to quickly and effectively adapt online to new tasks. To enable sample-efficient learning, we consider learning online adaptation in the context of model-based reinforcement learning. Our approach uses meta-learning to train a dynamics model prior such that, when combined with recent data, this prior can be rapidly adapted to the local context. Our experiments demonstrate online adaptation for continuous control tasks on both simulated and real-world agents. We first show simulated agents adapting their behavior online to novel terrains, crippled body parts, and highly-dynamic environments. We also illustrate the importance of incorporating online adaptation into autonomous agents that operate in the real world by applying our method to a real dynamic legged millirobot. We demonstrate the agent's learned ability to quickly adapt online to a missing leg, adjust to novel terrains and slopes, account for miscalibration or errors in pose estimation, and compensate for pulling payloads. 2 1 * Equal Contribution 2 Videos available at: https://sites.",
    "prev": "While plenty of other work in model-based RL does measure generalization [e.g.",
    "curr": "18,48,70,76], each approach is typically evaluated on a bespoke task, making it difficult to ascertain the state of generalization in model-based RL more broadly.",
    "next": "Model-free RL, like model-based RL, has also suffered from both the \"train=test\" paradigm and a lack of standardization around how to measure generalization.",
    "query": "Are there any research papers on methods for improving generalization in model-based reinforcement learning through online adaptation techniques in dynamic real-world environments?"
  },
  {
    "index": 536,
    "source_corpus_id": 262825568,
    "ref_id": "b48",
    "citation_corpus_id": 252715691,
    "start": 1769,
    "end": 1787,
    "title": "GLM-130B: AN OPEN BILINGUAL PRE-TRAINED MODEL",
    "abstract": "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and disconvergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B-the largest Chinese language model-across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization, without quantization aware training and with almost no performance loss, making it the first among 100B-scale models. More importantly, the property allows its effective inference on 4×RTX 3090 (24G) or 8×RTX 2080 Ti (11G) GPUs, the most ever affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at https://github.comIn this work, we introduce the pre-training of a 100B-scale model-GLM-130B, in terms of engineering efforts, model design choices, training strategies for efficiency and stability, and quantization for affordable inference. As it has been widely realized that it is computationally unaffordable to empirically enumerate all possible designs for training 100B-scale LLMs, we present not only the successful part for training GLM-130B but also many of the failed options and lessons learned. Particularly, the training stability is the decisive factor in the success of training models of such a scale. Different from practices such as manually adjusting learning rates in OPT-175B and using embedding norm in the sacrifice of performance in BLOOM-176B, we experiment with various options and find the strategy of embedding gradient shrink can significantly stabilize the training of GLM-130B.Specifically, GLM-130B is a bilingual (English and Chinese) bidirectional dense model with 130 billion parameters, pre-trained over 400 billion tokens on a cluster of 96 NVIDIA DGX-A100 (8×40G) GPU nodes between May 6 and July 3, 2022. Instead of using the GPT-style architecture, we adopt the General Language Model (GLM) algorithm (Du et al., 2022)  to leverage its bidirectional attention advantage and autoregressive blank infilling objective.Table 1summarizes the comparison between GLM-130B, GPT-3 and another two open-source efforts-OPT-175B and BLOOM-176B, as well as PaLM 540B (Chowdhery et al., 2022)-a 4× larger model-as a reference.Altogether, the conceptual uniqueness and engineering efforts enable GLM-130B to exhibit performance that surpasses the level of GPT-3 on a wide range of benchmarks (in total 112 tasks) and also outperforms PaLM 540B in many cases, while outperformance over GPT-3 has not been observed in OPT-175B and BLOOM-176B (Cf.Figure 1 (a)). For zero-shot performance, GLM-130B is better than GPT-3 175B (+5.0%), OPT-175B (+6.5%), and BLOOM-176B (+13.0%) on LAMBADA (Paperno et al., 2016), and achieves 3× better performance than GPT-3 on Big-bench-lite (Srivastava  et al., 2022). For the 5-shot MMLU (Hendrycks et al., 2021) tasks, it is better than GPT-3 175B (+0.9%) and BLOOM-176B (+12.7%). As a bilingual LLM also in Chinese, it offers significantly better results than ERNIE TITAN 3.0 260B (Wang et al., 2021)-the largest Chinese LLM-on 7 zero-shot CLUE (Xu et al., 2020) datasets (+24.26%) and 5 zero-shot FewCLUE  ones (+12.75%). Importantly, as summarized inFigure 1 (b), GLM-130B as an open model is associated with significantly less bias and generation toxicity than its 100B-scale counterparts.Finally, we design GLM-130B to empower as many people as possible to conduct 100B-scale LLM studies. First, instead of using 175B+ parameters as OPT and BLOOM, the 130B size is decided because such a size supports inference on a single A100 (8×40G) server. Second, to further lower the GPU requirements, we quantize GLM-130B into INT4 precision without quantization aware training while OPT and BLOOM can only reach INT8. Due to a unique property of the GLM architecture, GLM-130B's INT4 quantization introduces negligible performance degradation, e.g., -0.74% on LAMBADA and even +0.05% on MMLU, making it still better than the uncompressed GPT-3. This enables GLM-130B's fast inference with performance guarantee on a server of 4×RTX 3090 (24G) or 8×RTX 2080 Ti (11G), the most ever affordable GPU required for using 100B-scale LLMs to date.We open-source the model checkpoints, code, training logs, related toolkits, and lessons learned.THE DESIGN CHOICES OF GLM-130BThe architecture of a machine learning model defines its inductive bias. However, it has been realized that it is computationally unaffordable to explore various architectural designs for LLMs. We introduce and explain the unique design choices of GLM-130B. 2.1 GLM-130B'S ARCHITECTURE GLM as Backbone. Most recent 100B-scale LLMs, such as GPT-3, PaLM, OPT, and BLOOM, follow the traditional GPT-style (Radford et al., 2019) architecture of decoder-only autoregressive language modeling. In GLM-130B, we instead make an attempt to explore the potential of a bidirectional GLM-General Language Model (Du et al., 2022)-as its backbone.GLM is a transformer-based language model that leverages autoregressive blank infilling as its training objective. Briefly, for a text sequence x = [x 1 , · · · , x n ], text spans {s 1 , · · · , s m } are sampled from it, each of which s i denotes a span of consecutive tokens [s i,1 , · · · , s i,li ] and is replaced (i.e., corrupted) with a single mask token to form x corrupt . The model is asked to recover them autoregressively. To allow interactions between corrupted spans, their visibility to each other is decided by a He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters.",
    "prev": "-rank adaptation (QA-LoRA) algorithm.The motivation lies in the imbalanced degrees of freedom of quantization and adaptation, and the solution is to use group-wise operators which increase the degree of freedom of quantization meanwhile decreasing that of adaptation.QA-LoRA is easily implemented with a few lines of code, and it equips the original LoRA with two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized (e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy.We apply QA-LoRA to the LLaMA and LLaMA2 model families and validate its effectiveness in different fine-tuning datasets and downstream scenarios.Code will be made available at https://github.com/yuhuixu1993/qa-lora.",
    "curr": "INTRODUCTION\n\nRecently, large language models (LLMs) (Brown et al., 2020;Scao et al., 2022;Zhang et al., 2022;Touvron et al., 2023a;Chowdhery et al., 2022;OpenAI, 2023;Zeng et al., 2023) have shown unprecedented performance across a wide range of language understanding tasks (Wei et al., 2022a) and served as the foundation of state-of-the-art chat systems (Bubeck et al., 2023).The diversity of real-world applications calls for a pipeline in which LLMs can be fine-tuned to fit different scenarios and quantized to be deployed onto edge devices (e.g., mobile phones), and the key issue is to get rid of the heavy computational burden brought by the large number of parameters of LLMs.",
    "next": "There are two lines of research for this purpose.The first one is parameter-efficient fine-tuning (PEFT) (Houlsby et al., 2019;Li & Liang, 2021;Liu et al., 2021;He et al., 2022;Hu et al., 2021) which introduced a small number of learnable parameters while keeping most pre-trained parameters unchanged.Among them, low-rank adaptation (LoRA) (Hu et al., 2021), a popular PEFT algorithm, proposed to fine-tune low-rank matrices to complement the pre-trained weights.Despite the comparable performanc",
    "query": "Are there any research papers on quantization and adaptation methods for large language models to enable efficient deployment on edge devices using parameter-efficient fine-tuning techniques like LoRA?"
  },
  {
    "index": 537,
    "source_corpus_id": 210164865,
    "ref_id": "b48",
    "citation_corpus_id": 26238954,
    "start": 3616,
    "end": 3635,
    "title": "Under review as a conference paper at ICLR 2018 BREAKING THE SOFTMAX BOTTLENECK: A HIGH-RANK RNN LANGUAGE MODEL",
    "abstract": "We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. 1 * Equal contribution. Ordering determined by dice rolling. 1 Code is available at httpsUnder review as a conference paper at ICLR 2018 matrices that have much larger normalized singular values and thus much higher rank than Softmax and other baselines on real-world datasets.We evaluate our proposed approach on standard language modeling benchmarks. MoS substantially improves over the current state-of-the-art results on benchmarks, by up to 3.6 points in terms of perplexity, reaching perplexities 47.69 on Penn Treebank and 40.68 on WikiText-2. We further apply MoS to a dialog dataset and show improved performance over Softmax and other baselines.Our contribution is two-fold. First, we identify the Softmax bottleneck by formulating language modeling as a matrix factorization problem. Second, we propose a simple and effective method that substantially improves over the current state-of-the-art results.",
    "prev": "AR models based on recurrent (G)NNs can be viewed as special instantiations of SSMs in which the state transitions are restricted to being deterministic (Fraccaro, 2018, Section 4.2).",
    "curr": "Despite their simplicity, it has been pointed out that their modeling capability is bottlenecked by the deterministic state transitions (Chung et al., 2015;Fraccaro et al., 2016) and the oversimplified observation distributions (Yang et al., 2018).",
    "next": "In this study, we make the following contributions: (i) We propose the relational state-space model (R-SSM), a novel hierarchical deep SSM that simulates the stochastic state transitions of interacting objects with GNNs, extending GNN-based dynamics modeling to challenging stochastic multi-object systems.",
    "query": "Are there any studies that explore methods to enhance state-space models by incorporating graph neural networks for stochastic state transitions in multi-object systems?"
  },
  {
    "index": 538,
    "source_corpus_id": 252715691,
    "ref_id": "b9",
    "citation_corpus_id": 237416585,
    "start": 11827,
    "end": 11845,
    "title": "Published as a conference paper at ICLR 2022 FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS",
    "abstract": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning-finetuning language models on a collection of datasets described via instructions-substantially improves zeroshot performance on unseen tasks.",
    "prev": "To improve FFNs in Transformer, we pick GLU with the GeLU (Hendrycks & Gimpel, 2016) activation as the replacement.",
    "curr": "2.2 GLM-130B'S PRE-TRAINING SETUP Inspired by recent works (Aribandi et al., 2022;Wei et al., 2022a;Sanh et al., 2022), the GLM-130B pre-training objective includes not only the self-supervised GLM autoregressive blank infilling) but also multi-task learning for a small portion of tokens.",
    "next": "This is expected to help boost its downstream zero-shot performance.",
    "query": "Are there any research papers on improving zero-shot learning in language models through instruction tuning and multi-task pre-training strategies?"
  },
  {
    "index": 541,
    "source_corpus_id": 52892477,
    "ref_id": "b1",
    "citation_corpus_id": 900029,
    "start": 1151,
    "end": 1172,
    "title": "NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 20-28, Deep Neural Network Language Models",
    "abstract": "In recent years, neural network language models (NNLMs) have shown success in both peplexity and word error rate (WER) compared to conventional n-gram language models. Most NNLMs are trained with one hidden layer. Deep neural networks (DNNs) with more hidden layers have been shown to capture higher-level discriminative information about input features, and thus produce better networks. Motivated by the success of DNNs in acoustic modeling, we explore deep neural network language models (DNN LMs) in this paper. Results on a Wall Street Journal (WSJ) task demonstrate that DNN LMs offer improvements over a single hidden layer NNLM. Furthermore, our preliminary results are competitive with a model M language model, considered to be one of the current state-of-the-art techniques for language modeling.",
    "prev": "On the WIKITEXT-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the BILLION WORD benchmark, we achieve 23.02 perplexity.",
    "curr": "1\n\nINTRODUCTION\n\nLanguage modeling is a basic task in natural language processing, with many applications such as speech recognition (Arisoy et al., 2012) and statistical machine translation (Schwenk et al., 2012;Vaswani et al., 2013;Baltescu & Blunsom, 2015).",
    "next": "Recently, much progress has been made by neural methods (Bengio et al., 2003;Mikolov et al., 2010) based on LSTMs (Józefowicz et al., 2016), gated convolutional networks (Dauphin et al., 2017) and self-attentional networks (Al-Rfou et al., 2018).",
    "query": "Are there any research papers on deep neural network language models and their performance improvements in reducing perplexity compared to traditional n-gram models?"
  },
  {
    "index": 543,
    "source_corpus_id": 247596648,
    "ref_id": "b24",
    "citation_corpus_id": 3144218,
    "start": 8028,
    "end": 8049,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "For generalizing across parametric variations of a single morphology, various approaches have been proposed like using a learned hardware embedding (Chen et al., 2018), meta-learning for policy adaptation (Al-Shedivat et al., 2017;Ghadirzadeh et al., 2021), kinematics randomization (Exarchos et al., 2020), and dynamics randomization (Peng et al., 2018).",
    "curr": "In case of multiple different morphologies, one approach to tackle the challenge of differences in action and state spaces is to leverage Graph Neural Networks (Scarselli et al., 2008;Kipf & Welling, 2017;.",
    "next": "Wang et al.",
    "query": "Are there any research papers on the application of graph neural networks for semi-supervised classification in scenarios involving multiple morphologies or varying action and state spaces?"
  },
  {
    "index": 544,
    "source_corpus_id": 229297917,
    "ref_id": "b30",
    "citation_corpus_id": 14089312,
    "start": 2248,
    "end": 2264,
    "title": "Published as a conference paper at ICLR 2017 PRUNING FILTERS FOR EFFICIENT CONVNETS",
    "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on filter pruning techniques for improving the computational efficiency of convolutional neural networks?"
  },
  {
    "index": 546,
    "source_corpus_id": 219530969,
    "ref_id": "b2",
    "citation_corpus_id": 3517962,
    "start": 2457,
    "end": 2460,
    "title": "Published as a conference paper at ICLR 2018 DISTRIBUTED DISTRIBUTIONAL DETERMINISTIC POLICY GRADIENTS",
    "abstract": "This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of N -step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance.arXiv:1804.08617v1 [cs.LG] 23 Apr 2018Published as a conference paper at ICLR 2018 experience, which we implement using the ApeX framework(Horgan et al., 2018). This results in significant savings in terms of wall-clock time for difficult control tasks. We will also introduce a number of small improvements to the DDPG algorithm, and in our experiments will show the individual contributions of each component. Finally, this algorithm, which we call the Distributed Distributional DDPG algorithm (D4PG), obtains state-of-the-art performance across a wide variety of control tasks, including hard manipulation and locomotion tasks.",
    "prev": "Under review.",
    "curr": "Introduction\n\nReinforcement learning (RL) algorithms have recently demonstrated impressive success in learning behaviors for a variety of sequential decision-making tasks [3,24,42].",
    "next": "Virtually all of these demonstrations have relied on highly-frequent online access to the environment, with the RL algorithms often interleaving each update to the policy with additional experience collection of that policy acting in the environment.",
    "query": "Are there any research papers on distributed distributional reinforcement learning algorithms for continuous control tasks using off-policy methods and prioritized experience replay?"
  },
  {
    "index": 548,
    "source_corpus_id": 259096096,
    "ref_id": "b37",
    "citation_corpus_id": 213695995,
    "start": 2481,
    "end": 2484,
    "title": "Published as a conference paper at ICLR 2020 FEATURE INTERACTION INTERPRETABILITY: A CASE FOR EXPLAINING AD-RECOMMENDATION SYSTEMS VIA NEURAL INTERACTION DETECTION",
    "abstract": "Recommendation is a prevalent application of machine learning that affects many users; therefore, it is important for recommender models to be accurate and interpretable. In this work, we propose a method to both interpret and augment the predictions of black-box recommender systems. In particular, we propose to interpret feature interactions from a source recommender model and explicitly encode these interactions in a target recommender model, where both source and target models are black-boxes. By not assuming the structure of the recommender system, our approach can be used in general settings. In our experiments, we focus on a prominent use of machine learning recommendation: ad-click prediction. We found that our interaction interpretations are both informative and predictive, e.g., significantly outperforming existing recommender models. What's more, the same approach to interpret interactions can provide new insights into domains even beyond recommendation, such as text and image classification.",
    "prev": "Introduction\n\nA core research question in multimodal learning is to understand the nature of multimodal interactions across modalities in the context of a task: the emergence of new task-relevant information during learning from both modalities that was not present in either modality alone [6,65].",
    "curr": "In settings where labeled multimodal data is abundant, the study of multimodal interactions has inspired advances in theoretical analysis [1,43,66,84,94] and representation learning [51, 76,91,104] in language and vision [2], multimedia [9], healthcare [53], and robotics [57].",
    "next": "In this paper, we study the problem of interaction quantification in a setting where there is only unlabeled multimodal data D M = {(x 1 , x 2 )} but some labeled unimodal data D i = {(x i , y)} collected separately for each modality.",
    "query": "Are there any research papers on methods for interpreting feature interactions in ad recommendation systems using neural interaction detection?"
  },
  {
    "index": 549,
    "source_corpus_id": 248377437,
    "ref_id": "b12",
    "citation_corpus_id": 28202810,
    "start": 22619,
    "end": 22642,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "2, the learner policy is trained with a MaxEnt RL algorithm with per-timestep entropy-regularized reward given by f ω (s t , a t ) − α log π(a t |s t ), where α is the entropy coefficient.",
    "curr": "In our experiments, we use the clipped-ratio PPO algorithm (Schulman et al., 2017) and adaptively tune α as suggested in prior work (Haarnoja et al., 2018).",
    "next": "RELATED WORK\n\nThere is a vast amount of literature on IL since it is a powerful framework to train agents to perform complex behaviors without a reward specification.",
    "query": "Are there any research papers on off-policy maximum entropy reinforcement learning algorithms with stochastic actor-critic methods and entropy regularization for improved stability in continuous control tasks?"
  },
  {
    "index": 551,
    "source_corpus_id": 227054461,
    "ref_id": "b9",
    "citation_corpus_id": 8968704,
    "start": 2176,
    "end": 2192,
    "title": "A Neural Representation of Sketch Drawings",
    "abstract": "We present sketch-rnn, a recurrent neural network (RNN) able to construct stroke-based drawings of common objects. The model is trained on a dataset of human-drawn images representing many different classes. We outline a framework for conditional and unconditional sketch generation, and describe new robust training methods for generating coherent sketch drawings in a vector format.",
    "prev": "Sketching is often used during brainstorming to help the creative process, and is a popular creative activity in itself.",
    "curr": "Sketch-related AI so far has primarily focused on mimicking the human ability to perceive rich visual information from simple line drawings (Yu et al., 2015;Li et al., 2018) and to generate minimal depictions that capture the salient aspects of our visual world (Ha & Eck, 2018;Isola et al., 2017).",
    "next": "Most existing datasets contain sketches drawn by humans to realistically mimic common objects (Eitz et al., 2012;Sangkloy et al., 2016;Jongejan et al., 2016;Wang et al., 2019).",
    "query": "Are there any research papers on neural network models for generating sketch drawings using human-drawn datasets and robust training methods?"
  },
  {
    "index": 552,
    "source_corpus_id": 220713471,
    "ref_id": "b6",
    "citation_corpus_id": 53018855,
    "start": 5740,
    "end": 5743,
    "title": "DISCRIMINATOR REJECTION SAMPLING",
    "abstract": "We propose a rejection sampling scheme using the discriminator of a GAN to approximately correct errors in the GAN generator distribution. We show that under quite strict assumptions, this will allow us to recover the data distribution exactly. We then examine where those strict assumptions break down and design a practical algorithm-called Discriminator Rejection Sampling (DRS)-that can be used on real data-sets. Finally, we demonstrate the efficacy of DRS on a mixture of Gaussians and on the SAGAN model, state-of-the-art in the image generation task at the time of developing this work. On ImageNet, we train an improved baseline that increases the Inception Score from 52.52 to 62.36 and reduces the Fréchet Inception Distance from 18.65 to 14.79. We then use DRS to further improve on this baseline, improving the Inception Score to 76.08 and the FID to 13.75.",
    "prev": "In contrast, we rely on private GAN to reduce the support to only contain the set of privately generated samples, which makes PGB tractable even for high-dimensional data.",
    "curr": "We also provide an extension of the PGB method by incorporating the technique of discriminator rejection sampling [7,35].",
    "next": "We leverage the fact that the distinguisher's equilibrium strategy, which is a mixture of discriminators, can often accurately predict which samples are unlikely and thus can be used as a rejection sampler.",
    "query": "Are there any research papers on discriminator rejection sampling techniques for improving GAN-based image generation with enhanced distribution correction?"
  },
  {
    "index": 555,
    "source_corpus_id": 257637230,
    "ref_id": "b17",
    "citation_corpus_id": 229924317,
    "start": 4523,
    "end": 4541,
    "title": "EMERGENT SYMBOLS THROUGH BINDING IN EXTERNAL MEMORY",
    "abstract": "A key aspect of human intelligence is the ability to infer abstract rules directly from high-dimensional sensory data, and to do so given only a limited amount of training experience. Deep neural network algorithms have proven to be a powerful tool for learning directly from high-dimensional data, but currently lack this capacity for data-efficient induction of abstract rules, leading some to argue that symbol-processing mechanisms will be necessary to account for this capacity. In this work, we take a step toward bridging this gap by introducing the Emergent Symbol Binding Network (ESBN), a recurrent network augmented with an external memory that enables a form of variable-binding and indirection. This binding mechanism allows symbol-like representations to emerge through the learning process without the need to explicitly incorporate symbol-processing machinery, enabling the ESBN to learn rules in a manner that is abstracted away from the particular entities to which those rules apply. Across a series of tasks, we show that this architecture displays nearly perfect generalization of learned rules to novel entities given only a limited number of training examples, and outperforms a number of other competitive neural network architectures.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on neural network architectures that use external memory to enable emergent symbols through binding for data-efficient rule learning?"
  },
  {
    "index": 557,
    "source_corpus_id": 263620293,
    "ref_id": "b65",
    "citation_corpus_id": 3352260,
    "start": 5477,
    "end": 5499,
    "title": "Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning",
    "abstract": "The physical design of a robot and the policy that controls its motion are inherently coupled, and should be determined according to the task and environment. In an increasing number of applications, data-driven and learningbased approaches, such as deep reinforcement learning, have proven effective at designing control policies. For most tasks, the only way to evaluate a physical design with respect to such control policies is empirical-i.e., by picking a design and training a control policy for it. Since training these policies is timeconsuming, it is computationally infeasible to train separate policies for all possible designs as a means to identify the best one. In this work, we address this limitation by introducing a method that performs simultaneous joint optimization of the physical design and control network. Our approach maintains a distribution over designs and uses reinforcement learning to optimize a control policy to maximize expected reward over the design distribution. We give the controller access to design parameters to allow it to tailor its policy to each design in the distribution. Throughout training, we shift the distribution towards higher-performing designs, eventually converging to a design and control policy that are jointly optimal. We evaluate our approach in the context of legged locomotion, and demonstrate that it discovers novel designs and walking gaits, outperforming baselines in both performance and efficiency.",
    "prev": "Via our detailed ablation study, we construct ChemRLformer and find that pretraining on aligned datasets can significantly improve performance across all molecular design tasks, even exceeding the performance of agents pretrained on 100 times larger datasets.We also show that targeted algorithmic design, such as hill-climbing in the replay buffer and regularization, further increases the performance of ChemRLformer.To the best of our knowledge, ChemRLformer is the largest analysis of text-based RL methods for molecule discovery.",
    "curr": "[Yu et al., 2018, Schaff et al., 2019].Other methods have explicitly included the design process in the RL loop by training design problems together [Chen et al., 2021a, Ha, 2019, Luck et al., 2020, Kumar et al., 2022] with most prior work focusing on robot and agent design, not molecular design.Our molecular design work creates an autoregressive structure that grows the size of the state as the agent acts in the environment.",
    "next": "Molecular Discovery Using Sequence-Based Methods: Sequence-based methods treat molecular design as a sequence of tokens that get concatenated in order.Generative models for sequence-based methods span a diverse range, including variational autoencoders (VAEs) [Gómez-Bombarelli et al., 2018, Alperstein et al., 2019], recurrent neural networks (RNNs) [Gupta et al., 2018, Bjerrum and Threlfall, 2017, Grisoni et al., 2020, Flam-Shepherd et al., 2022] and transformer models [Wang et al., 2019, Fabian et al., 2020, Edwards et al., 2022a, Zeng et al., 2022, Taylor et al., 2022].The general procedure for all",
    "query": "Are there any research papers on jointly learning to construct and control agents using deep reinforcement learning, with methods for simultaneous optimization of physical design and control policies?"
  },
  {
    "index": 558,
    "source_corpus_id": 240354066,
    "ref_id": "b12",
    "citation_corpus_id": 219965819,
    "start": 2855,
    "end": 2858,
    "title": "LIPSCHITZ RECURRENT NEURAL NETWORKS",
    "abstract": "Differential equations are a natural choice for modeling recurrent neural networks because they can be viewed as dynamical systems with a driving input. In this work, we propose a recurrent unit that describes the hidden state's evolution with two parts: a well-understood linear component plus a Lipschitz nonlinearity. This particular functional form simplifies stability analysis, which enables us to provide an asymptotic stability guarantee. Further, we demonstrate that Lipschitz recurrent units are more robust with respect to perturbations. We evaluate our approach on a range of benchmark tasks, and we show it outperforms existing recurrent units.",
    "prev": "Since LRDs are perhaps the foremost challenge for sequence models, all standard model families such as continuous-time models (CTMs), RNNs, CNNs, and Transformers include many specialized variants designed to address them.",
    "curr": "Modern examples include orthogonal and Lipschitz RNNs [1,13] to combat vanishing gradients, dilated convolutions to increase context size [3,28], and an increasingly vast family of efficient Transformers that reduce the quadratic dependence on sequence length [8,22].",
    "next": "Despite being designed for LRDs, these solutions still perform poorly on challenging benchmarks such as LRA [40] or raw audio classification [18].",
    "query": "Are there any research papers on methods for addressing long-range dependencies in sequence models, including techniques like Lipschitz constraints, orthogonal RNNs, dilated convolutions, or efficient Transformers?"
  },
  {
    "index": 559,
    "source_corpus_id": 108306725,
    "ref_id": "b9",
    "citation_corpus_id": 6715185,
    "start": 35139,
    "end": 35165,
    "title": "Published as a conference paper at ICLR 2016 NEURAL PROGRAMMER: INDUCING LATENT PROGRAMS WITH GRADIENT DESCENT",
    "abstract": "Deep neural networks have achieved impressive supervised classification performance in many tasks including image recognition, speech recognition, and sequence to sequence learning. However, this success has not been translated to applications like question answering that may involve complex arithmetic and logic reasoning. A major limitation of these models is in their inability to learn even simple arithmetic and logic operations. For example, it has been shown that neural networks fail to learn to add two binary numbers reliably. In this work, we propose Neural Programmer, a neural network augmented with a small set of basic arithmetic and logic operations that can be trained end-to-end using backpropagation. Neural Programmer can call these augmented operations over several steps, thereby inducing compositional programs that are more complex than the built-in operations. The model learns from a weak supervision signal which is the result of execution of the correct program, hence it does not require expensive annotation of the correct program itself. The decisions of what operations to call, and what data segments to apply to are inferred by Neural Programmer. Such decisions, during training, are done in a differentiable fashion so that the entire network can be trained jointly by gradient descent. We find that training the model is difficult, but it can be greatly improved by adding random noise to the gradient. On a fairly complex synthetic table-comprehension dataset, traditional recurrent networks and attentional models perform poorly while Neural Programmer typically obtains nearly perfect accuracy. * Work done during an internship at Google.",
    "prev": "Neural Turing Machine (NTM) (Graves et al., 2014; enables general-purpose neural problem solving such as sorting by introducing an external memory that mimics the execution of Turing Machine.",
    "curr": "Neural program induction and synthesis (Neelakantan et al., 2016;Reed & De Freitas, 2016;Kaiser & Sutskever, 2016;Parisotto et al., 2017;Devlin et al., 2017;Bunel et al., 2018;Sun et al., 2018) are recently introduced to solve problems by synthesizing computer programs with neural augmentations.",
    "next": "Some works tackle the issue of the systematical generalization by introducing extra supervision (Cai et al., 2017).",
    "query": "Are there any research papers on neural architectures that integrate programmatic operations with gradient-based training for enhancing reasoning tasks involving arithmetic and logic?"
  },
  {
    "index": 560,
    "source_corpus_id": 257039090,
    "ref_id": "b12",
    "citation_corpus_id": 173990158,
    "start": 7470,
    "end": 7488,
    "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences",
    "abstract": "Human language is often multimodal, which comprehends a mixture of natural language, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human language time-series data exist: 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; and 2) long-range dependencies between elements across modalities. In this paper, we introduce the Multimodal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data. At the heart of our model is the directional pairwise crossmodal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin. In addition, empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in MulT.",
    "prev": "These datasets enable us to train a multimodal model y = f (x 1 , x 2 ; θ) which we are interested in visualizing.",
    "curr": "Modern parameterizations of multimodal models f are typically black-box neural networks, such as multimodal transformers (Hendricks et al., 2021;Tsai et al., 2019) and pretrained models (Li et al., 2019;Lu et al., 2019).",
    "next": "How can we visualize and understand the internal modeling of multimodal information and interactions in these models?",
    "query": "Are there any studies that explore methods for visualizing and understanding internal modeling of multimodal information in transformer-based models?"
  },
  {
    "index": 561,
    "source_corpus_id": 52055130,
    "ref_id": "b2",
    "citation_corpus_id": 18828233,
    "start": 41405,
    "end": 41429,
    "title": "TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.",
    "prev": "(13)\nSome intuition might help understand why this might work.",
    "curr": "As discussed in Arjovsky & Bottou (2017), if P and Q θ have support on a low-dimensional manifolds on X , unless they are perfectly aligned, there exists a discriminator that will be able to perfectly distinguish between samples coming from P and Q θ , which means that U ψ,P and U ψ,Q must also be disjoint.",
    "next": "However, to train the generator, U ψ,P and U ψ,Q need to share support on Y in order to ensure stable and non-zero gradients for the generator.",
    "query": "Are there any research papers on the theoretical analysis of training dynamics in generative adversarial networks, focusing on instability, saturation, and methods to address these challenges?"
  },
  {
    "index": 562,
    "source_corpus_id": 263608118,
    "ref_id": "b24",
    "citation_corpus_id": 253098274,
    "start": 2169,
    "end": 2187,
    "title": "SUPER-NATURALINSTRUCTIONS: Generalization via Declarative Instructions on 1600+ NLP Tasks",
    "abstract": "How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce SUPER-NATURALINSTRUCTIONS, 1 a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructionstraining models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones. Furthermore, we build Tk-INSTRUCT, a transformer model trained to follow a variety of incontext instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-INSTRUCT outperforms existing instruction-following models such as Instruct-GPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models. 2",
    "prev": "es the attention matrix with linear complexity via kernel-based linear attention, then creates a sparse approximation to the full attention matrix with a top-k selection to perform a sparse attention operation.For language modeling tasks (Wikitext2), previous linear and sparse attention methods show a roughly two-fold worse perplexity scores over the quadratic OPT-125M baseline, while SEA achieves an even better perplexity than OPT-125M, using roughly half as much memory as OPT-125M.Moreover, SEA maintains an interpretable attention matrix and can utilize knowledge distillation to lower the complexity of existing pretrained transformers.We believe that our work will have a large practical impact, as it opens the possibility of running large transformers on resource-limited devices with less memory.",
    "curr": "INTRODUCTION\n\nThe transformer (Vaswani et al., 2017) architecture has revolutionized various fields of artificial intelligence, such as natural language understanding (Touvron et al., 2023;Wang et al., 2022) and computer vision (Dosovitskiy et al., 2021) due to its ability to learn pairwise relationships between all T tokens in a given sequence (O(T 2 )).This has ushered in the era of large transformer-based foundation models with impressive generalization abilities (Brown et al., 2020;Chiang et al., 2023).However, since the transformer's attention mechanism comes with a quadratic space and time complexity, it becomes untenable for handling long sequences which is essential for tasks such as dialogue generation (Chen et al., 2023).To overcome this limitation, previous works have suggested approaches with linear complexity by using static or dynamic sparse attention patterns (Beltagy et al., 2020;Zaheer et al., 2020;Tay et al., 2020a;Kitaev et al., 2020;Tay et al., 2020b;Liu et al., 2021), or by replacing quadratic attention with kernel or low-rank approximations (Choromanski et al., 2021;Chen et al., 2021;Qin et al., 2022).",
    "next": "However, despite their promising aspects, previous linear attent",
    "query": "Are there any research papers on methods to reduce the quadratic computational complexity of transformer attention mechanisms, such as linear or sparse approximations, in language modeling tasks?"
  },
  {
    "index": 564,
    "source_corpus_id": 260887189,
    "ref_id": "b7",
    "citation_corpus_id": 258461287,
    "start": 3078,
    "end": 3097,
    "title": "Can Large Language Models Be an Alternative to Human Evaluation?",
    "abstract": "Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided. In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation. We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation. We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: openended story generation and adversarial attacks. We show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation: the texts rated higher by human experts are also rated higher by the LLMs. We also find that the results of LLM evaluation are stable over different formatting of the task instructions and the sampling algorithm used to generate the answer. We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation. , et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. , et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.",
    "prev": "1\n\nINTRODUCTION\n\nThe emergence of Large Language Models (LLMs) has played a pivotal role in driving the advancement of Artificial Intelligence (AI) systems.",
    "curr": "Noteworthy LLMs like ChatGPT (OpenAI, 2023a;b), Claude2 (Anthropic, 2023), Bard (Google, 2023), and Llama2 (Touvron et al., 2023a) have demonstrated their advanced capability to perform innovative applications, ranging from tool utilization, supplementing human evaluations, to stimulating human interactive behaviors Bubeck et al., 2023;Schick et al., 2023;Chiang & Lee, 2023;Park et al., 2023).",
    "next": "The outstanding competencies have fueled their widespread deployment, while the progression is shadowed by a significant challenge: ensuring the safety and reliability of the responses.",
    "query": "Are there any studies exploring the use of large language models as an alternative to human evaluation in natural language processing tasks, focusing on their consistency and stability in assessing text quality?"
  },
  {
    "index": 565,
    "source_corpus_id": 51969884,
    "ref_id": "b4",
    "citation_corpus_id": 5590763,
    "start": 28291,
    "end": 28309,
    "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "prev": "−E(y, h; x) = f i xiw x if j yjw y jf k h k w h kf + k w h k h k + j w y j yj = x T W x • y T W y • h T W h 1 + h T w h + y T w y(21)\nSetting aside of bias terms, the I × J × K parameter tensor of unfactored Higher-Order Boltzmann Machines is replaced with three matrices, W x ∈ R I×F , W y ∈ R J×F , and W h ∈ R K×F .",
    "curr": "D.3 MULTIPLICATIVE INTEGRATION WITH RECURRENT NEURAL NETWORKS\n\nMost of recurrent neural networks, including vanilla RNNs, Long Short Term Memory networks (Hochreiter & Schmidhuber, 1997) and Gated Recurrent Units (Cho et al., 2014), share a common expression as follows:\nφ(Wx + Uh + b)(22)\nwhere φ is a non-linear function, W ∈ R d×n , x ∈ R n , U ∈ R d×m , h ∈ R m , and b ∈ R d is a bias vector.",
    "next": "Note that, usually, x is an input state vector and h is an hidden state vector in recurrent neural networks.",
    "query": "Are there any research papers on RNN-based encoder-decoder models for learning phrase representations in statistical machine translation?"
  },
  {
    "index": 566,
    "source_corpus_id": 256846836,
    "ref_id": "b24",
    "citation_corpus_id": 222140788,
    "start": 6355,
    "end": 6375,
    "title": "Published as a conference paper at ICLR 2021 DENOISING DIFFUSION IMPLICIT MODELS",
    "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples 10× to 50× faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error.",
    "prev": "Different works also studied different approximations of the unknown q(z t−1 |z t , z 0 ) used to perform sampling.",
    "curr": "For example, denoising diffusion implicit model (DDIM) (Song et al., 2021a) first computed a predicted clean data point\nz 0 = z t − ( √ 1 − α t ) θ (z t , t) √ α t ,(3)\nand sample z t−1 from q(z t−1 |z t ,ẑ 0 ) by replacing unknown z 0 withẑ 0 .",
    "next": "On the other hand, while the details of individual sampling methods vary, all sampling methods produce z t−1 based on current sample z t , current time step t and a predicted noiseˆ .",
    "query": "Are there any research papers on methods for accelerating diffusion probabilistic models using non-Markovian processes and implicit sampling techniques?"
  },
  {
    "index": 568,
    "source_corpus_id": 264288947,
    "ref_id": "b27",
    "citation_corpus_id": 254877603,
    "start": 2411,
    "end": 2432,
    "title": "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories",
    "abstract": "Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the difficulty of encoding a wealth of world knowledge in their parameters. This paper aims to understand LMs' strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments on two open-domain entity-centric QA datasets: POPQA, our new dataset with 14k questions about long-tail entities, and EntityQuestions, a widely used opendomain QA dataset. We find that LMs struggle with less popular factual knowledge, and that retrieval augmentation helps significantly in these cases. Scaling, on the other hand, mainly improves memorization of popular knowledge, and fails to appreciably improve memorization of factual knowledge in the long tail. Based on those findings, we devise a new method for retrieval augmentation that improves performance and reduces inference costs by only retrieving non-parametric memories when necessary. 1  , et al.  2016. MS MARCO: A human generated machine reading comprehension dataset. . 2022. Quantifying memorization across neural language models. mented language models. . 2022. Large language models struggle to learn long-tail knowledge.",
    "prev": "1 1 Our code and trained models are available at https://selfrag.github.io/.",
    "curr": "INTRODUCTION\n\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023;Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022).Retrieval-Augmented Generation (RAG) methods (Figure 1 left; Lewis et al.",
    "next": "2020;Guu et al.",
    "query": "Are there any research papers on the effectiveness of parametric versus non-parametric memories in large language models for factual knowledge tasks, including retrieval-augmented methods?"
  },
  {
    "index": 570,
    "source_corpus_id": 252873467,
    "ref_id": "b14",
    "citation_corpus_id": 233289412,
    "start": 3502,
    "end": 3523,
    "title": "Editing Factual Knowledge in Language Models",
    "abstract": "The factual knowledge acquired during pretraining and stored in the parameters of Language Models (LMs) can be useful in downstream tasks (e.g., question answering or textual inference). However, some facts can be incorrectly induced or become obsolete over time. We present KNOWLEDGEEDITOR, a method which can be used to edit this knowledge and, thus, fix 'bugs' or unexpected predictions without the need for expensive retraining or fine-tuning. Besides being computationally efficient, KNOWLEDGEEDITOR does not require any modifications in LM pretraining (e.g., the use of meta-learning). In our approach, we train a hyper-network with constrained optimization to modify a fact without affecting the rest of the knowledge; the trained hyper-network is then used to predict the weight update at test time. We show KNOWL-EDGEEDITOR's efficacy with two popular architectures and knowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and ii) a sequence-to-sequence BART model for question answering. With our method, changing a prediction on the specific wording of a query tends to result in a consistent change in predictions also for its paraphrases. We show that this can be further encouraged by exploiting (e.g., automatically-generated) paraphrases during training. Interestingly, our hyper-network can be regarded as a 'probe' revealing which components need to be changed to manipulate factual knowledge; our analysis shows that the updates tend to be concentrated on a small subset of components. 1",
    "prev": "To that end, several knowledge-editing methods have been proposed to insert new memories directly into specific model parameters.",
    "curr": "The approaches include constrained fine-tuning (Zhu et al., 2020), hypernetwork knowledge editing (De Cao et al., 2021;Hase et al., 2021;Mitchell et al., 2021;, and rank-one model editing (Meng et al., 2022).",
    "next": "However, this body of work is typically limited to updating at most a few dozen facts; a recent study evaluates on a maximum of 75 (Mitchell et al., 2022) whereas others primarily focus on single-edit cases.",
    "query": "Are there any research papers on methods for editing factual knowledge in language models using hyper-networks with constrained optimization, and their applications in fact-checking and question answering tasks?"
  },
  {
    "index": 573,
    "source_corpus_id": 71145737,
    "ref_id": "b17",
    "citation_corpus_id": 49868626,
    "start": 27345,
    "end": 27363,
    "title": "META-LEARNING WITH LATENT EMBEDDING OPTIMIZATION",
    "abstract": "Gradient-based meta-learning techniques are both widely applicable and proficient at solving challenging few-shot learning and fast adaptation problems. However, they have practical difficulties when operating on high-dimensional parameter spaces in extreme low-data regimes. We show that it is possible to bypass these limitations by learning a data-dependent latent generative representation of model parameters, and performing gradient-based meta-learning in this lowdimensional latent space. The resulting approach, latent embedding optimization (LEO), decouples the gradient-based adaptation procedure from the underlying high-dimensional space of model parameters. Our evaluation shows that LEO can achieve state-of-the-art performance on the competitive miniImageNet and tieredImageNet few-shot classification tasks. Further analysis indicates LEO is able to capture uncertainty in the data, and can perform adaptation more effectively by optimizing in latent space. arXiv:1807.05960v3 [cs.LG]",
    "prev": "That said, in the past two years, there have been several other methods proposed for few-shot learning.",
    "curr": "Some bear similarity with MAML and correspond to a meta-learner trained to quickly adapt its parameters to various novel tasks (Ravi & Larochelle, 2017;Munkhdalai & Yu, 2017;Rusu et al., 2019;Yoon et al., 2018).",
    "next": "Others relate to Prototypical Networks by learning a data representation as well as a compact representation for a classifier of data under that representation (Bertinetto et al., 2019;Gidaris & Komodakis, 2018;Oreshkin et al., 2018;Gidaris & Komodakis, 2018).",
    "query": "Are there any research papers on gradient-based meta-learning techniques for few-shot learning that employ latent embedding optimization to address challenges in high-dimensional parameter spaces?"
  },
  {
    "index": 574,
    "source_corpus_id": 238634584,
    "ref_id": "b30",
    "citation_corpus_id": 211842237,
    "start": 5855,
    "end": 5879,
    "title": "Published as a conference paper at ICLR 2020 DIRECTIONAL MESSAGE PASSING FOR MOLECULAR GRAPHS",
    "abstract": "Graph neural networks have recently achieved great successes in predicting quantum mechanical properties of molecules. These models represent a molecule as a graph using only the distance between atoms (nodes). They do not, however, consider the spatial direction from one atom to another, despite directional information playing a central role in empirical potentials for molecules, e.g. in angular potentials. To alleviate this limitation we propose directional message passing, in which we embed the messages passed between atoms instead of the atoms themselves. Each message is associated with a direction in coordinate space. These directional message embeddings are rotationally equivariant since the associated directions rotate with the molecule. We propose a message passing scheme analogous to belief propagation, which uses the directional information by transforming messages based on the angle between them. Additionally, we use spherical Bessel functions and spherical harmonics to construct theoretically well-founded, orthogonal representations that achieve better performance than the currently prevalent Gaussian radial basis representations while using fewer than 1 /4 of the parameters. We leverage these innovations to construct the directional message passing neural network (DimeNet). DimeNet outperforms previous GNNs on average by 76 % on MD17 and by 31 % on QM9. Our implementation is available online. 1 1 https://www.daml.in.tum.de/dimenet arXiv:2003.03123v2 [cs.LG] 5 Apr 2022Published as a conference paper at ICLR 2020 embeddings are equivariant with respect to the above transformations since the directions move with the molecule. Hence, they preserve the relative directional information between neighboring atoms. We propose to let message embeddings interact based on the distance between atoms and the angle between directions. Both distances and angles are invariant to translation, rotation, and inversion of the molecule, as required. Additionally, we show that the distance and angle can be jointly represented in a principled and effective manner by using spherical Bessel functions and spherical harmonics. We leverage these innovations to construct the directional message passing neural network (DimeNet). DimeNet can learn both molecular properties and atomic forces. It is twice continuously differentiable and solely based on the atom types and coordinates, which are essential properties for performing molecular dynamics simulations. DimeNet outperforms previous GNNs on average by 76 % on MD17 and by 31 % on QM9. Our paper's main contributions are:",
    "prev": "The Open Catalyst Project (OCP) provides a platform for comparing different architectures by predicting energies and forces from the periodic structure of catalytic surfaces (Chanussot et al., 2021).",
    "curr": "Our encoder and decoder PGNNs directly use GNN architectures developed for the OCP (Klicpera et al., 2020b;Godwin et al., 2021), which are also closely related to SE(3) equivariant networks (Thomas et al., 2018;Fuchs et al., 2020).",
    "next": "Quantum mechanical search of stable materials.",
    "query": "Are there any research papers on graph neural networks that use directional message passing for molecular graphs, incorporating SE(3) equivariant architectures and spherical harmonics-based representations to predict quantum mechanical properties of materials?"
  },
  {
    "index": 576,
    "source_corpus_id": 3518190,
    "ref_id": "b0",
    "citation_corpus_id": 11212020,
    "start": 1453,
    "end": 1475,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores up to 32.8, without using even a single parallel sentence at training time.",
    "curr": "INTRODUCTION\n\nThanks to recent advances in deep learning (Sutskever et al., 2014;Bahdanau et al., 2015) and the availability of large-scale parallel corpora, machine translation has now reached impressive performance on several language pairs .",
    "next": "However, these models work very well only when provided with massive amounts of parallel data, in the order of millions of parallel sentences.",
    "query": "Are there any research papers on neural machine translation approaches that avoid reliance on parallel data and employ soft alignment mechanisms instead of fixed-length encodings?"
  },
  {
    "index": 577,
    "source_corpus_id": 239009574,
    "ref_id": "b22",
    "citation_corpus_id": 213085920,
    "start": 3232,
    "end": 3236,
    "title": "Published as a conference paper at ICLR 2020 STRATEGIES FOR PRE-TRAINING GRAPH NEURAL NETWORKS",
    "abstract": "Many applications of machine learning require a model to make accurate predictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that naïve strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction. * Equal contribution. Project website, data and code: Attribute Masking Supervised Attribute Prediction Structural Similarity Prediction Structure prediction Context Prediction (b) Categorization of our pre-training methods Graph space Node space Graph embeddings Node embeddings Linear classifier Figure 1: (a.i) When only node-level pre-training is used, nodes of different shapes (semantically different nodes) can be well separated, however, node embeddings are not composable, and thus resulting graph embeddings (denoted by their classes, + and −) that are created by pooling node-level embeddings are not separable. (a.ii) With graph-level pre-training only, graph embeddings are well separated, however the embeddings of individual nodes do not necessarily capture their domainspecific semantics. (a.iii) High-quality node embeddings are such that nodes of different types are well separated, while at the same time, the embedding space is also composable. This allows for accurate and robust representations of entire graphs and enables robust transfer of pre-trained models to a variety of downstream tasks. (b) Categorization of pre-training methods for GNNs. Crucially, our methods, i.e., Context Prediction, Attribute Masking, and graph-level supervised pre-training (Supervised Attribute Prediction) enable both node-level and graph-level pre-training.matter of increasing the number of labeled pre-training datasets that are from the same domain as the downstream task. Instead, it requires substantial domain expertise to carefully select examples and target labels that are correlated with the downstream task of interest. Otherwise, the transfer of knowledge from related pre-training tasks to a new downstream task can harm generalization, which is known as negative transfer(Rosenstein et al., 2005)and significantly limits the applicability and reliability of pre-trained models.Present work. Here, we focus on pre-training as an approach to transfer learning in Graph Neural Networks (GNNs)(Kipf & Welling, 2017;Hamilton et al., 2017a;Ying et al., 2018b;Xu et al., 2019;2018)for graph-level property prediction. Our work presents two key contributions.(1) We conduct the first systematic large-scale investigation of strategies for pre-training GNNs. For that, we build two large new pre-training datasets, which we share with the community: a chemistry dataset with 2 million graphs and a biology dataset with 395K graphs. We also show that large domain-specific datasets are crucial to investigate pre-training and that existing downstream benchmark datasets are too small to evaluate models in a statistically reliable way.(2) We develop an effective pretraining strategy for GNNs and demonstrate its effectiveness and its ability for out-of-distribution generalization on hard transfer-learning problems.",
    "prev": "Falling broadly into the second category, our paper studies self-supervised molecular representation learning by leveraging the consistency between 3D geometry and 2D topology.",
    "curr": "Motivated by the prominent success of the pretraining-finetuning pipeline [17], unsupervisedly pretrained graph neural networks for molecules yields promising performance on downstream tasks and becomes increasingly popular [42,54,82,90,103,104].",
    "next": "The key to pre-training lies in finding an effective proxy task (i.e., training objective) to leverage the power of large unlabeled datasets.",
    "query": "Are there any research papers on pre-training strategies for graph neural networks using self-supervised learning methods for molecular property prediction and protein function prediction?"
  },
  {
    "index": 578,
    "source_corpus_id": 241035330,
    "ref_id": "b11",
    "citation_corpus_id": 2381275,
    "start": 2402,
    "end": 2424,
    "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context *",
    "abstract": "We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAM-BADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-ofthe-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text. * Denis and Germán share first authorship. Marco, Gemma, and Raquel share senior authorship.",
    "prev": "Introduction\n\nLarge language models (LMs) such as GPT-3 (Brown et al., 2020, Lieber et al., 2021, Radford et al., 2019, Wang and Komatsuzaki, 2021 are pretrained on massive text corpora to predict the next word given previous words.",
    "curr": "They demonstrate the surprising ability to do in-context learning, where an LM \"learns\" to do a task simply by conditioning on a prompt containing input-output pairs, achieving SOTA results on LAMBADA (Paperno et al., 2016) and TriviaQA (Joshi et al., 2017) tasks (18% and 3% over previous SOTA (Brown et al., 2020)).",
    "next": "For example, consider the task of predicting nationalities from names.",
    "query": "Are there any research papers on datasets for evaluating language models' ability to understand broad discourse context, such as LAMBADA?"
  },
  {
    "index": 582,
    "source_corpus_id": 263834782,
    "ref_id": "b6",
    "citation_corpus_id": 52967399,
    "start": 2301,
    "end": 2322,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Henceforth, we will refer to this masked language model as the auxiliary model 1 .",
    "curr": "This pre-training task, known as replaced token detection (RTD), has shown great advantages in training and data efficiency compared to other pre-training tasks such as masked language modeling (MLM) (Devlin et al., 2019).",
    "next": "ELECTRAstyle pre-training and its variations have been increasingly popular in advancing natural language understanding capabilities (Meng et al., 2021;Chi et al., 2021;Meng et al., 2022;He et al., 2021;.",
    "query": "Are there any research papers on pre-training tasks like replaced token detection (RTD) and ELECTRA-style methods for improving data efficiency in natural language understanding?"
  },
  {
    "index": 585,
    "source_corpus_id": 238419211,
    "ref_id": "b19",
    "citation_corpus_id": 216867841,
    "start": 2364,
    "end": 2383,
    "title": "Named Entity Recognition without Labelled Data: A Weak Supervision Approach",
    "abstract": "Named Entity Recognition (NER) performance often degrades rapidly when applied to target domains that differ from the texts observed during training. When in-domain labelled data is available, transfer learning techniques can be used to adapt existing NER models to the target domain. But what should one do when there is no hand-labelled data for the target domain? This paper presents a simple but powerful approach to learn NER models in the absence of labelled data through weak supervision. The approach relies on a broad spectrum of labelling functions to automatically annotate texts from the target domain. These annotations are then merged together using a hidden Markov model which captures the varying accuracies and confusions of the labelling functions. A sequence labelling model can finally be trained on the basis of this unified annotation. We evaluate the approach on two English datasets (CoNLL 2003 and news articles from Reuters and Bloomberg) and demonstrate an improvement of about 7 percentage points in entity-level F 1 scores compared to an out-of-domain neural NER model.",
    "prev": "s.Moreover, we provide a theoretically-principled test of the distinguishability of PLRM for unseen labels, along with a generalization bound.On both image and text classification tasks as well as an industrial advertising application, we demonstrate the advantages of PLRM by outperforming baselines by a margin of 2%-9%.",
    "curr": "INTRODUCTION\n\nOne of the greatest bottlenecks of using modern machine learning models is the need for substantial amounts of manually-labeled training data.In real-world applications, such manual annotations are typically time-consuming, labor-intensive and static.To reduce the efforts of annotation, researchers have proposed Weak Supervision (WS) frameworks (Ratner et al., 2016;2018;2019;Fu et al., 2020) for synthesizing labels from multiple weak supervision sources, e.g., heuristics, knowledge bases, or pre-trained classifiers.These frameworks have been widely applied on various machine learning tasks (Dunnmon et al., 2020;Fries et al., 2021;Safranchik et al., 2020;Lison et al., 2020;Zhou et al., 2020;Hooper et al., 2021;Zhan et al., 2019;Varma et al., 2019) and industrial data (Bach et al., 2019).Among them, data programming (Ratner et al., 2016), one representative example that generalizes many approaches in the literature, represents weak supervision sources as labeling functions (LFs) and synthesizes training labels using Probabilistic Graphical Model (PGM).",
    "next": "Given both the increasing popularity of WS and the general increase in open-source availability of machine learning models and tools, there is a rising tide of available supervision sources that WS frameworks and practitioners could potentially leverage, including pre-trained machine learning models or prediction APIs (Chen et al., 2020;d'Andrea & Mintz, 2019;Yao et al., 2017).However, existing WS frameworks only utilize weak supervision sources with the same label space as the target task.This incompatibility largely limits the scope of usable sources, necessitating manual effort from domain experts to pr",
    "query": "Are there any research papers on weak supervision approaches for named entity recognition without labeled data, leveraging labeling functions and probabilistic models to improve performance?"
  },
  {
    "index": 588,
    "source_corpus_id": 248887351,
    "ref_id": "b16",
    "citation_corpus_id": 9192723,
    "start": 13304,
    "end": 13323,
    "title": "Constructing Datasets for Multi-hop Reading Comprehension Across Documents",
    "abstract": "Most Reading Comprehension methods limit themselves to queries which can be answered using a single sentence, paragraph, or document. Enabling models to combine disjoint pieces of textual evidence would extend the scope of machine comprehension methods, but currently there exist no resources to train and test this capability. We propose a novel task to encourage the development of models for text understanding across multiple documents and to investigate the limits of existing methods. In our task, a model learns to seek and combine evidence -effectively performing multi-hop (alias multi-step) inference. We devise a methodology to produce datasets for this task, given a collection of query-answer pairs and thematically linked documents. Two datasets from different domains are induced, 1 and we identify potential pitfalls and devise circumvention strategies. We evaluate two previously proposed competitive models and find that one can integrate information across documents. However, both models struggle to select relevant information, as providing documents guaranteed to be relevant greatly improves their performance. While the models outperform several strong baselines, their best accuracy reaches 42.9% compared to human performance at 74.0% -leaving ample room for improvement.",
    "prev": "To this end, we evaluated LLMs from the Gopher family in a 5-shot 1 setting on a larger set of 50 tasks that touch on different aspects of logical reasoning and vary in terms of the number of reasoning steps required, presence or absence of negation, whether the relevant context information was provided, and whether the model is required to evaluate the accuracy of multiple choices or generate the answer among others.",
    "curr": "The additional tasks were collected from six sources: bAbI (Weston et al., 2015), BigBench (Ghazal et al., 2017), AAC (Betz et al., 2020), Jeopardy (Tunguz, 2019), Proof Writer (Tafjord et al., 2020) and 2WikiMultiHop (Welbl et al., 2018) (see Fig.",
    "next": "S5a in Supplementary Information for raw results).",
    "query": "Are there any research papers on constructing datasets for multi-hop reading comprehension across multiple documents and evaluating models' ability to integrate information from disjoint textual evidence?"
  },
  {
    "index": 590,
    "source_corpus_id": 263310823,
    "ref_id": "b3",
    "citation_corpus_id": 225039882,
    "start": 25972,
    "end": 25973,
    "title": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.RELATED WORKTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.",
    "prev": "t is located with respect to the rest of the training set.This allows us to answer the question: Which data record S has the most significant influence and at which query Q?",
    "curr": "Proposition 4.1 (First-order optimality condition for influence).Consider a dataset D, a differing point S, and a kernel function K satisfying the same conditions of Theorem 3.1.Then, we have that\n∇ Q M (Q) | Q=S = −α −2 (1 − K SD M −1 D K DS )(y S − K SD M −1 D y D ) 2 KSD M −1 D K DS , where M (Q) is the mean distance LOOD in C.1, K SD , K DS , K DD are kernel matrices as defined in Section 2, KSD = ∂ ∂Q K QD | Q=S , M D = K DD + σ 2 I, and α = 1 − K SD M −1 D K DS + σ 2 .",
    "next": "Our first observation from Proposition 4.1 is that when S is far from the dataset D in the sense that K SD ≈ 0, the mean distance LOOD gradient ∇ Q M (Q) | Q=S is close to 0, i.e., the differing data S is a stationary point for the mean distance LOOD objective.We also analyze the second-order Hessian condition of mean distance LOOD in Lemma G. 4, and prove that the differing data S has locally maximal influence, as long as S far from the remaining training dataset D, and the data labels are bounded (which is the case in a classification task).In the opposite case where S is close to D in the sense that y S ≈ K SD M −1 D y D (i.e., S is perfectly predicted by the GP trained on remaining dataset D), then the gradient\n∇ Q M (Q) | Q=S of mean distance LOOD is also close to zero.",
    "query": "Are there any research papers on methods for analyzing data influence in machine learning using kernel functions and gradient-based approaches?"
  },
  {
    "index": 591,
    "source_corpus_id": 3524184,
    "ref_id": "b2",
    "citation_corpus_id": 11383178,
    "start": 7570,
    "end": 7590,
    "title": "IMPORTANCE WEIGHTED AUTOENCODERS",
    "abstract": "The variational autoencoder (VAE; ) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.",
    "prev": "(2015)).",
    "curr": "MARGINAL LOG-LIKELIHOOD ESTIMATION\n\nWe use two bounds to estimate the marginal log-likelihood of a model: IWAE (Burda et al., 2016) and AIS (Neal, 2001).",
    "next": "Here we describe the IWAE bound.",
    "query": "Are there any research papers on methods for improving variational autoencoders using importance weighting and marginal log-likelihood estimation?"
  },
  {
    "index": 593,
    "source_corpus_id": 237292758,
    "ref_id": "b24",
    "citation_corpus_id": 6628106,
    "start": 1545,
    "end": 1565,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "This suggests that the inferior generalization performance of Adam is fundamentally tied to the nonconvex landscape of deep learning optimization.",
    "curr": "Introduction\n\nAdaptive gradient methods (Duchi et al., 2011;Hinton et al., 2012;Kingma and Ba, 2015;Reddi et al., 2018) such as Adam are very popular optimizers for training deep neural networks.",
    "next": "By adjusting the learning rate coordinate-wisely based on historical gradient information, they are known to be able to automatically choose appropriate learning rates to achieve fast convergence in training.",
    "query": "Are there any studies on adaptive gradient optimization methods like Adam for stochastic optimization in deep learning?"
  },
  {
    "index": 595,
    "source_corpus_id": 254408767,
    "ref_id": "b33",
    "citation_corpus_id": 239768373,
    "start": 33117,
    "end": 33135,
    "title": "Published as a conference paper at ICLR 2022 LEARNING WITH NOISY LABELS REVISITED: A STUDY USING REAL-WORLD HUMAN ANNOTATIONS",
    "abstract": "Existing research on learning with noisy labels mainly focuses on synthetic label noise. The synthetic noise, though has clean structures which greatly enabled statistical analyses, often fails to model the real-world noise patterns. The recent literature has observed several efforts to offer real-world noisy datasets, e.g., Food-101N, WebVision, and Clothing1M. Yet the existing efforts suffer from two caveats: firstly, the lack of ground-truth verification makes it hard to theoretically study the property and treatment of real-world label noise. Secondly, these efforts are often of large scales, which may result in unfair comparisons of robust methods within reasonable and accessible computation power. To better understand real-world label noise, it is important to establish controllable, easy-to-use and moderate-sized real-world noisy datasets with both ground-truth and noisy labels. This work presents two new benchmark datasets, which we name as CIFAR-10N, CIFAR-100N (jointly we call them CIFAR-N), equipping the training datasets of CIFAR-10 and CIFAR-100 with human-annotated real-world noisy labels we collected from Amazon Mechanical Turk. We quantitatively and qualitatively show that realworld noisy labels follow an instance-dependent pattern rather than the classically assumed and adopted ones (e.g., class-dependent label noise). We then initiate an effort to benchmarking a subset of the existing solutions using CIFAR-10N and CIFAR-100N. We further proceed to study the memorization of correct and wrong predictions, which further illustrates the difference between human noise and class-dependent synthetic noise. We show indeed the real-world noise patterns impose new and outstanding challenges as compared to synthetic label noise. These observations require us to rethink the treatment of noisy labels, and we hope the availability of these two datasets would facilitate the development and evaluation of future learning with noisy label solutions. The corresponding datasets and the leaderboard are available at http://noisylabels.com.",
    "prev": "We can observe that models in Region 1 have the highest test accuracies in each dataset.",
    "curr": "Computation (a) The Clothing-1M dataset (b) The Animal-10N dataset (c) The CIFAR-10N dataset Selection (d) The Clothing-1M dataset (e) The Animal-10N dataset (f) The CIFAR-10N dataset Evaluation (g) The Clothing-1M dataset (h) The Animal-10N dataset (i) The CIFAR-10N dataset\n\nExperiments on Real-world Datasets with noisy labels\n\nTo evaluate the performance of our approach on real-world datasets, we have conducted additional experiments on the Clothing-1M dataset [Xiao et al., 2015], which is a dataset with 1M images of clothes, on the Animal-10N dataset [Song et al., 2019], which is a dataset with 50k images of animals and on the CIFAR-10N dataset [Wei et al., 2022], which is the CIFAR-10 dataset with human-annotated noisy labels obtained from Amazon Mechanical Turk.",
    "next": "In the Clothing-1M dataset, the images have been labeled from the texts that accompany them, hence there are both clean and noisy labels in the set, and in the Animal-10N dataset, the images have been gathered and labeled from search engines.",
    "query": "Are there any research papers on benchmark datasets for learning with real-world noisy labels using human annotations and their impact on machine learning models?"
  },
  {
    "index": 597,
    "source_corpus_id": 256358777,
    "ref_id": "b13",
    "citation_corpus_id": 3144218,
    "start": 1370,
    "end": 1392,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "These new theoretical results are supported by corresponding numerical investigations.",
    "curr": "INTRODUCTION\n\nGraph Convolutional Networks (GCNs) (Kipf & Welling, 2017;Hammond et al., 2011;Defferrard et al., 2016) generalize Euclidean convolutional networks to the graph setting by replacing convolutional filters by functional calculus filters; i.e.",
    "next": "scalar functions applied to a suitably chosen graph-shift-oprator capturing the geometry of the underlying graph.",
    "query": "Are there any studies that explore scalable methods for semi-supervised learning on graph-structured data using graph convolutional networks?"
  },
  {
    "index": 598,
    "source_corpus_id": 252683295,
    "ref_id": "b3",
    "citation_corpus_id": 247475874,
    "start": 2131,
    "end": 2150,
    "title": "E-KAR : A Benchmark for Rationalizing Natural Language Analogical Reasoning",
    "abstract": "The ability to recognize analogies is fundamental to human cognition. Existing benchmarks to test word analogy do not reveal the underneath process of analogical reasoning of neural models. Holding the belief that models capable of reasoning should be right for the right reasons, we propose a first-of-its",
    "prev": "1 Code and datasets are available in https://github.com/zjunlp/MKG_Analogy.",
    "curr": "INTRODUCTION\n\nAnalogical reasoning -the ability to perceive and use relational similarity between two situations or events -holds an important place in human cognition (Johnson-Laird, 2006;Bengio et al., 2021;Chen et al., 2022a) and can provide back-end support for various fields such as education (Thagard, 1992), creativity (Goel, 1997), thus appealing to the AI community.",
    "next": "Early, Mikolov et al.",
    "query": "Are there any research papers on benchmarks for evaluating the rationalization of natural language analogical reasoning in AI models?"
  },
  {
    "index": 599,
    "source_corpus_id": 53514358,
    "ref_id": "b2",
    "citation_corpus_id": 30745030,
    "start": 2650,
    "end": 2672,
    "title": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data",
    "abstract": "Neural networks exhibit good generalization behavior in the over-parameterized regime, where the number of network parameters exceeds the number of observations. Nonetheless, current generalization bounds for neural networks fail to explain this phenomenon. In an attempt to bridge this gap, we study the problem of learning a two-layer over-parameterized neural network, when the data is generated by a linearly separable function. In the case where the network has Leaky ReLU activations, we provide both optimization and generalization guarantees for overparameterized networks. Specifically, we prove convergence rates of SGD to a global minimum and provide generalization guarantees for this global minimum that are independent of the network size. Therefore, our result clearly shows that the use of SGD for optimization both finds a global minimum, and avoids overfitting despite the high capacity of the model. This is the first theoretical demonstration that SGD can avoid overfitting, when learning over-specified neural network classifiers.",
    "prev": "Yet these models have achieved state-ofthe-art generalization error on many real-world tasks.",
    "curr": "This observation has spurred an active line of research (Soudry et al., 2018;Brutzkus et al., 2018;Li & Liang, 2018) that has tried to understand what properties are possessed by stochastic gradient descent (SGD) training of deep networks that allows these networks to generalize well.",
    "next": "One particularly promising line of work in this area (Neyshabur et al., 2017;Arora et al., 2018) has been bounds that utilize the noise-resilience of deep networks on training data i.e., how much the training loss of the network changes with noise injected into the parameters, or roughly, how wide is the training loss minimum.",
    "query": "Are there any research papers on the generalization of over-parameterized neural networks using stochastic gradient descent, focusing on theoretical guarantees and noise-resilience properties in linearly separable data settings?"
  },
  {
    "index": 600,
    "source_corpus_id": 244714571,
    "ref_id": "b15",
    "citation_corpus_id": 52920808,
    "start": 17523,
    "end": 17540,
    "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks",
    "abstract": "One of the mystery in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an m hidden node shallow neural network with ReLU activation and n training data, we show as long as m is large enough and the data is non-degenerate, randomly initialized gradient descent converges a globally optimal solution with a linear convergence rate for the quadratic loss function.Our analysis is based on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods. * Equal Contribution.",
    "prev": "The first part of Assumption 4.1 assures that H is non-singular and that the input data lies in the unit sphere S d−1 .",
    "curr": "Such assumption is commonly made in overparameterized neural network literature (Arora et al., 2019;Du et al., 2019b;a;Cao & Gu, 2019).",
    "next": "The non-singularity is satisfied when e.g.",
    "query": "Are there any research papers on the convergence of gradient descent in over-parameterized neural networks under non-singularity assumptions?"
  },
  {
    "index": 602,
    "source_corpus_id": 221041408,
    "ref_id": "b2",
    "citation_corpus_id": 213152193,
    "start": 9171,
    "end": 9191,
    "title": "ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS",
    "abstract": "Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK]  and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.",
    "prev": "(2019) observes that parameters in different layers have similar attention distribution, and propose a parameter distillation method from shallow layers to deep layers.",
    "curr": "Another notable work is ELECTRA (Clark et al., 2019), which develops a new task using one discriminator and one generator.",
    "next": "The generator corrupts the sentence, and the discriminator is trained to predict whether each word in the corrupted sentence is replaced or not.",
    "query": "Are there any research papers on pre-training text encoders using a discriminator and generator for replaced token detection instead of masked language modeling?"
  },
  {
    "index": 603,
    "source_corpus_id": 247476419,
    "ref_id": "b8",
    "citation_corpus_id": 49667762,
    "start": 2546,
    "end": 2568,
    "title": "UNIVERSAL TRANSFORMERS",
    "abstract": "Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset. * Equal contribution, alphabetically by last name. † Work performed while at Google Brain.",
    "prev": "One solution is cross-layer parameter sharing, which reduces the memory needed to store parameters, which can also reduce the cost of communicating model updates in distributed training (Lan et al., 2020;Jaegle et al., 2021) and federated learning (Konečný et al., 2016;McMahan et al., 2017), as the model is smaller, and can help avoid overfitting (Jaegle et al., 2021).",
    "curr": "However, prior work in parameter sharing (e.g., Dehghani et al., 2019;Savarese & Maire, 2019;Lan et al., 2020;Jaegle et al., 2021) has two significant limitations.",
    "next": "First, they rely on suboptimal hand-crafted techniques for deciding where and how sharing occurs.",
    "query": "Are there any research papers on models that combine recurrent and feed-forward architectures for improved sequence modeling, such as the Universal Transformer?"
  },
  {
    "index": 604,
    "source_corpus_id": 247597138,
    "ref_id": "b0",
    "citation_corpus_id": 61153666,
    "start": 6808,
    "end": 6813,
    "title": "SECTOR: A Neural Model for Coherent Topic Segmentation and Classification",
    "abstract": "When searching for information, a human reader first glances over a document, spots relevant sections, and then focuses on a few sentences for resolving her intention. However, the high variance of document structure complicates the identification of the salient topic of a given section at a glance. To tackle this challenge, we present SECTOR, a model to support machine reading systems by segmenting documents into coherent sections and assigning topic labels to each section. Our deep neural network architecture learns a latent topic embedding over the course of a document. This can be leveraged to classify local topics from plain text and segment a document at topic shifts. In addition, we contribute WikiSection, a publicly available data set with 242k labeled sections in English and German from two distinct domains: diseases and cities. From our extensive evaluation of 20 architectures, we report a highest score of 71.6% F1 for the segmentation and classification of 30 topics from the English city domain, scored by our SECTOR long short-term memory model with Bloom filter embeddings and bidirectional segmentation. This is a significant improvement of 29.5 points F1 over state-of-the-art CNN classifiers with baseline segmentation. 1 Our source code is available under the Apache License 2.0 at https://github.com/sebastianarnold/ SECTOR.",
    "prev": "Forcing autoregressive models to generate longer texts exacerbates this incoherence because the models struggle to extrapolate beyond their expected text end point.",
    "curr": "Prior work has tried to address the problem of generating globally coherent text with planning-based approaches (Puduppully et al., 2019;Moryossef et al., 2019;Fan et al., 2019;Kiddon et al., 2016).",
    "next": "However, planning-based approaches rely on domain-specific heuristics for capturing text structure and dynamics.",
    "query": "Are there any research papers on neural models for coherent topic segmentation and classification in document processing?"
  },
  {
    "index": 605,
    "source_corpus_id": 252683988,
    "ref_id": "b44",
    "citation_corpus_id": 23892230,
    "start": 2156,
    "end": 2178,
    "title": "Challenges in Data-to-Document Generation",
    "abstract": "Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate humangenerated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy-and reconstructionbased extensions lead to noticeable improvements.",
    "prev": "With SLiC, we exceed or match SOTA results on a wide range of generation tasks spanning abstractive summarization, question generation, abstractive question answering and data-to-text generation, even with modest-sized models.",
    "curr": "INTRODUCTION\n\nConditional language generation aims to generate natural language text based on input context, and includes many useful and hard tasks such as abstractive summarization (Mani, 2001;Nenkova and McKeown, 2011), generative question answering (Bajaj et al., 2016), question generation (Zhou et al., 2017) and data-to-text (Wiseman et al., 2017;Gardent et al., 2017) tasks.",
    "next": "Pretraining large Transformer encoder-decoder models and fine-tuning them on downstream tasks is the common paradigm to address these tasks (Raffel et al., 2020;Lewis et al., 2019;Tay et al., 2022;Zhang et al., 2019a).",
    "query": "Are there any studies on challenges in data-to-text generation and the effectiveness of neural models compared to templated baselines?"
  },
  {
    "index": 606,
    "source_corpus_id": 263608698,
    "ref_id": "b8",
    "citation_corpus_id": 215717103,
    "start": 6356,
    "end": 6375,
    "title": "Collecting Highly Parallel Data for Paraphrase Evaluation",
    "abstract": "A lack of standard datasets and evaluation metrics has prevented the field of paraphrasing from making the kind of rapid progress enjoyed by the machine translation community over the last 15 years. We address both problems by presenting a novel data collection framework that produces highly parallel text data relatively inexpensively and on a large scale. The highly parallel nature of this data allows us to use simple n-gram comparisons to measure both the semantic adequacy and lexical dissimilarity of paraphrase candidates. In addition to being simple and efficient to compute, experiments show that these metrics correlate highly with human judgments.",
    "prev": "age), and AL (audio-language) data pairs.The videos of previous datasets are always truncated segments from long videos (Miech et al., 2019;Xue et al., 2022), resulting in fragmented semantics.To avoid this problem, we construct our video-text pairs from short videos with complete stories.To ensure the quality of the central language modality, we perform multi-view text generation and enhancement on VIDAL-10M.",
    "curr": "The proposed LanguageBind ensures that we can extend vision-language to multiple (N) modalities, and our dataset VIDAL-10M benefits more downstream tasks beyond VL tasks, including video retrieval (Luo et al., 2022), depth classification (Cao et al., 2017), infrared classification (Baffa & Lattari, 2018) and audio classification (Palanisamy et al., 2020).In text-to-video zero-shot retrieval based on CLIP-Large, LanguageBind achieves state-of-the-art (SOTA) performance on four datasets, surpassing InterVideo (Wang et al., 2022c) by 1.9% on MSR-VTT (Xu et al., 2016), 8.8% on MSVD (Chen & Dolan, 2011), 6.3% on DiDeMo (Anne Hendricks et al., 2017), and 4.4% on ActivityNet (Caba Heilbron et al., 2015).For zero-shot classification on depth and infrared data, LanguageBind achieves a substantial performance advantage over ImageBind.LanguageBind attains top-1 accuracy of 87.2% and 65.1% on LLVIP and NYU-D, respectively, outperforming Im-ageBind by 23.8% and 11.1%.For zero-shot audio classification tasks, LanguageBind outperforms ImageBind with a 22.9% higher top-1 accuracy on the ESC50 dataset.",
    "next": "We summarize our primary contributions as follows:\n\n• We propose LanguageBind, the langauge-based multi-modal pretraining approach.During the pretraining process, all modalities gradually align with the language modality through contrastive learning, and these modalities are unified within a shared embedding space.",
    "query": "Are there any research papers on multi-modal pretraining approaches that use contrastive learning to align multiple modalities with language, achieving state-of-the-art performance on downstream tasks such as video retrieval, depth classification, and audio classification?"
  },
  {
    "index": 608,
    "source_corpus_id": 247595075,
    "ref_id": "b21",
    "citation_corpus_id": 4606753,
    "start": 2916,
    "end": 2937,
    "title": "NEURAL-GUIDED DEDUCTIVE SEARCH FOR REAL- TIME PROGRAM SYNTHESIS FROM EXAMPLES",
    "abstract": "Synthesizing user-intended programs from a small number of input-output examples is a challenging problem with several important applications like spreadsheet manipulation, data wrangling and code refactoring. Existing synthesis systems either completely rely on deductive logic techniques that are extensively handengineered or on purely statistical models that need massive amounts of data, and in general fail to provide real-time synthesis on challenging benchmarks. In this work, we propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique that combines the best of both symbolic logic techniques and statistical models. Thus, it produces programs that satisfy the provided specifications by construction and generalize well on unseen examples, similar to data-driven systems. Our technique effectively utilizes the deductive search framework to reduce the learning problem of the neural component to a simple supervised learning setup. Further, this allows us to both train on sparingly available real-world data and still leverage powerful recurrent neural network encoders. We demonstrate the effectiveness of our method by evaluating on real-world customer scenarios by synthesizing accurate programs with up to 12× speed-up compared to state-ofthe-art systems.",
    "prev": "First, learning to search works best when it exploits the symbolic scaffolding of existing search algorithms already proven useful for the problem domain.",
    "curr": "For example, AlphaGo exploits Monte Carlo Tree Search (Silver et al., 2016), while NGDS exploits top-down deductive search (Kalyan et al., 2018).",
    "next": "We engineer CROSSBEAM around bottom-up enumerative search (Udupa et al., 2013), a backbone of several successful recent program synthesis algorithms (Shi et al., 2020a;Odena et al., 2021;Barke et al., 2020).",
    "query": "Are there any research papers on hybrid approaches combining neural networks with deductive search techniques for real-time program synthesis from examples?"
  },
  {
    "index": 610,
    "source_corpus_id": 245769552,
    "ref_id": "b20",
    "citation_corpus_id": 52967399,
    "start": 9232,
    "end": 9253,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Clusters of signalprocessing-based acoustic features, e.g., Mel-frequency cepstral coefficients (MFCC), exhibit nontrivial correlations with the inherent acoustic units of speech inputs.",
    "curr": "Using (A 1:T , z a 1:T ) pairs, the second step learns new feature representations by minimizing a masked prediction loss, similar to masked language modeling in BERT (Devlin et al., 2019).",
    "next": "The pressure to predict cluster assignments of masked audio regions forces the model to learn good local acoustic representations for unmasked regions and long-range temporal dependencies between latent features.",
    "query": "Are there any research papers on applying masked prediction loss and transformer-based models to learn acoustic feature representations from audio data?"
  },
  {
    "index": 612,
    "source_corpus_id": 252967802,
    "ref_id": "b9",
    "citation_corpus_id": 257232596,
    "start": 5074,
    "end": 5095,
    "title": "Published as a conference paper at ICLR 2023 FROM t-SNE TO UMAP WITH CONTRASTIVE LEARNING",
    "abstract": "Neighbor embedding methods t-SNE and UMAP are the de facto standard for visualizing high-dimensional datasets. Motivated from entirely different viewpoints, their loss functions appear to be unrelated. In practice, they yield strongly differing embeddings and can suggest conflicting interpretations of the same data. The fundamental reasons for this and, more generally, the exact relationship between t-SNE and UMAP have remained unclear. In this work, we uncover their conceptual connection via a new insight into contrastive learning methods. Noisecontrastive estimation can be used to optimize t-SNE, while UMAP relies on negative sampling, another contrastive method. We find the precise relationship between these two contrastive methods and provide a mathematical characterization of the distortion introduced by negative sampling. Visually, this distortion results in UMAP generating more compact embeddings with tighter clusters compared to t-SNE. We exploit this new conceptual connection to propose and implement a generalization of negative sampling, allowing us to interpolate between (and even extrapolate beyond) t-SNE and UMAP and their respective embeddings. Moving along this spectrum of embeddings leads to a trade-off between discrete / local and continuous / global structures, mitigating the risk of over-interpreting ostensible features of any single embedding. We provide a PyTorch implementation.",
    "prev": "A number of algorithms based on that framework, such as LargeVis (Tang et al., 2016), UMAP (McInnes et al., 2018), and TriMap (Amid & Warmuth, 2019) have been developed and got widespread adoption in recent years in a variety of application fields.",
    "curr": "All of them are closely related to SNE (Böhm et al., 2022;Damrich et al., 2023) and rely on the kNN graph of the data.",
    "next": "NE algorithms have been used to visualize latent representations of neural networks trained in a supervised setting (e.g.",
    "query": "Are there any research papers on the relationship between t-SNE and UMAP through contrastive learning methods such as noise-contrastive estimation and negative sampling?"
  },
  {
    "index": 613,
    "source_corpus_id": 52877454,
    "ref_id": "b2",
    "citation_corpus_id": 4630420,
    "start": 6172,
    "end": 6201,
    "title": "DEEP GAUSSIAN EMBEDDING OF GRAPHS: UNSUPERVISED INDUCTIVE LEARNING VIA RANKING",
    "abstract": "Methods that learn representations of nodes in a graph play a critical role in network analysis since they enable many downstream learning tasks. We propose Graph2Gauss -an approach that can efficiently learn versatile node embeddings on large scale (attributed) graphs that show strong performance on tasks such as link prediction and node classification. Unlike most approaches that represent nodes as point vectors in a low-dimensional continuous space, we embed each node as a Gaussian distribution, allowing us to capture uncertainty about the representation. Furthermore, we propose an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs: plain/attributed, directed/undirected. By leveraging both the network structure and the associated node attributes, we are able to generalize to unseen nodes without additional training. To learn the embeddings we adopt a personalized ranking formulation w.r.t. the node distances that exploits the natural ordering of the nodes imposed by the network structure. Experiments on real world networks demonstrate the high performance of our approach, outperforming state-of-the-art network embedding methods on several different tasks. Additionally, we demonstrate the benefits of modeling uncertainty -by analyzing it we can estimate neighborhood diversity and detect the intrinsic latent dimensionality of a graph.",
    "prev": "Contrastive methods are central to many popular word-embedding methods (Collobert & Weston, 2008;Mnih & Kavukcuoglu, 2013;Mikolov et al., 2013), but they are found in many unsupervised algorithms for learning representations of graphstructured input as well.",
    "curr": "There are many ways to score a representation, but in the graph literature the most common techniques use classification (Perozzi et al., 2014;Grover & Leskovec, 2016;Kipf & Welling, 2016b;Hamilton et al., 2017b), though other scoring functions are used (Duran & Niepert, 2017;Bojchevski & Günnemann, 2018).",
    "next": "DGI is also contrastive in this respect, as our objective is based on classifying local-global pairs and negative-sampled counterparts.",
    "query": "Are there any research papers on unsupervised graph embedding methods that use contrastive learning or ranking-based approaches for inductive learning scenarios?"
  },
  {
    "index": 615,
    "source_corpus_id": 209832425,
    "ref_id": "b8",
    "citation_corpus_id": 54443381,
    "start": 5528,
    "end": 5551,
    "title": "EFFICIENT LIFELONG LEARNING WITH A-GEM",
    "abstract": "In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC  and other regularizationbased methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency. 1",
    "prev": "Regularization and replay approaches fix the model architecture before training and prevent catastrophic forgetting by regularizing the change of a specific set of weights or replaying previously learned data.",
    "curr": "Hybrids of replay and regularization also exist, such as Gradient Episodic Memory (GEM) (Lopez-Paz & Ranzato, 2017;Chaudhry et al., 2019a).",
    "next": "On the other hand, methods based on expansion add new network components to learn new data.",
    "query": "Are there any research papers on efficient lifelong learning methods that combine replay and regularization techniques, such as A-GEM?"
  },
  {
    "index": 616,
    "source_corpus_id": 13019454,
    "ref_id": "b17",
    "citation_corpus_id": 629094,
    "start": 10109,
    "end": 10130,
    "title": "Word representations: A simple and general method for semi-supervised learning",
    "abstract": "If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize. com/projects/wordreprs/",
    "prev": "E x ∈ R |V |×E is the embedding matrix, where |V | is the number of unique events (the vocabulary size) and E is the embedding dimension.",
    "curr": "The use of embedding provides a dense representation for an event that improves learning (Turian et al., 2010).",
    "next": "Through training, the embedding vector of an event encodes its meaning relative to other events.",
    "query": "Are there any research papers on methods for improving semi-supervised learning in NLP tasks through the use of unsupervised word representations as additional features?"
  },
  {
    "index": 621,
    "source_corpus_id": 258999337,
    "ref_id": "b16",
    "citation_corpus_id": 6628106,
    "start": 32876,
    "end": 32895,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "o the points within its intermediate predicted mask from the previous layer.Then in the click-to-click attention module, each click query self-attends to each other to realize inter-query communications (Eq.2).To make the point features click-aware, we let the point features cross-attend to the click queries in a scene-toclick attention module (Eq.3).In equations ( 1) to (3), we omit the layer normalization and dropout for simplicity.W Q , W K , W V are learnable weights for query, key and value as in the standard attention mechanism (Vaswani et al., 2017).In all attention modules, we add the positional part to their respective keys/queries.",
    "curr": "Q l+1 c = softmax W Q (Q l c + Q p ) • W K (F l c + F p ) + H √ D • W V F l c + Q l c\n(1)\nQ l+1 c = softmax W Q (Q l c + Q p ) • W K (Q l c + Q p √ D • W V Q l c + Q l c\n(2)    Training.We set the λ CE = 1 and the λ Dice = 2 in the loss function.The loss is applied to every intermediate layer of the click attention module.We use the Adam optimizer (Kingma & Ba, 2015) with a weight decay factor 1e-4.We train the model on ScanNet40 for 1100 epochs with an initial learning rate 1e-4, which is decayed by 0.1 after 1000 epochs.Due to the smaller data size, we train the model on ScanNet20 for 850 epochs with an initial learning rate 1e-4, which is decayed by 0.1 after 800 epochs.We use a single TITAN RTX GPU with 24GB memory for training.",
    "next": "F l+1 c = softmax W Q (F l c + F p ) • W K (Q l c + Q p ) √ D • W V Q l c + F l c(3)\n\nB ADDITIONAL RESULTS\n\nB.1 ABLATION ON QUERY FUSION STRATEGY In our query fusion module, we apply a per-point max operation to aggregate click-specific masks to region-specific masks (Fig.",
    "query": "Are there any research papers on attention mechanisms for point cloud processing using click-to-click and scene-to-click modules with cross-attention and self-attention operations?"
  },
  {
    "index": 622,
    "source_corpus_id": 225076227,
    "ref_id": "b24",
    "citation_corpus_id": 52901777,
    "start": 7946,
    "end": 7968,
    "title": "Bias-Reduced Uncertainty Estimation for Deep Neural Classifiers",
    "abstract": "We consider the problem of uncertainty estimation in the context of (non-Bayesian) deep neural classification. In this context, all known methods are based on extracting uncertainty signals from a trained network optimized to solve the classification problem at hand. We demonstrate that such techniques tend to introduce biased estimates for instances whose predictions are supposed to be highly confident. We argue that this deficiency is an artifact of the dynamics of training with SGD-like optimizers, and it has some properties similar to overfitting. Based on this observation, we develop an uncertainty estimation algorithm that selectively estimates the uncertainty of highly confident points, using earlier snapshots of the trained model, before their estimates are jittered (and way before they are ready for actual classification). We present extensive experiments indicating that the proposed algorithm provides uncertainty estimates that are consistently better than all known methods.",
    "prev": "RELATED WORK\n\nSelective classification.",
    "curr": "Abstaining from uncertain predictions is a classic idea (Chow, 1957;Hellman, 1970), and uncertainty estimation is an active area of research, from the popular approach of using softmax probabilities (Geifman & El-Yaniv, 2017) to more sophisticated methods using dropout (Gal & Ghahramani, 2016), ensembles (Lakshminarayanan et al., 2017), or training snapshots (Geifman et al., 2018).",
    "next": "Others incorporate abstention into model training (Bartlett & Wegkamp, 2008;Geifman & El-Yaniv, 2019;Feng et al., 2019) and learn to abstain on examples human experts are more likely to get correct (Raghu et al., 2019;Mozannar & Sontag, 2020;De et al., 2020).",
    "query": "Are there any research papers on bias-reduced uncertainty estimation methods for deep neural classifiers?"
  },
  {
    "index": 626,
    "source_corpus_id": 3292002,
    "ref_id": "b23",
    "citation_corpus_id": 3144218,
    "start": 5643,
    "end": 5664,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "(2016) proposed to approximate the filters by means of a Chebyshev expansion of the graph Laplacian, removing the need to compute the eigenvectors of the Laplacian and yielding spatially localized filters.",
    "curr": "Finally, Kipf & Welling (2017) simplified the previous method by restricting the filters to operate in a 1-step neighborhood around each node.",
    "next": "However, in all of the aforementioned spectral approaches, the learned filters depend on the Laplacian eigenbasis, which depends on the graph structure.",
    "query": "Are there any research papers on graph convolutional networks for semi-supervised classification that use approximations of spectral graph convolutions, such as Chebyshev expansions or localized filters, to avoid dependence on the Laplacian eigenbasis?"
  },
  {
    "index": 627,
    "source_corpus_id": 202888950,
    "ref_id": "b16",
    "citation_corpus_id": 3507990,
    "start": 21092,
    "end": 21110,
    "title": "Published as a conference paper at ICLR 2018 META-LEARNING FOR SEMI-SUPERVISED FEW-SHOT CLASSIFICATION",
    "abstract": "In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples. Recent progress in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set. In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode. We consider two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more challenging situation where examples from other distractor classes are also provided. To address this paradigm, we propose novel extensions of Prototypical Networks (Snell et al., 2017) that are augmented with the ability to use unlabeled examples when producing prototypes. These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully. We evaluate these methods on versions of the Omniglot and miniImageNet benchmarks, adapted to this new framework augmented with unlabeled examples. We also propose a new split of ImageNet, consisting of a large set of classes, with a hierarchical structure. Our experiments confirm that our Prototypical Networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would. * Equal contribution. 1 See the following blog post for an overview:",
    "prev": "Detailed description of the architecture can be found in the appendix.",
    "curr": "Experiments are performed on three data sets: Omniglot (Lake et al., 2015), miniImageNet (Vinyals et al., 2016), and tieredImageNet (Ren et al., 2018).",
    "next": "We followed standard data processing procedures which are detailed in the appendix.",
    "query": "Are there any research papers on meta-learning approaches for semi-supervised few-shot classification using unlabeled examples and Prototypical Networks?"
  },
  {
    "index": 628,
    "source_corpus_id": 91175758,
    "ref_id": "b34",
    "citation_corpus_id": 13751870,
    "start": 23320,
    "end": 23332,
    "title": "A Call for Clarity in Reporting BLEU Scores",
    "abstract": "The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to \"the\" BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for usersupplied reference processing, and provide a new tool, SACREBLEU, 1 to facilitate this. . 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. ArXiv eprints, abs/1609.08144.",
    "prev": "If implementation details can cause the scores produced by a metric to vary significantly, this conflates comparison of when different implementations are used to compare different methods.",
    "curr": "This has caused issues in benchmarking machine translation (Post, 2018) and music information retrieval (Raffel et al., 2014).",
    "next": "This problem is particularly pronounced for NNDs because they require implementing a neural network architecture and training scheme, and the use of different software frameworks or even driver versions can cause results to vary (Henderson et al., 2018;Oliver et al., 2018).",
    "query": "Are there any research papers on addressing inconsistencies in BLEU score reporting caused by varying implementation details in machine translation?"
  },
  {
    "index": 629,
    "source_corpus_id": 257405483,
    "ref_id": "b6",
    "citation_corpus_id": 52967399,
    "start": 1699,
    "end": 1720,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "INTRODUCTION\n\nHyperparameter optimization (HPO) and neural architecture search (NAS) yield state-of-the-art models, but often are a very costly endeavor, especially when working with large datasets and models.",
    "curr": "For example, using the results of (Sharir et al., 2020) we can estimate that evaluating 50 configurations for a 340-million-parameter BERT model (Devlin et al., 2019) on the 15GB Wikipedia and Book corpora would cost around $500,000.",
    "next": "To make HPO and NAS more efficient, researchers explored how we can learn from cheaper evaluations (e.g.",
    "query": "Are there any research papers on methods to reduce the computational cost of hyperparameter optimization and neural architecture search for large-scale machine learning models using cheaper evaluations?"
  },
  {
    "index": 630,
    "source_corpus_id": 262828485,
    "ref_id": "b13",
    "citation_corpus_id": 246241126,
    "start": 5613,
    "end": 5630,
    "title": "Published as a conference paper at ICLR 2022 LEARNING TO COMPLETE CODE WITH SKETCHES",
    "abstract": "Code completion is usually cast as a language modelling problem, i.e., continuing an input in a left-to-right fashion. However, in practice, some parts of the completion (e.g., string literals) may be very hard to predict, whereas subsequent parts directly follow from the context. To handle this, we instead consider the scenario of generating code completions with \"holes\" inserted in places where a model is uncertain. We develop GRAMMFORMER, a Transformer-based model that guides code generation by the programming language grammar, and compare it to a variety of more standard sequence models. We train the models on code completion for C# and Python given partial code context. To evaluate models, we consider both ROUGE as well as a new metric REGEXACC that measures success of generating completions matching long outputs with as few holes as possible. In our experiments, GRAMMFORMER generates 10-50% more accurate completions compared to traditional generative models and 37-50% longer sketches compared to sketch-generating baselines trained with similar techniques.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on code completion techniques that utilize grammar-guided models and incorporate sketch-based methods with holes to enhance accuracy?"
  },
  {
    "index": 632,
    "source_corpus_id": 204905143,
    "ref_id": "b16",
    "citation_corpus_id": 3633127,
    "start": 1617,
    "end": 1639,
    "title": "Published as a conference paper at ICLR 2018 CGANS WITH PROJECTION DISCRIMINATOR",
    "abstract": "We propose a novel, projection based way to incorporate the conditional information into the discriminator of GANs that respects the role of the conditional information in the underlining probabilistic model. This approach is in contrast with most frameworks of conditional GANs used in application today, which use the conditional information by concatenating the (embedded) conditional vector to the feature vectors. With this modification, we were able to significantly improve the quality of the class conditional image generation on ILSVRC2012 (Im-ageNet) 1000-class image dataset from the current state-of-the-art result, and we achieved this with a single pair of a discriminator and a generator. We were also able to extend the application to super-resolution and succeeded in producing highly discriminative super-resolution images. This new structure also enabled high quality category transformation based on parametric functional transformation of conditional batch normalization layers in the generator. The code with Chainer (Tokui et al., 2015), generated images and pretrained models are available at https://github.com/pfnet-research/sngan_projection.",
    "prev": "Moreover, Our consistency regularized GAN (CR-GAN) improves stateof-the-art FID scores for conditional generation from 14.73 to 11.67 on CIFAR-10 and from 8.73 to 6.66 on ImageNet-2012.",
    "curr": "INTRODUCTION\n\nGenerative Adversarial Networks (GANs) (Goodfellow et al., 2014) have recently demonstrated impressive results on image-synthesis benchmarks Zhang et al., 2017;Miyato & Koyama, 2018;Zhang et al., 2018;Brock et al., 2018;Karras et al., 2019).",
    "next": "In the original setting, GANs are composed of two neural networks trained with competing goals: the generator is trained to synthesize realistic samples to fool the discriminator and the discriminator is trained to distinguish real samples from fake ones produced by the generator.",
    "query": "Are there any research papers on conditional GANs with projection-based discriminators for improving image generation quality and super-resolution?"
  },
  {
    "index": 635,
    "source_corpus_id": 85449634,
    "ref_id": "b41",
    "citation_corpus_id": 5592690,
    "start": 1968,
    "end": 1988,
    "title": "MACHINE COMPREHENSION USING MATCH-LSTM AND ANSWER POINTER",
    "abstract": "Machine comprehension of text is an important problem in natural language processing. A recently released dataset, the Stanford Question Answering Dataset (SQuAD), offers a large number of real questions and their answers created by humans through crowdsourcing. SQuAD provides a challenging testbed for evaluating machine comprehension algorithms, partly because compared with previous datasets, in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths. We propose an end-to-end neural architecture for the task. The architecture is based on match-LSTM, a model we proposed previously for textual entailment, and Pointer Net, a sequence-to-sequence model proposed byVinyals et al. (2015)to constrain the output tokens to be from the input sequences. We propose two ways of using Pointer Net for our task. Our experiments show that both of our two models substantially outperform the best results obtained byRajpurkar et al. (2016)using logistic regression and manually crafted features.",
    "prev": "A lot of effort has been put into designing sophisticated neural MRC architectures for reading short context (e.g.",
    "curr": "a single paragraph), with much success (Wang & Jiang, 2017;Seo et al., 2017;Xiong et al., 2017;Wang et al., 2018c;Yu et al., 2018, inter alia).",
    "next": "However, the performance of such systems degrades significantly when combined with a retriever in open domain settings.",
    "query": "Are there any studies on neural architectures for machine comprehension in open domain settings using match-LSTM and pointer networks?"
  },
  {
    "index": 636,
    "source_corpus_id": 11324902,
    "ref_id": "b24",
    "citation_corpus_id": 14124313,
    "start": 22935,
    "end": 22962,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "All evaluations are repeated 10 times based on different shuffles of the data, and the mean accuracies and standard deviations are analyzed.",
    "curr": "Office: Since the office dataset is rather small with only 2817 images in its largest domain, we use the latent representations of the convolution neural network VGG16 of Simonyan & Zisserman (2014).",
    "next": "In particular we train a classifier with one hidden layer, 256 hidden nodes and sigmoid activation function on top of the output of the first dense layer in the network.",
    "query": "Are there any research papers on the impact of deep convolutional network architectures on image recognition performance and their application in transfer learning tasks?"
  },
  {
    "index": 637,
    "source_corpus_id": 57759353,
    "ref_id": "b10",
    "citation_corpus_id": 7034786,
    "start": 9408,
    "end": 9433,
    "title": "NEURAL PROGRAMMER-INTERPRETERS",
    "abstract": "We propose the neural programmer-interpreter (NPI): a recurrent and compositional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-tosequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these programs and all 21 associated subprograms.Published as a conference paper at ICLR 2016 INPUT GOTO KEY ARG END h M key M prog GOTO() HGOTO() LGOTO() VGOTO() LGOTO() ACT(LEFT) DGOTO() ACT(DOWN)",
    "prev": "(2017) Learning to execute programs.",
    "curr": "Neural Program Interpreters (NPI) have been extensively studied for programs that abstract and execute tasks such as sorting, shape manipulation, and grade-school arithmetic (Reed & De Freitas, 2016;Cai et al., 2017;Bošnjak et al., 2017).",
    "next": "In NPI (Reed & De Freitas, 2016), the key insight is that a program execution trace can be decomposed into predefined operations that are more primitive; and at each step, an NPI learns to predict what operation to take next depending on the general environment, domain specific state , and previous actions.",
    "query": "Are there any research papers on neural program interpreters with compositional architectures and task-agnostic components for executing programs like sorting and arithmetic?"
  },
  {
    "index": 638,
    "source_corpus_id": 263605565,
    "ref_id": "b21",
    "citation_corpus_id": 238419652,
    "start": 7180,
    "end": 7202,
    "title": "Published as a conference paper at ICLR 2022 GRADIENT STEP DENOISER FOR CONVERGENT PLUG- AND-PLAY",
    "abstract": "Plug-and-Play (PnP) methods constitute a class of iterative algorithms for imaging problems where regularization is performed by an off-the-shelf denoiser. Although PnP methods can lead to tremendous visual performance for various image problems, the few existing convergence guarantees are based on unrealistic (or suboptimal) hypotheses on the denoiser, or limited to strongly convex data-fidelity terms. We propose a new type of PnP method, based on half-quadratic splitting, for which the denoiser is realized as a gradient descent step on a functional parameterized by a deep neural network. Exploiting convergence results for proximal gradient descent algorithms in the nonconvex setting, we show that the proposed PnP algorithm is a convergent iterative scheme that targets stationary points of an explicit global functional. Besides, experiments show that it is possible to learn such a deep denoiser while not compromising the performance in comparison to other state-of-the-art deep denoisers used in PnP schemes. We apply our proximal gradient algorithm to various ill-posed inverse problems, e.g. deblurring, superresolution and inpainting. For all these applications, numerical results empirically confirm the convergence results. Experiments also show that this new algorithm reaches state-of-the-art performance, both quantitatively and qualitatively. . Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. . Ffdnet: Toward a fast and flexible solution for cnnbased image denoising.",
    "prev": "(2020); Kamilov et al.",
    "curr": "(2023)).For example, the proximal-gradient method variant of PnP can be written as (Hurault et al., 2022a)\nx k ← prox γg (z k ) with z k ← x k−1 − γτ (x k−1 − D σ (x k−1 )),(3)\nwhere D σ is a denoiser with a parameter σ > 0 for controlling its strength, τ > 0 is a regularization parameter, and γ > 0 is a step-size.The theoretical convergence of PnP methods has been established for convex functions g using monotone operator theory (Sreehari et al., 2016;Sun et al., 2019;Ryu et al., 2019), as well as for nonconvex functions based on interpreting the denoiser as a MMSE estimator (Xu et al., 2020) or ensuring that the term (I − D σ ) in (3) corresponds to a gradient ∇h of a function h parameterized by a deep neural network (Hurault et al., 2022a,b;Cohen et al., 2021a).Many variants of PnP have been developed over the past few years (Romano et al., 2017;Metzler et al., 2018;Zhang et al., 2017b;Meinhardt et al., 2017;Dong et al., 2019;Zhang et al., 2019;Wei et al., 2020), which has motivated an extensive research on its theoretical properties (Chan et al., 2017;Buzzard et al., 2018;Ryu et al.,",
    "next": null,
    "query": "Are there any research papers on convergent plug-and-play methods for imaging problems that utilize deep neural network parameterized denoisers and half-quadratic splitting?"
  },
  {
    "index": 639,
    "source_corpus_id": 235368204,
    "ref_id": "b16",
    "citation_corpus_id": 4722462,
    "start": 2279,
    "end": 2296,
    "title": "EMERGENT TRANSLATION IN MULTI-AGENT COMMUNICATION",
    "abstract": "While most machine translation systems to date are trained on large parallel corpora, humans learn language in a different way: by being grounded in an environment and interacting with other humans. In this work, we propose a communication game where two agents, native speakers of their own respective languages, jointly learn to solve a visual referential task. We find that the ability to understand and translate a foreign language emerges as a means to achieve shared goals. The emergent translation is interactive and multimodal, and crucially does not require parallel corpora, but only monolingual, independent text and corresponding images. Our proposed translation model achieves this by grounding the source and target languages into a shared visual modality, and outperforms several baselines on both word-level and sentence-level translation tasks. Furthermore, we show that agents in a multilingual community learn to translate better and faster than in a bilingual communication setting.",
    "prev": "The success of deep learning (DL) models on complicated cognitive tasks (Krizhevsky et al., 2012;LeCun et al., 2015;Silver et al., 2016) then inspired researchers to apply DL-based models to language games to investigate the agents' ability to invent communication protocols without preset rules (e.g.",
    "curr": "Lee et al., 2018;Lazaridou et al., 2018).",
    "next": "In the existing works (e.g.",
    "query": "Are there any studies that explore emergent translation in multi-agent communication through visual referential tasks without relying on parallel corpora?"
  },
  {
    "index": 640,
    "source_corpus_id": 236318292,
    "ref_id": "b25",
    "citation_corpus_id": 108300573,
    "start": 3303,
    "end": 3306,
    "title": "INTERPOLATION-PREDICTION NETWORKS FOR IRREGULARLY SAMPLED TIME SERIES",
    "abstract": "In this paper, we present a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series. The architecture is based on the use of a semi-parametric interpolation network followed by the application of a prediction network. The interpolation network allows for information to be shared across multiple dimensions of a multivariate time series during the interpolation stage, while any standard deep learning model can be used for the prediction network. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. We investigate the performance of this architecture on both classification and regression tasks, showing that our approach outperforms a range of baseline and recently proposed models. 1 1 Our implementation is available at : https://github.com/mlds-lab/interp-net 1 arXiv:1909.07782v1 [cs.LG] 13 Sep 2019Published as a conference paper at ICLR 2019 trends. Similar to the work ofLipton et al. (2016)andChe et al. (2018a), our architecture also explicitly leverages a separate information channel related to patterns of observation times. However, our representation uses a semi-parametric intensity function representation of this information that is more closely related to the work of Lasko (2014) on modeling medical event point processes.Our architecture thus produces three output time series for each input time series: a smooth interpolation modeling broad trends in the input, a short time-scale interpolation modeling transients, and an intensity function modeling local observation frequencies.This work is motivated by problems in the analysis of electronic health records (EHRs)(Marlin et al., 2012;Lipton et al., 2016;Futoma et al., 2017;Che et al., 2018a). It remains rare for hospital systems to capture dense physiological data streams. Instead, it is common for the physiological time series data in electronic health records to be both sparse and irregularly sampled. The additional issue of the lack of alignment in the observation times across physiological variables is also very common.We evaluate the proposed architecture on two datasets for both classification and regression tasks. Our approach outperforms a variety of simple baseline models as well as the basic and advanced GRU models introduced by Che et al. (2018a) across several metrics. We also compare our model with to the Gaussian process adapter (Li & Marlin, 2016) and multi-task Gaussian process RNN classifier(Futoma et al., 2017). Further, we perform full ablation testing of the information channels our architecture can produce to assess their impact on classification and regression performance.",
    "prev": "The main challenges in dealing with such data include the presence of variable time gaps between the observation time points, partially observed feature vectors caused by the lack of temporal alignment across different dimensions, as well as different data cases, and variable numbers of observations across dimensions and data cases.",
    "curr": "Significant recent work has focused on developing specialized models and architectures to address these challenges in modeling irregularly sampled multivariate time series [17,18,19,9,3,26,24,12,16,27,7,30,14].",
    "next": "Recently, Shukla and Marlin [27] introduced the Multi-Time Attention Network (mTAN) model, a variational autoencoder (VAE) architecture for continuous-time interpolation of irregularly sampled time series.",
    "query": "Are there any research papers on deep learning architectures for interpolation and prediction in irregularly sampled multivariate time series, particularly those addressing sparse and misaligned data in electronic health records?"
  },
  {
    "index": 642,
    "source_corpus_id": 52903499,
    "ref_id": "b29",
    "citation_corpus_id": 4429876,
    "start": 4428,
    "end": 4432,
    "title": "Global optimality conditions for deep neural networks",
    "abstract": "We study the error landscape of deep linear and nonlinear neural networks with square error loss. We build on the recent results in the literature and present necessary and sufficient conditions for a critical point of the empirical risk function to be a global minimum in the deep linear network case. Our simple conditions can also be used to determine whether a given critical point is a global minimum or a saddle point. We further extend these results to deep nonlinear neural networks and prove similar sufficient conditions for global optimality in the function space.",
    "prev": "Bridging this gap between theory and practice is a very active area of research.",
    "curr": "There have been many attempts to understand why optimization works well for neural networks, by studying the loss surface [1,10,12,17,18,21,23,24,[26][27][28][29][30] and the role of (stochastic) gradient-based methods [4,8,25].",
    "next": "Besides nonconvexity, for ReLU networks significant additional challenges in the analysis arise due to nondifferentiability, and obtaining a precise understanding of the nondifferentiable points is still elusive.",
    "query": "Are there any research papers on global optimality conditions for deep neural networks, focusing on error landscape analysis and the role of gradient-based optimization methods?"
  },
  {
    "index": 644,
    "source_corpus_id": 53215593,
    "ref_id": "b3",
    "citation_corpus_id": 11212020,
    "start": 12131,
    "end": 12154,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "(1) Our key intuition is that the inverse dynamics model should attend to the most relevant part of the observation, which is controllable by the agent, to be able to classify the actions.",
    "curr": "We determine whether each region in a H × W grid is controllable, or in other words, useful for predicting the agent's action, by using a spatial attention mechanism (Bahdanau et al., 2015;Xu et al., 2015).",
    "next": "An overview of the model is shown in Figure 1.",
    "query": "Are there any research papers on attention mechanisms in neural machine translation for improving translation performance?"
  },
  {
    "index": 645,
    "source_corpus_id": 220347587,
    "ref_id": "b36",
    "citation_corpus_id": 196178484,
    "start": 5174,
    "end": 5178,
    "title": "Self-Attentive, Multi-Context One-Class Classification for Unsupervised Anomaly Detection on Text",
    "abstract": "There exist few text-specific methods for unsupervised anomaly detection, and for those that do exist, none utilize pre-trained models for distributed vector representations of words. In this paper we introduce a new anomaly detection method-Context Vector Data Description (CVDD)-which builds upon word embedding models to learn multiple sentence representations that capture multiple semantic contexts via the self-attention mechanism. Modeling multiple contexts enables us to perform contextual anomaly detection of sentences and phrases with respect to the multiple themes and concepts present in an unlabeled text corpus. These contexts in combination with the self-attention weights make our method highly interpretable. We demonstrate the effectiveness of CVDD quantitatively as well as qualitatively on the wellknown Reuters, 20 Newsgroups, and IMDB Movie Reviews datasets.",
    "prev": "These methods attempt to separate nominal samples from anomalies in an unsupervised manner by concentrating nominal data in feature space while mapping anomalies to distant locations [36,6].",
    "curr": "In the domain of NLP, DSVDD [36] has been successfully applied to text, which yields a form of interpretation using attention mechanisms [37].",
    "next": "For images, Kauffmann et al.",
    "query": "Are there any research papers on unsupervised anomaly detection methods for text that utilize self-attention mechanisms and multi-context representations?"
  },
  {
    "index": 647,
    "source_corpus_id": 264128269,
    "ref_id": "b19",
    "citation_corpus_id": 21850704,
    "start": 2428,
    "end": 2449,
    "title": "A Deep Reinforced Model for Abstractive Summarization",
    "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. However, for longer documents and summaries, these models often include repetitive and incoherent phrases. We introduce a neural network model with intra-attention and a new training method. This method combines standard supervised word prediction and reinforcement learning (RL). Models trained only with the former often exhibit \"exposure bias\" -they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, a 5.7 absolute points improvement over previous state-of-the-art models. It also performs well as the first abstractive model on the New York Times corpus. Human evaluation also shows that our model produces higher quality summaries.",
    "prev": "xplanation for why Goodhart's law occurs in Markov decision processes.We use these theoretical insights to propose an optimal early stopping method that provably avoids the aforementioned pitfall and derive theoretical regret bounds for this method.Moreover, we derive a training method that maximises worst-case reward, for the setting where there is uncertainty about the true reward function.Finally, we evaluate our early stopping method experimentally.Our results support a foundation for a theoretically-principled study of reinforcement learning under reward misspecification.",
    "curr": "INTRODUCTION\n\nTo solve a problem using Reinforcement Learning (RL), it is necessary first to formalise that problem using a reward function (Sutton & Barto, 2018).However, due to the complexity of many real-world tasks, it is exceedingly difficult to directly specify a reward function that fully captures the task in the intended way.However, misspecified reward functions will often lead to undesirable behaviour (Paulus et al., 2018;Ibarz et al., 2018;Knox et al., 2023;Pan et al., 2021).This makes designing good reward functions a major obstacle to using RL in practice, especially for safety-critical applications.",
    "next": "An increasingly popular solution is to learn reward functions from mechanisms such as human or automated feedback (e.g.Christiano et al., 2017;Ng & Russell, 2000).However, this approach comes with its own set of challenges: the right data can be difficult to collect (e.g.Paulus et al., 2018), and it is often challenging to interpret it correctly (e.g.Mindermann & Armstrong, 2018;Skalse & Abate, 2023).Moreover, optimising a policy against a learned reward model effectively constitutes a distributional shift (Gao et al., 2023); i.e., even if a reward function is accurate under the training distribution, it may fail to induce desirable behaviour from the RL agent.",
    "query": "Are there any research papers on methods for addressing reward misspecification in reinforcement learning, including early stopping techniques, worst-case reward maximization, and approaches to mitigate distributional shift in learned reward models?"
  },
  {
    "index": 649,
    "source_corpus_id": 247476256,
    "ref_id": "b33",
    "citation_corpus_id": 15197911,
    "start": 3725,
    "end": 3741,
    "title": "PUSHING THE BOUNDARIES OF BOUNDARY DETEC- TION USING DEEP LEARNING",
    "abstract": "In this work we show that adapting Deep Convolutional Neural Network training to the task of boundary detection can result in substantial improvements over the current state-of-the-art in boundary detection. Our contributions consist firstly in combining a careful design of the loss for boundary detection training, a multi-resolution architecture and training with external data to improve the detection accuracy of the current state of the art. When measured on the standard Berkeley Segmentation Dataset, we improve theoptimal dataset scale F-measure from 0.780 to 0.808 -while human performance is at 0.803. We further improve performance to 0.813 by combining deep learning with grouping, integrating the Normalized Cuts technique within a deep network. We also examine the potential of our boundary detector in conjunction with the task of semantic segmentation and demonstrate clear improvements over state-ofthe-art systems. Our detector is fully integrated in the popular Caffe framework and processes a 320x420 image in less than a second.",
    "prev": "Such a strategy has been successful in other dense prediction tasks (Ronneberger et al., 2015) as well.",
    "curr": "Others propose different loss functions (Kokkinos, 2016;Kervadec et al., 2019) to address class imbalance.",
    "next": "Despite the improvements, we identify two issues regarding crisp boundary detection.",
    "query": "Are there any research papers on deep learning methods for boundary detection that address class imbalance through customized loss functions and integration with semantic segmentation?"
  },
  {
    "index": 650,
    "source_corpus_id": 220055921,
    "ref_id": "b36",
    "citation_corpus_id": 7034786,
    "start": 9963,
    "end": 9967,
    "title": "NEURAL PROGRAMMER-INTERPRETERS",
    "abstract": "We propose the neural programmer-interpreter (NPI): a recurrent and compositional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-tosequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these programs and all 21 associated subprograms.Published as a conference paper at ICLR 2016 INPUT GOTO KEY ARG END h M key M prog GOTO() HGOTO() LGOTO() VGOTO() LGOTO() ACT(LEFT) DGOTO() ACT(DOWN)",
    "prev": "Our proposed framework bears some resemblance to SPIRAL [7]; however, SPIRAL does not provide a density function, and it has no hierarchical structure, limiting its applications to image reconstruction and unconditional generation.",
    "curr": "Neuro-symbolic modeling has become an active area of research, with applications to learning inputoutput programs [37,16,3,34], question answering [44,29] and image description [4].",
    "next": "GNS modeling distinguishes from prior work through its focus on hybrid generative modeling, combining both structured program execution and neural networks directly in the probabilistic generative process.",
    "query": "Are there any research papers on hybrid generative models combining symbolic program execution with neural networks for tasks like learning input-output programs and image description?"
  },
  {
    "index": 652,
    "source_corpus_id": 202712898,
    "ref_id": "b2",
    "citation_corpus_id": 14124313,
    "start": 1541,
    "end": 1568,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "Nonetheless, these architectures may not necessarily lead to better generalization performance compared with other candidate architectures in the same search space, and therefore further improvement is possible by revising existing NAS algorithms.",
    "curr": "INTRODUCTION\n\nVarious neural network architectures (Krizhevsky et al., 2012;Simonyan & Zisserman, 2015;He et al., 2016;Huang et al., 2017) have been devised over the past decades, achieving superhuman performance for a wide range of tasks.",
    "next": "Designing these neural networks typically takes substantial efforts from domain experts by trial and error.",
    "query": "Are there any research papers on the impact of deep convolutional network depth on large-scale image recognition performance and generalization capabilities?"
  },
  {
    "index": 654,
    "source_corpus_id": 13807351,
    "ref_id": "b45",
    "citation_corpus_id": 252796,
    "start": 5644,
    "end": 5665,
    "title": "Building a Large Annotated Corpus of English: The Penn Treebank",
    "abstract": "There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenomena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large corpora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valuable for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investigation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models.In this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus 1 consisting of over 4.5 million words of American English. During the first three-year phase of the Penn Treebank Project (1989)(1990)(1991)(1992), this corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure. These materials are available to members of the Linguistic Data Consortium; for details, see Section 5.1.The paper is organized as follows. Section 2 discusses the POS tagging task. After outlining the considerations that informed the design of our POS tagset and presenting the tagset itself, we describe our two-stage tagging process, in which text is first assigned POS tags automatically and then corrected by human annotators. Section 3 briefly presents the results of a comparison between entirely manual and semi-automated tagging, with the latter being shown to be superior on three counts: speed, consistency, and accuracy. In Section 4, we turn to the bracketing task. Just as with the tagging task, we have partially automated the bracketing task: the output of",
    "prev": "Actively biasing the optimization towards wide valleys in the energy landscape results in better generalization error.",
    "curr": "We present experimental results on fully-connected and convolutional neural networks (CNNs) on the MNIST and CIFAR-10 (Krizhevsky, 2009) datasets and recurrent neural networks (RNNs) on the Penn Tree Bank dataset (PTB) (Marcus et al., 1993) and character-level text prediction.",
    "next": "Our experiments show that Entropy-SGD scales to deep networks used in practice, obtains comparable generalization error as competitive baselines and also trains much more quickly than SGD (we get a 2x speed-up over SGD on RNNs).",
    "query": "Are there any research papers on optimization techniques for improving generalization and training efficiency in deep neural networks, such as Entropy-SGD applied to CNNs, RNNs, and other architectures on datasets like MNIST, CIFAR-10, and Penn Tree Bank?"
  },
  {
    "index": 655,
    "source_corpus_id": 3470596,
    "ref_id": "b39",
    "citation_corpus_id": 11445252,
    "start": 29665,
    "end": 29689,
    "title": "LEARNING FEATURES OF MUSIC FROM SCRATCH",
    "abstract": "We introduce a new large-scale music dataset, MusicNet, to serve as a source of supervision and evaluation of machine learning methods for music research. Mu-sicNet consists of hundreds of freely-licensed classical music recordings by 10 composers, written for 11 instruments, together with instrument/note annotations resulting in over 1 million temporal labels on 34 hours of chamber music performances under various studio and microphone conditions. We define a multi-label classification task to predict notes in musical recordings, along with an evaluation protocol. We benchmark several machine learning architectures for this task: i) learning from \"hand-crafted\" spectrogram features; ii) end-to-end learning with a neural net; iii) end-to-end learning with a convolutional neural net. We show that several end-to-end learning proposals outperform approaches based on learning from hand-crafted audio features.",
    "prev": "The nature of an audio signal allows one to exploit complex operations as presented earlier in the paper.",
    "curr": "The experiments were performed on the MusicNet dataset [Thickstun et al., 2016].",
    "next": "For computational efficiency we resampled the original input from 44.1kHz to 11kHz using the algorithm described in Smith [2002].",
    "query": "Are there any research papers on using the MusicNet dataset for machine learning approaches in music, comparing end-to-end learning with hand-crafted audio features?"
  },
  {
    "index": 656,
    "source_corpus_id": 222066778,
    "ref_id": "b2",
    "citation_corpus_id": 3708505,
    "start": 5522,
    "end": 5539,
    "title": "DEEP NEURAL NETWORKS AS GAUSSIAN PROCESSES",
    "abstract": "A deep fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP) in the limit of infinite network width. This correspondence enables exact Bayesian inference for neural networks on regression tasks by means of straightforward matrix computations. For single hiddenlayer networks, the covariance function of this GP has long been known. Recently, kernel functions for multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified the correspondence between using these kernels as the covariance function for a GP and performing fully Bayesian prediction with a deep neural network. In this work, we derive this correspondence and develop a computationally efficient pipeline to compute the covariance functions. We then use the resulting GP to perform Bayesian inference for deep neural networks on MNIST and CIFAR-10. We find that the GP-based predictions are competitive and can outperform neural networks trained with stochastic gradient descent. We observe that the trained neural network accuracy approaches that of the corresponding GP-based computation with increasing layer width, and that the GP uncertainty is strongly correlated with prediction error. We connect our observations to the recent development of signal propagation in random neural networks. * Both authors contributed equally to this work. † Work done as a member of the Google Brain Residency program (g.co/brainresidency). 1 Throughout this paper, we assume the conditions on the parameter distributions and nonlinearities are such that the Central Limit Theorem will hold; for instance, that the weight variance is scaled inversely proportional to the layer width.",
    "prev": "Related work.",
    "curr": "Kernels for deep learning were originally derived by Neal (1996) for shallow networks, and later for deep networks (Cho & Saul, 2009;Daniely et al., 2016;Lee et al., 2018;Matthews et al., 2018).",
    "next": "Smola et al.",
    "query": "Are there any research papers on deep neural networks as Gaussian processes using kernel methods and Bayesian inference?"
  },
  {
    "index": 657,
    "source_corpus_id": 250243645,
    "ref_id": "b46",
    "citation_corpus_id": 6628106,
    "start": 21932,
    "end": 21934,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "Starting with Figure 1 We then plot the MI accuracy and precision as a function of time, both before injection and also for ten epochs after.",
    "curr": "We observe that MI is impossible until the injection step, after which precision remains very 46 47 48 49 50 51 52 53 54 55 56 57 58 59  high for many epochs, taking 10 epochs to decay to roughly 65%, which is permitted by DP 2 with ε ≈ 0.6.",
    "next": "This demonstrates that forgetting does occur for this setting.",
    "query": "Are there any research papers on the performance of the Adam optimizer in terms of precision and accuracy over training epochs, particularly in scenarios involving data injection and forgetting?"
  },
  {
    "index": 659,
    "source_corpus_id": 249848252,
    "ref_id": "b11",
    "citation_corpus_id": 196831582,
    "start": 2748,
    "end": 2751,
    "title": "Published as a conference paper at ICLR 2020 ON THE \"STEERABILITY\" OF GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "An open secret in contemporary machine learning is that many models work beautifully on standard benchmarks but fail to generalize outside the lab. This has been attributed to biased training data, which provide poor coverage over real world events. Generative models are no exception, but recent advances in generative adversarial networks (GANs) suggest otherwise -these models can now synthesize strikingly realistic and diverse images. Is generative modeling of photos a solved problem? We show that although current GANs can fit standard datasets very well, they still fall short of being comprehensive models of the visual manifold. In particular, we study their ability to fit simple transformations such as camera movements and color changes. We find that the models reflect the biases of the datasets on which they are trained (e.g., centered objects), but that they also exhibit some capacity for generalization: by \"steering\" in latent space, we can shift the distribution while still creating realistic images. We hypothesize that the degree of distributional shift is related to the breadth of the training data distribution. Thus, we conduct experiments to quantify the limits of GAN transformations and introduce techniques to mitigate the problem. Code is released on our project page:",
    "prev": "Under review.",
    "curr": "Introduction\n\nGenerative Adversarial Networks (GANs) [8] have achieved significant advancement over the past several years, enabling many computer vision tasks such as image manipulation [17,12,28,9,2], domain translation [11,34,6,7,20,18], and image or video generation [19,21,15,16,13,14,29,32,30,31].",
    "next": "In addition, the resolution and quality of images synthesized by generative models have seen rapid improvement recently in terms of quantitative metrics [3,10,27].",
    "query": "Are there any studies that investigate the steerability of generative adversarial networks and their limitations in generalizing beyond training data distributions?"
  },
  {
    "index": 660,
    "source_corpus_id": 222125236,
    "ref_id": "b21",
    "citation_corpus_id": 207870430,
    "start": 1696,
    "end": 1721,
    "title": "Published as a conference paper at ICLR 2020 GENERALIZATION THROUGH MEMORIZATION: NEAREST NEIGHBOR LANGUAGE MODELS",
    "abstract": "We introduce kNN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a k-nearest neighbors (kNN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong WIKITEXT-103 LM, with neighbors drawn from the original training set, our kNN-LM achieves a new stateof-the-art perplexity of 15.79 -a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail. . Pointer sentinel mixture models. ICLR, 2017. Tomáš Mikolov, Martin Karafiát, Lukáš Burget, JanČernockỳ, and Sanjeev Khudanpur. Recurrent neural network based language model. In Eleventh annual conference of the international speech communication association, 2010. A. Emin Orhan. A simple cache model for image recognition. In NeurIPS, 2018. Nicolas Papernot and Patrick McDaniel. Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning. arXiv preprint arXiv:1803.04765, 2018.Ofir Press and Lior Wolf. Using the output embedding to improve language models. In ICLR, 2017. machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015. Catanzaro. Megatron-lm: Training multi-billion parameter language models using gpu model parallelism. arXiv preprint arXiv:1909.08053, 2019.",
    "prev": "rity search.This approach requires no additional training and scales to give the decoder direct access to billions of examples at test time, resulting in a highly expressive model that consistently improves performance across many settings.Simply adding nearest neighbor search improves a state-of-the-art German-English translation model by 1.5 BLEU.kNN-MT allows a single model to be adapted to diverse domains by using a domain-specific datastore, improving results by an average of 9.2 BLEU over zero-shot transfer, and achieving new state-of-the-art results-without training on these domains.A massively multilingual model can also be specialized for particular language pairs, with improvements of 3 BLEU for translating from English into German and Chinese.Qualitatively, kNN-MT is easily interpretable; it combines source and target context to retrieve highly relevant examples.",
    "curr": "INTRODUCTION\n\nNon-parametric methods have recently been successfully applied to tasks such as language modeling (Khandelwal et al., 2020) and question answering (Guu et al., 2020;Lewis et al., 2020).They allow models that are (1) expressive, because they can use an arbitrary amount of data at test time; (2) adaptable, because predictions can be controlled by changing the datastore, and (3) interpretable, because the data used to make the prediction can be directly inspected.We introduce kNN-MT, a simple non-parametric method for machine translation (MT) using nearest neighbor retrieval.kNN-MT can be added to any pre-trained neural translation model without further training, and significantly improves performance for in-domain, out-of-domain, and multi-lingual evaluations.",
    "next": "More specifically, kNN-MT interpolates the target-token softmax distribution from a neural MT model with a multinomial generated using nearest neighbor search over examples cached in a data store.The cache is over translation contexts (i.e.",
    "query": "Are there any research papers on non-parametric methods for machine translation and language modeling using nearest neighbor retrieval and interpolation with neural models?"
  },
  {
    "index": 661,
    "source_corpus_id": 85543148,
    "ref_id": "b37",
    "citation_corpus_id": 14124313,
    "start": 7304,
    "end": 7332,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "It is challenging due to the complexity of features in high-dimensional space (Krizhevsky et al., 2012), the large intra-class variation and inter-class similarity across categories in benchmarks (Deng et al., 2009;Tsung-Yi Lin, 2015).",
    "curr": "Thanks to the development of deep networks structure (Simonyan & Zisserman, 2015;He et al., 2016) and modern GPU hardware acceleration, this community has witnessed a great bloom in both performance and efficiency.",
    "next": "The detection of small objects is addressed in concurrent literature mainly through two manners.",
    "query": "Are there any research papers on the impact of deep convolutional network architectures on large-scale image recognition performance and efficiency?"
  },
  {
    "index": 663,
    "source_corpus_id": 251953412,
    "ref_id": "b39",
    "citation_corpus_id": 201646309,
    "start": 3249,
    "end": 3275,
    "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    "abstract": "BERT(Devlin et al., 2018)and RoBERTa (Liu  et al., 2019)  has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods. 1",
    "prev": "As there are millions to billions of documents in a collection, efficiency is the most fundamental prerequisite for large-scale retrieval.",
    "curr": "To this end, query-agnostic document representations (i.e., indexing the collection independently) and lightweight relevance metrics (e.g., cosine similarity, dot-product) have become the common practices to meet the prerequisite -usually achieved by a two-tower structure (Reimers & Gurevych, 2019), a.k.a., bi-encoder and dual-encoder, in representation learning literature.",
    "next": "Besides the prevalent 'dense-vector retrieval' paradigm that encodes both queries and documents in the same low-dimension, real-valued latent semantic space , another retrieval paradigm, 'lexicon-weighting retrieval', aims to leverage weighted sparse representation in vocabulary space (Formal et al., 2021a;.",
    "query": "Are there any research papers on methods for improving retrieval efficiency in large-scale document collections using bi-encoder architectures or similar two-tower structures?"
  },
  {
    "index": 665,
    "source_corpus_id": 262065523,
    "ref_id": "b19",
    "citation_corpus_id": 13046179,
    "start": 2139,
    "end": 2165,
    "title": "A BASELINE FOR DETECTING MISCLASSIFIED AND OUT-OF-DISTRIBUTION EXAMPLES IN NEURAL NETWORKS",
    "abstract": "We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.Published as a conference paper at ICLR 2017 one method which outperforms the baseline on some (but not all) tasks. This new method evaluates the quality of a neural network's input reconstruction to determine if an example is abnormal.",
    "prev": "ion (UQ) methods to modulate CIs.To this end, we propose G-∆UQ, a new single model UQ method that extends the recently proposed stochastic centering framework to support structured data and partial stochasticity.Evaluated across covariate, concept, and graph size shifts, G-∆UQ not only outperforms several popular UQ methods in obtaining calibrated CIs, but also outperforms alternatives when CIs are used for generalization gap prediction or OOD detection.Overall, our work not only introduces a new, flexible GNN UQ method, but also provides novel insights into GNN CIs on safety-critical tasks.",
    "curr": "INTRODUCTION\n\nAs graph neural networks (GNNs) are increasingly deployed in critical applications with test-time distribution shifts (Zhang & Chen, 2018;Gaudelet et al., 2020;Yang et al., 2018;Yan et al., 2019;Zhu et al., 2022), it becomes necessary to expand model evaluation to include safety-centric metrics, such as calibration errors (Guo et al., 2017), out-of-distribution (OOD) rejection rates (Hendrycks & Gimpel, 2017), and generalization gap estimates (Jiang et al., 2019), to holistically understand model performance in such shifted regimes (Hendrycks et al., 2022b;Trivedi et al., 2023b).Notably, such additional metrics often rely on confidence indicators (CIs), such as maximum softmax or predictive entropy, which can be derived from prediction probabilities.Although there is a clear understanding in the computer vision literature that the quality of confidence indicators can noticeably deteriorate under distribution shifts (Wiles et al., 2022;Ovadia et al., 2019), and additional factors like model size or expressivity can exacerbate this deterioration (Minderer et al., 2021), the impact of these phenomena on graph neural networks (GNNs) remains under-explored.Indeed, there is an expectation that adopting more advanced or expressive architectures (Chuang & Jegelka, 2022;Alon & Yahav, 2021;Topping et al., 2022;Rampášek et al., 2022;Zhao et al., 2022) would inherently improve CI calibration on graph",
    "next": null,
    "query": "Are there any research papers on uncertainty quantification methods for graph neural networks with a focus on improving calibration and out-of-distribution detection under distribution shifts?"
  },
  {
    "index": 666,
    "source_corpus_id": 3473900,
    "ref_id": "b16",
    "citation_corpus_id": 5959482,
    "start": 1479,
    "end": 1501,
    "title": "Efficient Estimation of Word Representations in Vector Space",
    "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities. arXiv:1301.3781v3 [cs.CL] 7 Sep 2013 1 The test set is available at www.fit.vutbr.cz/˜imikolov/rnnlm/word-test.v1.txt 2",
    "prev": "INTRODUCTION\n\nLearning feature representations of natural data such as text and images has become increasingly important for understanding real-world concepts.",
    "curr": "These representations are useful for many tasks, ranging from semantic understanding of words and sentences (Mikolov et al., 2013;Kiros et al., 2015), image caption generation (Vinyals et al., 2015), textual entailment prediction (Rocktäschel et al., 2015), to language communication with robots (Bisk et al., 2016).",
    "next": "Meaningful representations of text and images capture visual-semantic information, such as hierarchical structure where certain entities are abstractions of others.",
    "query": "Are there any research papers on efficient methods for learning word representations in vector space using large-scale data and their applications in natural language processing and vision tasks?"
  },
  {
    "index": 668,
    "source_corpus_id": 225103201,
    "ref_id": "b8",
    "citation_corpus_id": 24029589,
    "start": 9634,
    "end": 9654,
    "title": "The Relationship Between High-Dimensional Geometry and Adversarial Examples",
    "abstract": "Machine learning models with very low test error have been shown to be consistently vulnerable to small, adversarially chosen perturbations of the input. We hypothesize that this counterintuitive behavior is a result of the high-dimensional geometry of the data manifold, and explore this hypothesis on a simple highdimensional dataset. For this dataset we show a fundamental bound relating the classification error rate to the average distance to the nearest misclassification, which is independent of the model. We train different neural network architectures on this dataset and show their error sets approach this theoretical bound. As a result of the theory, the vulnerability of machine learning models to small adversarial perturbations is a logical consequence of the amount of test error observed. We hope that our theoretical analysis of this foundational synthetic case will point a way forward to explore how the geometry of complex real-world data sets leads to adversarial examples.",
    "prev": "Finally, we also go beyond the max-margin settings in these works to analyze the dynamics of a finite-time gradient-descent trained classifier.",
    "curr": "We also refer the reader to an orthogonal line of theory work (Fawzi et al., 2018;Gilmer et al., 2018) that has investigated the failure of standard classifiers in the adversarial perturbation setting.",
    "next": "Algorithms for OoD generalization.",
    "query": "Are there any research papers on the relationship between high-dimensional geometry and adversarial examples in machine learning, particularly focusing on the vulnerability of models with low test error to adversarial perturbations?"
  },
  {
    "index": 670,
    "source_corpus_id": 212874725,
    "ref_id": "b3",
    "citation_corpus_id": 22163777,
    "start": 11270,
    "end": 11273,
    "title": "Boosting the Actor with Dual Critic",
    "abstract": "This paper proposes a new actor-critic-style algorithm called Dual Actor-Criticor Dual-AC. It is derived in a principled way from the Lagrangian dual form of the Bellman optimality equation, which can be viewed as a two-player game between the actor and a critic-like function, which is named as dual critic. Compared to its actor-critic relatives, Dual-AC has the desired property that the actor and dual critic are updated cooperatively to optimize the same objective function, providing a more transparent way for learning the critic that is directly related to the objective function of the actor. We then provide a concrete algorithm that can effectively solve the minimax optimization problem, using techniques of multi-step bootstrapping, path regularization, and stochastic dual ascent algorithm. We demonstrate that the proposed algorithm achieves the state-of-the-art performances across several benchmarks. * The first two authors equally contributed.",
    "prev": "Therefore, B π describes how visitation flows from (ξ, α) to (s, a) and hence, we call it the backward flow operator.",
    "curr": "Note that similar forms of B π have appeared in the literature, usually used to encode constraints in a dual linear program for an MDP [4,37,38].",
    "next": "However, the application of B π for the off-policy estimation problem as considered here appears new to the best of our knowledge.",
    "query": "Are there any research papers on actor-critic algorithms with dual critics derived from the Lagrangian dual form of the Bellman equation for off-policy estimation in reinforcement learning?"
  },
  {
    "index": 674,
    "source_corpus_id": 263620583,
    "ref_id": "b38",
    "citation_corpus_id": 57825721,
    "start": 29466,
    "end": 29486,
    "title": "ON THE TURING COMPLETENESS OF MODERN NEURAL NETWORK ARCHITECTURES",
    "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.",
    "prev": "(2022) for a survey).Since it is infeasible to precisely define real-world data and tasks, several works have studied it in stylized well-defined settings (Chan et al., 2022;Hahn & Goyal, 2023;Xie et al., 2021).Garg et al.",
    "curr": "(2022) presented a meta-learning-like Transformers and Sequence Models.The analysis of the capabilities and limitations of recurrent architectures dates back to a few decades ago (Kolen & Kremer, 2001).Given the recent success of Transformers, several works have sought to investigate their theoretical expressiveness (Pérez et al., 2019;Merrill et al., 2022;Chiang & Cholak, 2022;Hahn, 2020;Yun et al., 2020;Liu et al., 2022) as well as their empirical capabilities (Bhattamishra et al., 2023;2020b;Ebrahimi et al., 2020) and limitations (Bhattamishra et al., 2020a;Chiang & Cholak, 2022).Delétang et al.",
    "next": "( 2022) conduct a comprehensive study of the performance of various sequence models such as Transformers and RNNs on formal language tasks.While most of these prior works focus on classification or related tasks, our work complements these as we conduct a comprehensive study on in-context learning tasks.",
    "query": "Are there any research papers on the Turing completeness of modern neural network architectures such as Transformers and Neural GPUs?"
  },
  {
    "index": 675,
    "source_corpus_id": 53483414,
    "ref_id": "b13",
    "citation_corpus_id": 9059612,
    "start": 2706,
    "end": 2728,
    "title": "ADVERSARIAL MACHINE LEARNING AT SCALE",
    "abstract": "Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet (Russakovsky  et al., 2014). Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than singlestep attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a \"label leaking\" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.",
    "prev": "Various defensive methods have been proposed to mitigate the effect of the adversarial examples.",
    "curr": "Adversarial training which augments the training set with adversarial examples shows good defensive performance in terms of white box attacks (Kurakin et al., 2017;.",
    "next": "Apart from adversarial training, there are many other defensive approaches including defensive distillation (Papernot et al., 2016b), using randomization at inference time (Xie et al., 2018), and thermometer encoding (Buckman et al., 2018), etc.",
    "query": "Are there any research papers on adversarial training methods for improving robustness against attacks in large-scale machine learning models?"
  },
  {
    "index": 677,
    "source_corpus_id": 53208122,
    "ref_id": "b1",
    "citation_corpus_id": 102351048,
    "start": 23582,
    "end": 23607,
    "title": "Jointly Measuring Diversity and Quality in Text Generation Models",
    "abstract": "Text generation is an important Natural Language Processing task with various applications. Although several metrics have already been introduced to evaluate the text generation methods, each of them has its own shortcomings. The most widely used metrics such as BLEU only consider the quality of generated sentences and neglect their diversity. For example, repeatedly generation of only one high quality sentence would result in a high BLEU score. On the other hand, the more recent metric introduced to evaluate the diversity of generated texts known as Self-BLEU ignores the quality of generated texts. In this paper, we propose metrics to evaluate both the quality and diversity simultaneously by approximating the distance of the learned generative model and the real data distribution. For this purpose, we first introduce a metric that approximates this distance using n-gram based measures. Then, a feature-based measure which is based on a recent highly deep model trained on a large text corpus called BERT is introduced. Finally, for oracle training mode in which the generators density can also be calculated, we propose to use the distance measures between the corresponding explicit distributions. Eventually, the most popular and recent text generation models are evaluated using both the existing and the proposed metrics and the preferences of the proposed metrics are determined.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on metrics that jointly evaluate diversity and quality in text generation models?"
  },
  {
    "index": 679,
    "source_corpus_id": 220265948,
    "ref_id": "b33",
    "citation_corpus_id": 3524184,
    "start": 20908,
    "end": 20912,
    "title": "INFERENCE SUBOPTIMALITY IN VARIATIONAL AUTOENCODERS",
    "abstract": "Amortized inference has led to efficient approximate inference for large datasets. The quality of posterior inference is largely determined by two factors: a) the ability of the variational distribution to model the true posterior and b) the capacity of the recognition network to generalize inference over all datapoints. We analyze approximate inference in variational autoencoders in terms of these factors. We find that suboptimal inference is often due to amortizing inference rather than the limited complexity of the approximating distribution. We show that this is due partly to the generator learning to accommodate the choice of approximation. Furthermore, we show that the parameters used to increase the expressiveness of the approximation play a role in generalizing inference rather than simply improving the complexity of the approximation.",
    "prev": "Figure 3  We evaluate the accuracy of mean reconstruction at different ratios of observed entries by measuring the peak signal to noise ratio (PSNR) between the ground truth data and mean recoveries ( Figure  3(b)).",
    "curr": "To evaluate probabilistic performance we approximately measure the likelihood assigned by the recovered posteriors to the ground truth data through a reconstruction ELBO, by training a new inference function with the clean ground truths, but leaving the posterior fixed, as is common for evaluating ELBOs in unsupervised settings [34,35,11].",
    "next": "Results are shown in figure 3(c).",
    "query": "Are there any studies that investigate the factors affecting inference quality in variational autoencoders, particularly the role of amortized inference and reconstruction ELBO evaluation methods?"
  },
  {
    "index": 680,
    "source_corpus_id": 51942590,
    "ref_id": "b14",
    "citation_corpus_id": 1428702,
    "start": 10417,
    "end": 10421,
    "title": "Learning Word Vectors for Sentiment Analysis",
    "abstract": "Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term-document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.",
    "prev": "It just sucks.",
    "curr": "( ) This sentence is contained in a movie review from the IMDB movie data set [15], and it is classified as negative sentiment by a machine learning model to be discussed in the sequel.",
    "next": "Now suppose we wish to quantify the importance of feature \"not\" in prediction.",
    "query": "Are there any research papers on combining unsupervised and supervised methods to learn sentiment-aware word vectors for natural language processing tasks?"
  },
  {
    "index": 681,
    "source_corpus_id": 212633677,
    "ref_id": "b27",
    "citation_corpus_id": 5959482,
    "start": 10709,
    "end": 10713,
    "title": "Efficient Estimation of Word Representations in Vector Space",
    "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities. arXiv:1301.3781v3 [cs.CL] 7 Sep 2013 1 The test set is available at www.fit.vutbr.cz/˜imikolov/rnnlm/word-test.v1.txt 2",
    "prev": ", x n ) in one go.",
    "curr": "An embedding is a mapping from plain input, for example words or characters, to a high dimensional vector, for which learning algorithms and toolkits exists, e.g., word2vec [26].",
    "next": "Given the encoders output z = (z 0 , .",
    "query": "Are there any research papers on efficient algorithms for computing high-quality word representations with reduced computational costs?"
  },
  {
    "index": 683,
    "source_corpus_id": 249375359,
    "ref_id": "b1",
    "citation_corpus_id": 52922363,
    "start": 2697,
    "end": 2717,
    "title": "A CONVERGENCE ANALYSIS OF GRADIENT DESCENT FOR DEEP LINEAR NEURAL NETWORKS",
    "abstract": "We analyze speed of convergence to global optimum for gradient descent training a deep linear neural network (parameterized as x → W N W N −1 · · · W 1 x) by minimizing the 2 loss over whitened data. Convergence at a linear rate is guaranteed when the following hold: (i) dimensions of hidden layers are at least the minimum of the input and output dimensions; (ii) weight matrices at initialization are approximately balanced; and (iii) the initial loss is smaller than the loss of any rank-deficient solution. The assumptions on initialization (conditions (ii) and (iii)) are necessary, in the sense that violating any one of them may lead to convergence failure. Moreover, in the important case of output dimension 1, i.e. scalar regression, they are met, and thus convergence to global optimum holds, with constant probability under a random initialization scheme. Our results significantly extend previous analyses, e.g., of deep linear residual networks(Bartlett et al., 2018).",
    "prev": "From the theoretical perspective, understanding the roles of nonlinearity in deep neural networks is one critical part of understanding how modern deep models work.",
    "curr": "Currently, most works focus on linear variants of deep models (Jacot et al., 2018;Arora et al., 2019a;Kawaguchi, 2016;Jing et al., 2022;Tian et al., 2021;Wang et al., 2021).",
    "next": "When nonlinearity is involved, deep models are often treated as richer families of black-box functions than linear ones (Arora et al., 2019b;HaoChen et al., 2021).",
    "query": "Are there any research papers on the convergence analysis of gradient descent for deep linear neural networks and the conditions required for guaranteed convergence?"
  },
  {
    "index": 684,
    "source_corpus_id": 257365130,
    "ref_id": "b13",
    "citation_corpus_id": 102350747,
    "start": 5336,
    "end": 5357,
    "title": "Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Autoencoders",
    "abstract": "We introduce deep inside-outside recursive autoencoders (DIORA), a fully-unsupervised method for discovering syntax that simultaneously learns representations for constituents within the induced tree. Our approach predicts each word in an input sentence conditioned on the rest of the sentence and uses inside-outside dynamic programming to consider all possible binary trees over the sentence. At test time the CKY algorithm extracts the highest scoring parse. DIORA achieves a new state-of-the-art F1 in unsupervised binary constituency parsing (unlabeled) in two benchmark datasets, WSJ and MultiNLI.",
    "prev": "PRELIMINARY\n\n\nESSENTIAL PROPERTIES OF STRUCTURED LANGUAGE MODELS\n\nStructured language models feature combining the powerful representation of neural networks with syntax structures.",
    "curr": "Though many attempts have been made about structured language models (Kim et al., 2019;Drozdov et al., 2019;Shen et al., 2021), three prerequisites need to be met before a model is selected as the backbone of our method.",
    "next": "Firstly, it should have the ability to learn reasonable syntax structure in an unsupervised manner.",
    "query": "Are there any research papers on unsupervised methods for syntax induction in structured language models using recursive autoencoders?"
  },
  {
    "index": 685,
    "source_corpus_id": 244117004,
    "ref_id": "b71",
    "citation_corpus_id": 52895589,
    "start": 17109,
    "end": 17125,
    "title": "HOW POWERFUL ARE GRAPH NEURAL NETWORKS?",
    "abstract": "Graph Neural Networks (GNNs) for representation learning of graphs broadly follow a neighborhood aggregation framework, where the representation vector of a node is computed by recursively aggregating and transforming feature vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs in capturing different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",
    "prev": "RELATED WORK\n\nGeometric Deep Learning.",
    "curr": "Graph Neural Networks (GNNs) are becoming the de facto choice for learning with graph data (Bruna et al., 2013;Defferrard et al., 2016;Kipf and Welling, 2016;Gilmer et al., 2017;Xu et al., 2018;.",
    "next": "Motivated by symmetries naturally occurring in different data types, architectures are tailored to explicitly incorporate such properties (Cohen and Welling, 2016a;Thomas et al., 2018;Fuchs et al., 2020;Finzi et al., 2020;Eismann et al., 2020;Satorras et al., 2021).",
    "query": "Are there any research papers on the theoretical analysis of expressive power in graph neural networks and their comparison to the Weisfeiler-Lehman graph isomorphism test?"
  },
  {
    "index": 686,
    "source_corpus_id": 233033761,
    "ref_id": "b9",
    "citation_corpus_id": 54101493,
    "start": 3683,
    "end": 3705,
    "title": "IMAGENET-TRAINED CNNS ARE BIASED TOWARDS TEXTURE; INCREASING SHAPE BIAS IMPROVES ACCURACY AND ROBUSTNESS",
    "abstract": "Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNettrained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on 'Stylized-ImageNet', a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation. de Beeck. Deep neural networks as a computational model for human shape sensitivity. DiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual cortex.",
    "prev": "First, obtaining photorealistic appearance features at the micro-level, such as texture and illumination, is challenging due to the limits of simulation complexity and rendering granularity.",
    "curr": "Without special treatment, CNNs tend to be biased towards textures (Geirhos et al., 2019) and suffer from badly learned representations on synthetic data.",
    "next": "Second, the common lack of texture and shape variations on synthetic images often leads to collapsed and trivial representations without any diversity.",
    "query": "Are there any studies on the bias of CNNs towards texture in image recognition and approaches to enhance shape-based representations for improved accuracy and robustness?"
  },
  {
    "index": 687,
    "source_corpus_id": 235367997,
    "ref_id": "b20",
    "citation_corpus_id": 52967399,
    "start": 26528,
    "end": 26549,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Training: There are two steps of training involved for B2F: 1) model agnostic autoregressive BSEQ prediction task to pre-train BSEQENC; 2) model-specific training for BFRP.",
    "curr": "Autoregressive BSEQ prediction: Pre-training on auxiliary tasks to improve the quality of latent embedding is a well-known technique for deep learning methods (Devlin et al., 2019;Radford et al., 2018).",
    "next": "We pre-train BSEQENC to predict the next values of backfill sequences {x (t r +1) t ,i } i∈F .",
    "query": "Are there any research papers on pre-training methods for language models using autoregressive tasks and bidirectional transformers to improve latent embeddings?"
  },
  {
    "index": 688,
    "source_corpus_id": 220514300,
    "ref_id": "b32",
    "citation_corpus_id": 990233,
    "start": 5556,
    "end": 5560,
    "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    "abstract": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.",
    "prev": "Structured Convolutional Models\n\nWe address neural models designed to learn from (multi-)relational, structured data, while exploiting some form of parameter sharing, as \"structured convolutional models\" 1 .",
    "curr": "Examples of such models include Recursive Neural Networks [33,23], Graph Neural Networks [16,28], and various Relational Neural Networks [34,26,5,31], designed for regular trees, graphs, and general relational structures, respectively.",
    "next": "They all utilize the idea of convolution (templating), where the same parameterized pattern is carried over different subparts of the data (representation) with the same local structure.",
    "query": "Are there any research papers on recursive or structured neural network models for semantic compositionality in sentiment analysis, particularly using specialized treebank datasets?"
  },
  {
    "index": 691,
    "source_corpus_id": 4117071,
    "ref_id": "b27",
    "citation_corpus_id": 11758569,
    "start": 8398,
    "end": 8420,
    "title": "UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks -demonstrating their applicability as general image representations.arXiv:1511.06434v2 [cs.LG] 7 Jan 2016Under review as a conference paper at ICLR 2016• We show that the generators have interesting vector arithmetic properties allowing for easy manipulation of many semantic qualities of generated samples.",
    "prev": "However there is an assumption that the class conditional distributions share some commonality and so information can be transferred from the source domain to the one-shot target domain.",
    "curr": "Generative Adversarial Networks (GAN) , and specifically Deep Convolutional GANs (DCGAN) (Radford et al., 2015) use of the ability to discriminate between true and generated examples as an objective, GAN approaches can learn complex joint densities.",
    "next": "Recent improvements in the optimization process  have reduced some of the failure modes of the GAN learning process.",
    "query": "Are there any research papers on unsupervised representation learning using deep convolutional generative adversarial networks (DCGANs) for image tasks?"
  },
  {
    "index": 693,
    "source_corpus_id": 255749563,
    "ref_id": "b62",
    "citation_corpus_id": 239616181,
    "start": 3323,
    "end": 3343,
    "title": "ILLITERATE DALL-E LEARNS TO COMPOSE",
    "abstract": "Although DALL·E has shown an impressive ability of composition-based systematic generalization in image generation, it requires the dataset of text-image pairs and the compositionality is provided by the text. In contrast, object-centric representation models like the Slot Attention model learn composable representations without the text prompt. However, unlike DALL·E its ability to systematically generalize for zero-shot generation is significantly limited. In this paper, we propose a simple but novel slot-based autoencoding architecture, called SLATE 1 , for combining the best of both worlds: learning object-centric representations that allows systematic generalization in zero-shot image generation without text. As such, this model can also be seen as an illiterate DALL·E model. Unlike the pixel-mixture decoders of existing object-centric representation models, we propose to use the Image GPT decoder conditioned on the slots for capturing complex interactions among the slots and pixels. In experiments, we show that this simple and easy-to-implement architecture not requiring a text prompt achieves significant improvement in in-distribution and out-of-distribution (zero-shot) image generation and qualitatively comparable or better slot-attention structure than the models based on mixture decoders. https://sites.google.com/view/slate-autoencoder",
    "prev": "Yet it is quite elusive what should be the appropriate structure and granularity of these tokens to support systematic generalization and how to obtain them, particularly in the unsupervised setting where the model should learn this ability only by observing.",
    "curr": "In visual scenes, binding has recently been pursued by object-centric learning methods through the spatial binding approach (Locatello et al., 2020;Singh et al., 2022a).",
    "next": "Spatial binding aims to divide a scene spatially into smaller areas so that each area contains a meaningful entity like an object.",
    "query": "Are there any studies that explore object-centric learning methods for achieving systematic generalization in zero-shot image generation without relying on text prompts?"
  },
  {
    "index": 695,
    "source_corpus_id": 36060542,
    "ref_id": "b21",
    "citation_corpus_id": 1450294,
    "start": 3838,
    "end": 3842,
    "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
    "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [5], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks[13].",
    "prev": "A third weakness, that both cases share, is inherited by the way in which they generate spatial filter-wise responses, i.e.",
    "curr": "either through deconvolutionbased heatmaps [22,23,28] or by up-scaling the activation maps at a given layer/filter to the image space [2,30].",
    "next": "On the one hand, deconvolution-based methods are able to produce heatmaps with high level of detail from any filter in the network.",
    "query": "Are there any research papers on visualization techniques for convolutional networks using deconvolution-based methods or up-scaling activation maps to generate saliency maps?"
  },
  {
    "index": 700,
    "source_corpus_id": 258833682,
    "ref_id": "b54",
    "citation_corpus_id": 604334,
    "start": 1738,
    "end": 1742,
    "title": "Intriguing properties of neural networks",
    "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
    "prev": "Under review.",
    "curr": "Introduction\n\nDeep Neural Network (DNN) has been shown to exhibit susceptibility to adversarial attacks [55], wherein intentionally crafted imperceptible perturbations introduced into the original input cause the model's predictions to be altered.",
    "next": "Among various defense methods [32,37,60,9,42], Adversarial Training (AT) [39] stands out as one of the most effective techniques [3,59] to enhance DNN's adversarial robustness.",
    "query": "Are there any research papers on adversarial attacks against deep neural networks and techniques to enhance their robustness through adversarial training?"
  },
  {
    "index": 702,
    "source_corpus_id": 262013578,
    "ref_id": "b21",
    "citation_corpus_id": 2134321,
    "start": 31943,
    "end": 31961,
    "title": "Published as a conference paper at ICLR 2016 DEEP COMPRESSION: COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING",
    "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35× to 49× without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9× to 13×; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35×, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49× from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3× to 4× layerwise speedup and 3× to 7× better energy efficiency.",
    "prev": null,
    "curr": "g (large) Transformers more efficient is currently a highly active area of research.Probably the currently most popular and practical approach is quantization, that is reducing the numerical precision of weights (and sometimes also activations) (Frantar et al., 2022;Dettmers & Zettlemoyer, 2022;Xiao et al., 2022).Further, there are also many works on Mixture-of-Expert (MoE) models, large ensembles of models/individual layers where each input is only processed by a small part, thus keeping the overall computation cost constant (Du et al., 2022;Fedus et al., 2022;Artetxe et al., 2022;Riquelme et al., 2021).MoEs are a form of dynamic activation sparsity, which is very different from the static weight sparsity that we study in this work; the former trades off increased memory for faster inference, whereas the latter reduces both inference and memory costs.In general, we note that quantization, MoEs and weight sparsity are all complementary techniques that may be stacked for compound gains (Han et al., 2016;Kurtic et al., 2022).",
    "next": "DISCUSSION\n\nLimitations.While we have conducted extensive experiments, for both vision and language domains, our results still have limitations, which we hope will be addressed in future work.",
    "query": "Are there any research papers on methods for improving the efficiency of large Transformers, such as quantization and Mixture-of-Expert models?"
  },
  {
    "index": 704,
    "source_corpus_id": 222272028,
    "ref_id": "b5",
    "citation_corpus_id": 54443381,
    "start": 7151,
    "end": 7174,
    "title": "EFFICIENT LIFELONG LEARNING WITH A-GEM",
    "abstract": "In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC  and other regularizationbased methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency. 1",
    "prev": "With the trending popularity of deep learning, continual learning has gained a critical importance because the catastrophic forgetting problem imposes key challenges to deploy deep learning models in various applications (e.g Lange et al., 2019;.",
    "curr": "A growing body of research has attempted to tackle this problem in recent years (e.g Parisi et al., 2018;Toneva et al., 2018;Nguyen et al., 2019;Farajtabar et al., 2019;Hsu et al., 2018;Rusu et al., 2016;Kirkpatrick et al., 2017;Zenke et al., 2017;Shin et al., 2017;Rolnick et al., 2018;Lopez-Paz & Ranzato, 2017;Chaudhry et al., 2018b;Riemer et al., 2018;Wallingford et al., 2020).",
    "next": "Among these works, our proposed MC-SGD bares most similarities to rehearsal based methods such us (e.g.",
    "query": "Are there any studies on efficient lifelong learning techniques to mitigate catastrophic forgetting, particularly those employing rehearsal-based methods or similar algorithmic approaches?"
  },
  {
    "index": 705,
    "source_corpus_id": 252408526,
    "ref_id": "b35",
    "citation_corpus_id": 247628243,
    "start": 3094,
    "end": 3098,
    "title": "EVALUATING DISTRIBUTIONAL DISTORTION IN NEURAL LANGUAGE MODELING",
    "abstract": "A fundamental characteristic of natural language is the high rate at which speakers produce novel expressions. Because of this novelty, a heavy-tail of rare events accounts for a significant amount of the total probability mass of distributions in language(Baayen, 2001). Standard language modeling metrics such as perplexity quantify the performance of language models (LM) in aggregate. As a result, we have relatively little understanding of whether neural LMs accurately estimate the probability of sequences in this heavy-tail of rare events. To address this gap, we develop a controlled evaluation scheme which uses generative models trained on natural data as artificial languages from which we can exactly compute sequence probabilities. Training LMs on generations from these artificial languages, we compare the sequence-level probability estimates given by LMs to the true probabilities in the target language. Our experiments reveal that LSTM and Transformer language models (i) systematically underestimate the probability of sequences drawn from the target language, and (ii) do so more severely for lessprobable sequences. Investigating where this probability mass went, (iii) we find that LMs tend to overestimate the probability of ill-formed (perturbed) sequences. In addition, we find that this underestimation behaviour (iv) is weakened, but not eliminated by greater amounts of training data, and (v) is exacerbated for target distributions with lower entropy.",
    "prev": "Prior work has developed a rough taxonomy of data properties, or metadata which different examples might exhibit, including but not limited to: noisy [68,71,62,63], atypical [25,10,21,60], challenging [24,3,8,49,2], prototypical or core subset selection [49,55,56,27] and out-of-distribution This approach can bring to light biases, mislabelled examples, and other dataset issues.",
    "curr": "[36].",
    "next": "While important progress has been made on some of these metadata categories individually, these categories are typically addressed in isolation reflecting an overly strong assumption that only one, known issue is at play in a given dataset.",
    "query": "Are there any studies on evaluating distributional distortion in neural language models, particularly methods to assess their ability to estimate probabilities of rare and ill-formed sequences?"
  },
  {
    "index": 707,
    "source_corpus_id": 247222761,
    "ref_id": "b26",
    "citation_corpus_id": 3507990,
    "start": 25089,
    "end": 25107,
    "title": "Published as a conference paper at ICLR 2018 META-LEARNING FOR SEMI-SUPERVISED FEW-SHOT CLASSIFICATION",
    "abstract": "In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples. Recent progress in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set. In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode. We consider two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more challenging situation where examples from other distractor classes are also provided. To address this paradigm, we propose novel extensions of Prototypical Networks (Snell et al., 2017) that are augmented with the ability to use unlabeled examples when producing prototypes. These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully. We evaluate these methods on versions of the Omniglot and miniImageNet benchmarks, adapted to this new framework augmented with unlabeled examples. We also propose a new split of ImageNet, consisting of a large set of classes, with a hierarchical structure. Our experiments confirm that our Prototypical Networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would. * Equal contribution. 1 See the following blog post for an overview:",
    "prev": "Furthermore to ensure that T > 0, we parametrized it with an exponential activation.",
    "curr": "FEW-SHOT IMAGE CLASSIFICATION\n\nWe evaluate COMLN on two standard few-shot image classification benchmarks: the miniImageNet (Vinyals et al., 2016) and the tieredImageNet datasets (Ren et al., 2018), both datasets being derived from ILSVRC-2012 (Russakovsky et al., 2015).",
    "next": "The process for creating tasks follows the standard procedure from the few-shot classification literature (Santoro et al., 2016), with distinct classes between the different splits.",
    "query": "Are there any research papers on semi-supervised few-shot image classification methods that incorporate unlabeled examples and meta-learning approaches?"
  },
  {
    "index": 708,
    "source_corpus_id": 218889280,
    "ref_id": "b6",
    "citation_corpus_id": 848112,
    "start": 2128,
    "end": 2131,
    "title": "TRAINING GANS WITH OPTIMISM",
    "abstract": "We address the issue of limit cycling behavior in training Generative Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for training Wasserstein GANs. Recent theoretical results have shown that optimistic mirror decent (OMD) can enjoy faster regret rates in the context of zero-sum games. WGANs is exactly a context of solving a zero-sum game with simultaneous noregret dynamics. Moreover, we show that optimistic mirror decent addresses the limit cycling problem in training WGANs. We formally show that in the case of bi-linear zero-sum games the last iterate of OMD dynamics converges to an equilibrium, in contrast to GD dynamics which are bound to cycle. We also portray the huge qualitative difference between GD and OMD dynamics with toy examples, even when GD is modified with many adaptations proposed in the recent literature, such as gradient penalty or momentum. We apply OMD WGAN training to a bioinformatics problem of generating DNA sequences. We observe that models trained with OMD achieve consistently smaller KL divergence with respect to the true underlying distribution, than models trained with GD variants. Finally, we introduce a new algorithm, Optimistic Adam, which is an optimistic variant of Adam. We apply it to WGAN training on CIFAR10 and observe improved performance in terms of inception score as compared to Adam.",
    "prev": "The interactions between learning agents make for vastly more complex mechanics: naively applying gradient descent on each loss is known to diverge even in simple bilinear games.",
    "curr": "Related Work A large number of methods have been proposed recently to tackle the question of local convergence, or global convergence in the convex setting: adaptations of single-loss algorithms such as Extragradient (EG) [2] and Optimistic Mirror Descent (OMD) [7], Consensus Optimization (CO) for GAN training [20], Competitive Gradient Descent (CGD) based on solving a bilinear approximation of the loss functions [26], Local Symplectic Surgery (LSS) for finding local Nash equilibria [19], Symplectic Gradient Adjustment (SGA) based on a novel decomposition of game mechanics [4,16], and opponent-shaping algorithms including Learning with Opponent-Learning Awareness (LOLA) [10] and its convergent counterpart, Stable Opponent Shaping (SOS) [17].",
    "next": "Each has shown promising theoretical implications and empirical results, but none offers insight into global convergence in the non-convex setting, which includes the vast majority of machine learning applications.",
    "query": "Are there any research papers on methods for improving convergence in GAN training using optimistic algorithms like Optimistic Mirror Descent, particularly in non-convex settings?"
  },
  {
    "index": 709,
    "source_corpus_id": 249209577,
    "ref_id": "b2",
    "citation_corpus_id": 219558760,
    "start": 10125,
    "end": 10144,
    "title": "On the Bottleneck of Graph Neural Networks and its Practical Implications",
    "abstract": "Graph neural networks (GNNs) were shown to effectively learn from highly structured data containing elements (nodes) with relationships (edges) between them. GNN variants differ in how each node in the graph absorbs the information flowing from its neighbor nodes. In this paper, we highlight an inherent problem in GNNs: the mechanism of propagating information between neighbors creates a bottleneck when every node aggregates messages from its neighbors. This bottleneck causes the over-squashing of exponentially-growing information into fixed-size vectors. As a result, the graph fails to propagate messages flowing from distant nodes and performs poorly when the prediction task depends on long-range information. We demonstrate that the bottleneck hinders popular GNNs from fitting the training data. We show that GNNs that absorb incoming edges equally, like GCN and GIN, are more susceptible to over-squashing than other GNN types. We further show that existing, extensively-tuned, GNN-based models suffer from over-squashing and that breaking the bottleneck improves state-of-the-art results without any hyperparameter tuning or additional weights.Preprint. Under review.",
    "prev": "Modern approaches implement variations of this idea as differentiable neural architectures (Gori et al., 2005;Scarselli et al., 2008;Kipf & Welling, 2017;Gilmer et al., 2017).",
    "curr": "This allows to express more elaborate forms of propagation (Li et al., 2018;Alon & Yahav, 2021) and aggregation (Wu et al., 2019;Xu et al., 2019;Li et al., 2016), including attention-based mechanisms (Veličković et al., 2018;Brody et al., 2022).",
    "next": "Nonetheless, a key result by Wu et al.",
    "query": "Are there any research papers on the bottleneck problem in graph neural networks and methods to address over-squashing in information propagation?"
  },
  {
    "index": 711,
    "source_corpus_id": 247292293,
    "ref_id": "b47",
    "citation_corpus_id": 3292002,
    "start": 25827,
    "end": 25852,
    "title": "GRAPH ATTENTION NETWORKS",
    "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-theart results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a proteinprotein interaction dataset (wherein test graphs remain unseen during training).",
    "prev": "3(c), we compare the test accuracy averaged on eight graphs when using different GNNs e.g.",
    "curr": "GCN, SGC (Wu et al., 2019) and GAT (Velickovic et al., 2018), for data generation (See Appendix G for more results).",
    "next": "The results verify that our approach achieves consistently superior performance in different cases.",
    "query": "Are there any research papers on graph attention networks and their performance comparison with other GNN models like GCN and SGC in graph-structured data tasks?"
  },
  {
    "index": 713,
    "source_corpus_id": 245334722,
    "ref_id": "b6",
    "citation_corpus_id": 6628106,
    "start": 32657,
    "end": 32676,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "The only exception of this is the fine-tuning done for the few-shot learning experiment in Section 7.",
    "curr": "We use Adam (Kingma & Ba, 2015) and cosine decay (Loshchilov & Hutter, 2016) with warmup.",
    "next": "For all experiments we used a embedding size of 512, only for few-shot classification we used 1024.",
    "query": "Are there any research papers on the use of Adam optimizer with cosine decay and warmup in stochastic optimization for machine learning tasks?"
  },
  {
    "index": 719,
    "source_corpus_id": 208268589,
    "ref_id": "b26",
    "citation_corpus_id": 5037032,
    "start": 7227,
    "end": 7247,
    "title": "ZERO-SHOT VISUAL IMITATION",
    "abstract": "The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both what and how to imitate.We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss.In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference.The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task.Our method is \"zero-shot\" in the sense that the agent never has access to expert actions during training or for the task demonstration at inference.We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot.Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance.Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/. * Denotes equal contribution.",
    "prev": "• Combine the local alignment and global alignment components into a reinforcement learning framework by a regularized policy update objective.",
    "curr": "RELATED WORK\n\nImitation learning is widely used in solving complicated tasks where pure reinforcement learning might suffer from high sample complexity, like robotics control (Le et al., 2017;Ye & Alterovitz, 2017;Pathak et al., 2018), autonomous vehicle (Fu et al.",
    "next": ";Pomerleau, 1989), and playing video game Pohlen et al., 2018;Aytar et al., 2018a).",
    "query": "Are there any research papers on zero-shot visual imitation methods that use exploration and forward consistency loss to learn goal-conditioned policies without expert supervision during training or inference?"
  },
  {
    "index": 720,
    "source_corpus_id": 252280667,
    "ref_id": "b1",
    "citation_corpus_id": 7167114,
    "start": 67362,
    "end": 67382,
    "title": "DEEP VARIATIONAL INFORMATION BOTTLENECK",
    "abstract": "We present a variational approximation to the information bottleneck ofTishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method \"Deep Variational Information Bottleneck\", or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack.",
    "prev": "GWAE (GMP), AUC=1.0000.",
    "curr": "Figure 3 :\n3The ROC curves of the OoD detection inMNIST (LeCun et al., 1998) against Omniglot (Lake et al., 2015\n\nFrom\nthe Information Bottleneck (IB)(Tishby et al., 1999)  point of view, the β-VAE objective is re-interpreted as the following optimization problem(Alemi et al., 2018;Achille & Soatto, 2018): maximize θ,φ I φ (z; y)\n\n\nZhao et al., 2019)  is an extension of VAE to prevent posterior collapse by the retention of data information in the latent variables.",
    "next": "The InfoVAE objective is the sum of the ELBO and the inference model mutual information I φ in Eq.",
    "query": "Are there any research papers on variational information bottleneck methods in deep learning for out-of-distribution detection and mutual information maximization?"
  },
  {
    "index": 722,
    "source_corpus_id": 258480276,
    "ref_id": "b14",
    "citation_corpus_id": 238583580,
    "start": 12824,
    "end": 12840,
    "title": "TOWARDS A UNIFIED VIEW OF PARAMETER-EFFICIENT TRANSFER LEARNING",
    "abstract": "Fine-tuning large pretrained language models on downstream tasks has become the de-facto learning paradigm in NLP.However, conventional approaches finetune all the parameters of the pretrained model, which becomes prohibitive as the model size and the number of tasks grow.Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance.While effective, the critical ingredients for success and the connections among the various methods are poorly understood.In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them.Specifically, we re-frame them as modifications to specific hidden states in pretrained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification.Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods.Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks. 1 * Equal Contribution.Order determined by random dice rolling.",
    "prev": "SegGPT (Wang et al., 2023) introduce a robust in-context learning paradigm and can segment any images by a given image-mask prompt.SEEM (Zou et al., 2023) further presents a general segmentation model prompted by multi-modal references, e.g., language and audio, incorporating versatile semantic knowledge.In this study, we introduce a new task termed personalized object segmentation, and annotate a new dataset PerSeg for evaluation.Instead of developing large segmentation models, our goal is to personalize them to segment user-provided objects in any poses or scenes.We propose two approaches, PerSAM and PerSAM-F, which efficiently customize SAM for personalized segmentation.",
    "curr": "Parameter-efficient Fine-tuning.Directly tuning the entire foundation models on downstream tasks can be computationally expensive and memory-intensive, posing challenges for resourceconstrained applications.To address this issue, recent works have focused on developing parameterefficient methods (Sung et al., 2022;He et al., 2022;Rebuffi et al., 2017;Qin & Eisner, 2021) to freeze the weights of foundation models and append small-scale modules for fine-tuning.Prompt tuning (Lester et al., 2021;Zhou et al., 2022;Jia et al., 2022;Liu et al., 2021) suggests using learnable soft prompts alongside frozen models to perform specific downstream tasks, achieving more competitive performance with scale and robust domain transfer compared to full model tuning.",
    "next": "Low-Rank Adaption (LoRA) (Hu et al., 2021;Cuenca & Paul, 2023;Zhang et al., 2023b;Hedegaard et al., 2022) injects trainable rank decomposition matrices concurrently to each pre-trained weight, which significantly reduces the number of learnable parameters required for downstream tasks.Adapters (Houlsby et al., 2019;Pfeiffer et al., 2020;Lin et al., 2020;Chen et al., 2022) are designed to be inserted between layers of the original transformer, introducing lightweight MLPs for feature transformation.Different from existing works, we adopt a more efficient adaption met",
    "query": "Are there any research papers on parameter-efficient transfer learning methods for fine-tuning large pretrained models, such as prompt tuning, low-rank adaptation, and adapters, and their unified frameworks?"
  },
  {
    "index": 724,
    "source_corpus_id": 2721941,
    "ref_id": "b0",
    "citation_corpus_id": 748227,
    "start": 2947,
    "end": 2968,
    "title": "Generating Sentences from a Continuous Space",
    "abstract": "The standard recurrent neural network language model (rnnlm) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an rnn-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.",
    "prev": "We find particularly promising the opportunity to leverage deep generative models for search in high-dimensional discrete spaces (Gómez-Bombarelli et al., 2016b;Kusner et al., 2017).",
    "curr": "Discrete search is at the heart of problems in drug discovery (Gómez-Bombarelli et al., 2016a), natural language processing (Bowman et al., 2016;Guimaraes et al., 2017), and symbolic regression (Kusner et al., 2017).",
    "next": "The application of deep modeling to search involves 'lifting' the search from the discrete space to a continuous space, via an autoencoder (Rumelhart et al., 1985).",
    "query": "Are there any research papers on using variational autoencoders to address discrete search problems in high-dimensional spaces by converting them into continuous representations, with applications in natural language processing, drug discovery, and symbolic regression?"
  },
  {
    "index": 727,
    "source_corpus_id": 261100669,
    "ref_id": "b6",
    "citation_corpus_id": 246652381,
    "start": 2548,
    "end": 2550,
    "title": "BACKDOOR DEFENSE VIA DECOUPLING THE TRAIN- ING PROCESS",
    "abstract": "Recent studies have revealed that deep neural networks (DNNs) are vulnerable to backdoor attacks, where attackers embed hidden backdoors in the DNN model by poisoning a few training samples. The attacked model behaves normally on benign samples, whereas its prediction will be maliciously changed when the backdoor is activated. We reveal that poisoned samples tend to cluster together in the feature space of the attacked DNN model, which is mostly due to the endto-end supervised training paradigm. Inspired by this observation, we propose a novel backdoor defense via decoupling the original end-to-end training process into three stages. Specifically, we first learn the backbone of a DNN model via self-supervised learning based on training samples without their labels. The learned backbone will map samples with the same ground-truth label to similar locations in the feature space. Then, we freeze the parameters of the learned backbone and train the remaining fully connected layers via standard training with all (labeled) training samples. Lastly, to further alleviate side-effects of poisoned samples in the second stage, we remove labels of some 'low-credible' samples determined based on the learned model and conduct a semi-supervised fine-tuning of the whole model. Extensive experiments on multiple benchmark datasets and DNN models verify that the proposed defense is effective in reducing backdoor threats while preserving high accuracy in predicting benign samples. Our code is available at https://github.comPublished as a conference paper at ICLR 2022 door trigger (dubbed poisoned samples) tend to cluster together in the feature space. We reveal that this phenomenon is mostly due to the end-to-end supervised training paradigm. Specifically, the excessive learning capability allows DNNs to learn features about the backdoor trigger, while the DNNs can shrink the distance between poisoned samples in the feature space and connect the learned trigger-related features with the target label by the end-to-end supervised training. Based on this understanding, we propose to decouple the end-to-end training process for the backdoor defense. Specifically, we treat the DNNs as two disjoint parts, including a feature extractor (i.e., backbone) and a simple classifier (i.e., the remaining fully connected layers). We first learn the purified feature extractor via self-supervised learning (Kolesnikov et al., 2019; Chen et al., 2020a; Jing & Tian,  2020)  with unlabeled training samples (obtained by removing their labels), and then learn the simple classifier via standard supervised training process based on the learned feature extractor and all training samples. The strong data augmentations involved in the self-supervised learning damage trigger patterns, making them unlearnable during representation learning; and the decoupling process further disconnects trigger patterns and the target label. Accordingly, hidden backdoors cannot be successfully created even the model is trained on the poisoned dataset based on our defense.Moreover, we further reveal that the representation of poisoned samples generated by the purified extractor is significantly different from those generated by the extractor learned with standard training process. Specifically, the poisoned sample lies closely to samples with its ground-truth label instead of the target label. This phenomenon makes the training of the simple classifier similar to label-noise learning (Wang et al., 2019b; Ma et al., 2020; Berthon et al., 2021). As such, we first filter high-credible training samples (i.e., training samples that are most probably to be benign) and then use those samples as labeled samples and the remaining part to form unlabeled samples to fine-tune the whole model via semi-supervised learning (Rasmus et al., 2015;Berthelot et al., 2019;Sohn et al., 2020). This approach is to further reduce the adverse effects of poisoned samples.The main contributions of this paper are three-fold. (1) We reveal that the backdoor is embedded in the feature space, which is mostly due to the end-to-end supervised training paradigm.(2) Based on our understanding, we propose a decoupling-based backdoor defense (DBD) to alleviate the threat of poisoning-based backdoor attacks. (3) Experiments on classical benchmark datasets are conducted, which verify the effectiveness of our defense.",
    "prev": "del, our defense, BaDExpert (Backdoor Input Detection with Backdoor Expert), effectively mitigates 17 SOTA backdoor attacks while minimally impacting clean utility.The effectiveness of BaDExpert has been verified on multiple datasets (CIFAR10, GTSRB, and ImageNet) across multiple model architectures (ResNet, VGG,\n\nIntroduction\n\nA prominent security concern of deep neural networks (DNNs) is the threat of backdoor attacks [1,2], wherein an adversary embeds hidden behaviors (backdoors) into a model through techniques such as data poisoning [3] or weights tampering [4].During inference, such a backdoor remains dormant when processing benign inputs but can be activated by trigger-planted backdoor samples devised by attackers.Upon activation, the compromised model produces anomalous outputs, which could lead to severe security breaches.",
    "curr": "The existing literature has extensively explored defensive strategies against backdoor attacks, with a significant focus on development-stage defenses [5,6,7,8].These defenses are operated before and during the model training process, primarily targeting data-poisoning-based attacks [3].",
    "next": "In this work, we rather focus on post-development defenses that operate after the model development [9,10,11,12].Given an arbitrary model that may potentially be backdoored, post-development defenses tackle the challenge of secure deployment head-on, without knowing how the model was generated.Implementing such defenses faces non-trivial technical challenges.From a methodological point of view, these defenses do not have access to the training dataset or information about training dynamics (such as gradient updates or loss information) and thus forfeit rich information that could aid in system defense.For example, approaches that directly analyze poisoned datasets [5,8] or the backdoor training dynamics [6,7] cannot be applied.",
    "query": "Are there any research papers on post-development backdoor defense techniques that decouple the training process using self-supervised learning and semi-supervised fine-tuning to mitigate poisoned samples in deep neural networks?"
  },
  {
    "index": 733,
    "source_corpus_id": 13046179,
    "ref_id": "b7",
    "citation_corpus_id": 6706414,
    "start": 1596,
    "end": 1621,
    "title": "Published as a conference paper at ICLR 2015 EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES",
    "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
    "prev": "INTRODUCTION\n\nWhen machine learning classifiers are employed in real-world tasks, they tend to fail when the training and test distributions differ.",
    "curr": "Worse, these classifiers often fail silently by providing highconfidence predictions while being woefully incorrect (Goodfellow et al., 2015;Amodei et al., 2016).",
    "next": "Classifiers failing to indicate when they are likely mistaken can limit their adoption or cause serious accidents.",
    "query": "Are there any research papers on adversarial examples in machine learning and methods to explain and mitigate their impact on classifier reliability?"
  },
  {
    "index": 734,
    "source_corpus_id": 254877510,
    "ref_id": "b6",
    "citation_corpus_id": 52967399,
    "start": 1882,
    "end": 1903,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "g data sets to obtain a single model that performs well both across all data set domains and can generalize on out-ofdomain data.We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models.Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling.Further, we find that our method is a promising alternative to multi-task learning that can preserve or sometimes improve over the individual models without access to the training data.Finally, model merging is more efficient than training a multi-task model, thus making it applicable to a wider set of scenarios.",
    "curr": "1\n\nINTRODUCTION\n\nThe dominant paradigm for solving NLP tasks ranging from classification to sequence tagging involves fine-tuning a pretrained language model (PLM) using task-specific labeled data (Devlin et al., 2019;He et al., 2021).This results in specialized models that are explicitly trained to run inference over a single domain and task.Multi-task learning has shown that leveraging information across domains or tasks can be beneficial if the data sets, data set size and algorithms are well selected (Phang et al., 2018;Pruksachatkun et al., 2020;Poth et al., 2021;Weller et al., 2022).Combining knowledge of multiple data sets in a single model can lead to better overall performance on in-domain data (Poth et al., 2021), can better generalize on out-of-domain data (Wang et al., 2020b) and results in a model that is more practical and parameter efficient than maintaining specialized models.",
    "next": "However, the multi-task learning setup suffers from two practical limitations.First, the training process requires access to the original labeled data, which may not be realistic as annotated data may be private to the agent fine-tuning the model which can happen in order to ensure data or annotation privacy o",
    "query": "Are there any research papers on dataless model merging techniques for combining pre-trained language models without access to training data, as an alternative to multi-task learning in NLP tasks?"
  },
  {
    "index": 735,
    "source_corpus_id": 13298214,
    "ref_id": "b13",
    "citation_corpus_id": 8395799,
    "start": 12275,
    "end": 12299,
    "title": "Language Understanding for Text-based Games using Deep Reinforcement Learning",
    "abstract": "In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. We evaluate our approach on two game worlds, comparing against baselines using bag-ofwords and bag-of-bigrams for state representations. Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive representations. 1",
    "prev": "Recent work has developed on-policy RL methods such as advantage actor-critic that use asynchronous training of multiple agents in parallel (Mnih et al., 2016).",
    "curr": "Recurrent networks have also been successfully incorporated to enable state disambiguation in partially observable environments (Koutnik et al., 2013;Hausknecht & Stone, 2015;Mnih et al., 2016;Narasimhan et al., 2015).",
    "next": ", 1999)) to transfer between navigation tasks.",
    "query": "Are there any research papers on deep reinforcement learning methods for state representation and action policy learning in text-based games with partial observability?"
  },
  {
    "index": 736,
    "source_corpus_id": 1257772,
    "ref_id": "b0",
    "citation_corpus_id": 11212020,
    "start": 1804,
    "end": 1826,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.",
    "curr": "INTRODUCTION\n\nRecent advances in machine learning and deep neural networks enabled researchers to solve multiple important practical problems like image, video, text classification and others (Krizhevsky et al., 2012;Hinton et al., 2012;Bahdanau et al., 2015).",
    "next": "However, machine learning models are often vulnerable to adversarial manipulation of their input intended to cause incorrect classification (Dalvi et al., 2004).",
    "query": "Are there any research papers on neural machine translation methods that use encoder-decoder architectures with soft alignment mechanisms instead of fixed-length vectors?"
  },
  {
    "index": 738,
    "source_corpus_id": 259274820,
    "ref_id": "b0",
    "citation_corpus_id": 235436185,
    "start": 25378,
    "end": 25381,
    "title": "BEIT: BERT Pre-Training of Image Transformers",
    "abstract": "We introduce a self-supervised vision representation model BEIT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT [DCLT19] developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e., image patches (such as 16×16 pixels), and visual tokens (i.e., discrete tokens). We first \"tokenize\" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEIT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods.",
    "prev": "is typically utilized for downstream initialization.",
    "curr": "Except for the common supervised pre-training [16,10,24], contrastive learning (CL) [4,14,6,12] and masked image modeling (MIM) [1,44,13] dominate the recent research.",
    "next": "The former is achieved by pulling close the features of two different augment views of the input image.",
    "query": "Are there any research papers on self-supervised pre-training methods for vision Transformers using masked image modeling and contrastive learning techniques?"
  },
  {
    "index": 739,
    "source_corpus_id": 265037895,
    "ref_id": "b14",
    "citation_corpus_id": 3366315,
    "start": 2403,
    "end": 2423,
    "title": "Published as a conference paper at ICLR 2018 SPECTRAL NORMALIZATION FOR GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. The code with Chainer (Tokui et al., 2015), generated images and pretrained models are available at https://github.com/pfnet-research/sngan_ projection.Published as a conference paper at ICLR 2018• Lipschitz constant is the only hyper-parameter to be tuned, and the algorithm does not require intensive tuning of the only hyper-parameter for satisfactory performance. • Implementation is simple and the additional computational cost is small.",
    "prev": "The original GAN is, however, highly unstable and often suffers from mode collapse.",
    "curr": "Much of recent researches has focused on improving the stability of GANs (Radford et al., 2015;Heusel et al., 2017;Miyato et al., 2018;Karras et al., 2018).",
    "next": "On the theoretical aspect, Nagarajan & Kolter (2017) proved that gradient based training of the original GAN is locally stable.",
    "query": "Are there any research papers on methods for stabilizing generative adversarial networks using spectral normalization techniques?"
  },
  {
    "index": 740,
    "source_corpus_id": 56895453,
    "ref_id": "b33",
    "citation_corpus_id": 2705742,
    "start": 8430,
    "end": 8449,
    "title": "Learning Language Games through Interaction",
    "abstract": "We introduce a new language learning setting relevant to building adaptive natural language interfaces. It is inspired by Wittgenstein's language games: a human wishes to accomplish some task (e.g., achieving a certain configuration of blocks), but can only communicate with a computer, who performs the actual actions (e.g., removing all red blocks). The computer initially knows nothing about language and therefore must learn it from scratch through interaction, while the human adapts to the computer's capabilities. We created a game called SHRDLURN in a blocks world and collected interactions from 100 people playing it. First, we analyze the humans' strategies, showing that using compositionality and avoiding synonyms correlates positively with task performance. Second, we compare computer strategies, showing that modeling pragmatics on a semantic parsing model accelerates learning for more strategic players.",
    "prev": "However, they have non-symmetric addition of agents in communication channel and are restricted to only cooperative scenarios.",
    "curr": "In contrast, a lot of work has focused on understanding agents' communication content; mostly in discrete settings with two agents (Wang et al., 2016;Havrylov & Titov, 2017;Kottur et al., 2017;Lazaridou et al., 2016;.",
    "next": "Lazaridou et al.",
    "query": "Are there any research papers on language learning through interactive games that analyze human and computer communication strategies in adaptive natural language interfaces?"
  },
  {
    "index": 741,
    "source_corpus_id": 252917667,
    "ref_id": "b6",
    "citation_corpus_id": 231698861,
    "start": 7500,
    "end": 7503,
    "title": "Published as a conference paper at ICLR 2021 LEARNING PARAMETRISED GRAPH SHIFT OPERATORS",
    "abstract": "In many domains data is currently represented as graphs and therefore, the graph representation of this data becomes increasingly important in machine learning. Network data is, implicitly or explicitly, always represented using a graph shift operator (GSO) with the most common choices being the adjacency, Laplacian matrices and their normalisations. In this paper, a novel parametrised GSO (PGSO) is proposed, where specific parameter values result in the most commonly used GSOs and message-passing operators in graph neural network (GNN) frameworks. The PGSO is suggested as a replacement of the standard GSOs that are used in state-of-the-art GNN architectures and the optimisation of the PGSO parameters is seamlessly included in the model training. It is proved that the PGSO has real eigenvalues and a set of real eigenvectors independent of the parameter values and spectral bounds on the PGSO are derived. PGSO parameters are shown to adapt to the sparsity of the graph structure in a study on stochastic blockmodel networks, where they are found to automatically replicate the GSO regularisation found in the literature. On several real-world datasets the accuracy of state-of-theart GNN architectures is improved by the inclusion of the PGSO in both nodeand graph-classification tasks. * Equal contribution.Published as a conference paper at ICLR 2021On different tasks and datasets, the choice between the different representations encoded by the different graph shift operator matrices has shown to be a consequential decision. Due to the past successful approaches that use different GSOs for different tasks and datasets, it is natural to assume that there is no single optimal representation for all scenarios. Finding an optimal representation of network data could contribute positively to a range of learning tasks such as node and graph classification or community detection. Fundamental to this search is an answer to Question 1. In addition, we pose the following second research question.Question 2: Can we learn such an optimal representation to encode graph structure in a numerically stable and computationally efficient way?The utilisation of a GSO as a topology representation is currently a hand-engineered choice of normalised variants of the adjacency matrix. Thus, the learnable representation of node interactions is transferred into either convolutional filters(Kipf & Welling, 2017;Hamilton et al., 2017a)or attention weights(Veličković et al., 2018), keeping the used GSO constant. In this work, we suggest a parametrisation of the GSO. Specific parameter values in our proposed parametrised (and differentiable) GSO result in the most commonly used GSOs, namely the adjacency, unnormalised Laplacian and both normalised Laplacian matrices, and GNN aggregation functions, e.g., the averaging and summation message passing operations. The beauty of this innovation is that it can be seamlessly included in both message passing and convolutional GNN architectures. Optimising the operator parameters will allow us to find answers to our two research questions.",
    "prev": "Together with Dasoulas et al.",
    "curr": "[7] and Sahbi [33], our work can be listed as a theoretically grounded way to learn the GSO.",
    "next": "Justified by formulas grounded in Monte-Carlo analysis, we show how to compensate for the nonuniformity in the sampling when computing non-uniform geometric GSOs.",
    "query": "Are there any research papers on learning parametrised graph shift operators with theoretical grounding and computational efficiency in graph neural networks?"
  },
  {
    "index": 745,
    "source_corpus_id": 259212224,
    "ref_id": "b5",
    "citation_corpus_id": 208547755,
    "start": 2027,
    "end": 2030,
    "title": "Published as a conference paper at ICLR 2020 DREAM TO CONTROL: LEARNING BEHAVIORS BY LATENT IMAGINATION",
    "abstract": "Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.",
    "prev": "Introduction\n\nThe ability to predict the outcome of an agent's action over long horizons is a crucial unresolved challenge in Reinforcement Learning (RL) [1][2][3][4].",
    "curr": "This is especially important in model-based RL and planning, where deriving a policy from the learned dynamics models allows one to efficiently accomplish a wide variety of tasks in an environment [5][6][7][8][9].",
    "next": "In fact, state-of-the-art model-free techniques also rely on dynamics models to learn a better representation for downstream value prediction tasks [30].",
    "query": "Are there any research papers on model-based reinforcement learning approaches that utilize learned dynamics models for long-horizon planning and behavior generation?"
  },
  {
    "index": 749,
    "source_corpus_id": 239016655,
    "ref_id": "b2",
    "citation_corpus_id": 231648113,
    "start": 23035,
    "end": 23062,
    "title": "ZERO-COST PROXIES FOR LIGHTWEIGHT NAS",
    "abstract": "Neural Architecture Search (NAS) is quickly becoming the standard methodology to design neural network models. However, NAS is typically compute-intensive because multiple models need to be evaluated before choosing the best one. To reduce the computational power and time needed, a proxy task is often used for evaluating each model instead of full training. In this paper, we evaluate conventional reduced-training proxies and quantify how well they preserve ranking between multiple models during search when compared with the rankings produced by final trained accuracy. We propose a series of zero-cost proxies, based on recent pruning literature, that use just a single minibatch of training data to compute a model's score. Our zero-cost proxies use 3 orders of magnitude less computation but can match and even outperform conventional proxies. For example, Spearman's rank correlation coefficient between final validation accuracy and our best zero-cost proxy on NAS-Bench-201 is 0.82, compared to 0.61 for EcoNAS (a recently proposed reduced-training proxy). Finally, we use these zerocost proxies to enhance existing NAS search algorithms such as random search, reinforcement learning, evolutionary search and predictor-based search. For all search methodologies and across three different NAS datasets, we are able to significantly improve sample efficiency, and thereby decrease computation, by using our zero-cost proxies. For example on NAS-Bench-101, we achieved the same accuracy 4× quicker than the best previous result.",
    "prev": "Experimental setup details are included in Appendix A.3.",
    "curr": "To align with the experimental setup of prior work (Abdelfattah et al., 2021b;Mellor et al., 2021), we use two criteria to evaluate the correlations between different metrics and test accuracies across approximately 20k networks:\n\nSpearman's ρ (Daniel et al., 1990) characterizes the monotonic relationships between two variables.",
    "next": "The correlation score is restricted in range [-1, 1], where ρ = 1 denotes a perfect positive monotonic relationship and ρ = −1 denotes a perfect negative monotonic relationship.",
    "query": "Are there any research papers on zero-cost proxies for lightweight neural architecture search that use Spearman's correlation to evaluate their effectiveness in improving sample efficiency?"
  },
  {
    "index": 752,
    "source_corpus_id": 232290577,
    "ref_id": "b43",
    "citation_corpus_id": 196183669,
    "start": 10921,
    "end": 10940,
    "title": "Generating Fluent Adversarial Examples for Natural Languages",
    "abstract": "Efficiently building an adversarial attacker for natural language processing (NLP) tasks is a real challenge. Firstly, as the sentence space is discrete, it is difficult to make small perturbations along the direction of gradients. Secondly, the fluency of the generated examples cannot be guaranteed. In this paper, we propose MHA, which addresses both problems by performing Metropolis-Hastings sampling, whose proposal is designed with the guidance of gradients. Experiments on IMDB and SNLI show that our proposed MHA outperforms the baseline model on attacking capability. Adversarial training with MHA also leads to better robustness and performance.",
    "prev": "Different from this work, MARS is built upon the general MCMC sampling framework, which allows further enhancement with adaptive proposal learning to edit molecular graphs efficiently.",
    "curr": "Actually, generating instances from a discrete space with MCMC sampling methods is previously employed in various other applications, e.g., generating natural language sentences under various constraints Zhang et al., 2019;.",
    "next": "PROPOSED MARS APPROACH\n\nIn this section, we present the MArkov moleculaR Sampling method (MARS) for multi-objective molecular design.",
    "query": "Are there any research papers on methods for generating fluent adversarial examples in natural language processing using Metropolis-Hastings sampling or MCMC techniques?"
  },
  {
    "index": 753,
    "source_corpus_id": 43964415,
    "ref_id": "b10",
    "citation_corpus_id": 38796293,
    "start": 1781,
    "end": 1784,
    "title": "Published as a conference paper at ICLR 2018 DEEP GRADIENT COMPRESSION: REDUCING THE COMMUNICATION BANDWIDTH FOR DISTRIBUTED TRAINING",
    "abstract": "Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD are redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during this compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270× to 600× without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.",
    "prev": "Communication has been reported to be a major bottleneck for many large scale deep learning applications, see e.g.",
    "curr": "[3,11,24,31].",
    "next": "Mini-batch parallel SGD addresses this issue by increasing compute before communication.",
    "query": "Are there any research papers on methods to reduce communication overhead in distributed deep learning training through gradient compression techniques?"
  },
  {
    "index": 754,
    "source_corpus_id": 251765117,
    "ref_id": "b27",
    "citation_corpus_id": 218487034,
    "start": 3201,
    "end": 3222,
    "title": "On Faithfulness and Factuality in Abstractive Summarization",
    "abstract": "It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.",
    "prev": "respects; for example, they are not able to retroactively modify or refine their own outputs.",
    "curr": "Beyond that, they are hard to control (Korbak et al., 2022) and verifying their outputs is challenging as they often hallucinate content (Maynez et al., 2020;Shuster et al., 2021;Nakano et al., 2021) and lack the ability to explain their intentions.",
    "next": "All of this makes it very difficult for humans to collaborate with such models for writing coherent, factual texts.",
    "query": "Are there any studies that investigate methods for improving faithfulness and factuality in abstractive summarization, particularly addressing hallucination and control mechanisms in neural text generation models?"
  },
  {
    "index": 755,
    "source_corpus_id": 235254358,
    "ref_id": "b43",
    "citation_corpus_id": 52895589,
    "start": 2361,
    "end": 2377,
    "title": "HOW POWERFUL ARE GRAPH NEURAL NETWORKS?",
    "abstract": "Graph Neural Networks (GNNs) for representation learning of graphs broadly follow a neighborhood aggregation framework, where the representation vector of a node is computed by recursively aggregating and transforming feature vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs in capturing different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",
    "prev": "In a GNN, each node iteratively updates its state by interacting with its neighbors.",
    "curr": "GNN variants (Wu et al., 2019;Xu et al., 2019;Li et al., 2016) mostly differ in how each node aggregates and combines the representations of its neighbors with its own.",
    "next": "pioneered the use of attention-based neighborhood aggregation, in one of the most common GNN variants -Graph Attention Network (GAT).",
    "query": "Are there any research papers on the theoretical analysis of graph neural networks' expressive power and their ability to distinguish different graph structures?"
  },
  {
    "index": 756,
    "source_corpus_id": 221470196,
    "ref_id": "b12",
    "citation_corpus_id": 3463260,
    "start": 7365,
    "end": 7385,
    "title": "Published as a conference paper at ICLR 2018 DISTRIBUTED PRIORITIZED EXPERIENCE REPLAY",
    "abstract": "We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on distributed prioritized experience replay in deep reinforcement learning?"
  },
  {
    "index": 757,
    "source_corpus_id": 248986748,
    "ref_id": "b31",
    "citation_corpus_id": 247476396,
    "start": 8286,
    "end": 8302,
    "title": "Published as a conference paper at ICLR 2022 REVISITING DESIGN CHOICES IN OFFLINE MODEL-BASED REINFORCEMENT LEARNING",
    "abstract": "Offline reinforcement learning enables agents to leverage large pre-collected datasets of environment transitions to learn control policies, circumventing the need for potentially expensive or unsafe online data collection. Significant progress has been made recently in offline model-based reinforcement learning, approaches which leverage a learned dynamics model. This typically involves constructing a probabilistic model, and using the model uncertainty to penalize rewards where there is insufficient data, solving for a pessimistic MDP that lower bounds the true MDP. Existing methods, however, exhibit a breakdown between theory and practice, whereby pessimistic return ought to be bounded by the total variation distance of the model from the true dynamics, but is instead implemented through a penalty based on estimated model uncertainty. This has spawned a variety of uncertainty heuristics, with little to no comparison between differing approaches. In this paper, we compare these heuristics, and design novel protocols to investigate their interaction with other hyperparameters, such as the number of models, or imaginary rollout horizon. Using these insights, we show that selecting these key hyperparameters using Bayesian Optimization produces superior configurations that are vastly different to those currently used in existing hand-tuned state-of-the-art methods, and result in drastically stronger performance. . Deep reinforcement learning in a handful of trials using probabilistic dynamics models. . MOReL : Modelbased offline reinforcement learning. In Advances in Neural Information Processing Systems. 2020.Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit Q-learning, 2021.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any studies that compare uncertainty estimation strategies and hyperparameter configurations in offline model-based reinforcement learning algorithms?"
  },
  {
    "index": 759,
    "source_corpus_id": 264289264,
    "ref_id": "b44",
    "citation_corpus_id": 247446904,
    "start": 9874,
    "end": 9893,
    "title": "Published as a conference paper at ICLR 2023 PROTEIN REPRESENTATION LEARNING BY GEOMETRIC STRUCTURE PRETRAINING",
    "abstract": "Learning effective protein representations is critical in a variety of tasks in biology such as predicting protein function or structure. Existing approaches usually pretrain protein language models on a large number of unlabeled amino acid sequences and then finetune the models with some labeled data in downstream tasks. Despite the effectiveness of sequence-based approaches, the power of pretraining on known protein structures, which are available in smaller numbers only, has not been explored for protein property prediction, though protein structures are known to be determinants of protein function. In this paper, we propose to pretrain protein representations according to their 3D structures. We first present a simple yet effective encoder to learn the geometric features of a protein. We pretrain the protein graph encoder by leveraging multiview contrastive learning and different self-prediction tasks. Experimental results on both function prediction and fold classification tasks show that our proposed pretraining methods outperform or are on par with the state-of-the-art sequence-based methods, while using much less pretraining data. Our implementation is available at https://github.com/ DeepGraphLearning/GearNet.",
    "prev": "mising methods in the realm of protein design, which encompasses protein diffusion (Yim et al., 2023) and inverse folding (Gao et al., 2022a;Jendrusch et al., 2021;Wu et al., 2021;Ovchinnikov & Huang, 2021;Dauparas et al., 2022;Ingraham et al., 2019;Hsu et al., 2022;Gao et al., 2023b;Derevyanko et al., 2018).Specifically, a protein diffusion model first generates the backbone structure of a protein, followed by an inverse folding network that designs the corresponding sequence for this backbone.The feasibility of both these steps has been experimentally validated through cryo-electron microscopy (Watson et al., 2023;Dauparas et al., 2022), marking a significant breakthrough in the field of protein design.However, while protein diffusion methods based on frame representation achieve significant success, in these methods, atom representation is absent, rendering previous general purpose encoders unusable.",
    "curr": "GENERAL PURPOSE ENCODER\n\nIn the past, numerous encoders (Hermosilla et al., 2020;Zhang et al., 2022;Hermosilla & Ropinski, 2022;Veličković et al., 2017;Baldassarre et al., 2021;Li et al., 2022;Gao et al., 2022b;Shroff et al., 2019;Dumortier et al., 2022;McPartlon et al., 2022;Cao et al., 2021;Anishchenko et al., 2021;Karimi et al., 2020;Zhang et al., 2020;Wang et al., 2022b;Derevyanko et al., 2018) have been proposed for tasks such as model quality assessment (Townshend et al., 2021) and fold classification (Hou et al., 2018), where atomic information is available.However, these methods are not suitable for protein design tasks where atomic representations of proteins are unavailable.For instance, GVP (Jing et al., 2020) transforms input atomic coordinates into vector and scalars variables as the network input, facilitating the model's SE(3) invariance.Meanwhile, Wang et al.",
    "next": "(2022a); Jin et al.",
    "query": "Are there any research papers on protein representation learning using geometric structure pretraining and encoders that handle 3D structural features without relying on atomic representations?"
  },
  {
    "index": 760,
    "source_corpus_id": 244527086,
    "ref_id": "b17",
    "citation_corpus_id": 3566136,
    "start": 12140,
    "end": 12169,
    "title": "Published as a conference paper at ICLR 2018 RELATIONAL NEURAL EXPECTATION MAXIMIZATION: UNSUPERVISED DISCOVERY OF OBJECTS AND THEIR INTERACTIONS",
    "abstract": "Common-sense physical reasoning is an essential ingredient for any intelligent agent operating in the real-world. For example, it can be used to simulate the environment, or to infer the state of parts of the world that are currently unobserved. In order to match real-world conditions this causal knowledge must be learned without access to supervised data. To address this problem we present a novel method that learns to discover objects and model their physical interactions from raw visual images in a purely unsupervised fashion. It incorporates prior knowledge about the compositional nature of human perception to factor interactions between object-pairs and learn efficiently. On videos of bouncing balls we show the superior modelling capabilities of our method compared to other unsupervised neural approaches that do not incorporate such prior knowledge. We demonstrate its ability to handle occlusion and show that it can extrapolate learned knowledge to scenes with different numbers of objects. * Work performed while at IDSIA.",
    "prev": "We minimize the pixel-wise squared reconstruction error (averaged over the batch), summed over both the temporal and spatial dimensions:\nL rec = T t=1 y t − y true t 2 .",
    "curr": "(4)\n\nRELATED WORK\n\nObject-centric representation learning There is a rich literature on learning object representations from static scenes (Greff et al., 2016;Eslami et al., 2016;Greff et al., 2017;2019;Burgess et al., 2019;Engelcke et al., 2020;Crawford & Pineau, 2019;Lin et al., 2020;Locatello et al., 2020;Du et al., 2021a) or videos (van Steenkiste et al., 2018;Kosiorek et al., 2018;Stelzner et al., 2019;Kipf et al., 2020;Crawford & Pineau, 2020;Creswell et al., 2021) without explicit supervision.",
    "next": "PSGNet (Bear et al., 2020) learns to decompose static images or individual frames from a video into hierarchical scene graphs using motion information estimated from neighboring video frames.",
    "query": "Are there any studies on unsupervised methods for discovering objects and their interactions in video data using relational models?"
  },
  {
    "index": 761,
    "source_corpus_id": 258762142,
    "ref_id": "b10",
    "citation_corpus_id": 251252882,
    "start": 9708,
    "end": 9711,
    "title": "Prompt-to-Prompt Image Editing with Cross Attention Control",
    "abstract": "Recent large-scale text-driven synthesis models have attracted much attention thanks to their remarkable capabilities of generating highly diverse images that follow given text prompts. Such text-based synthesis methods are particularly appealing to humans who are used to verbally describe their intent. Therefore, it is only natural to extend the text-driven image synthesis to text-driven image editing. Editing is challenging for these generative models, since an innate property of an editing technique is to preserve most of the original image, while in the text-based models, even a small modification of the text prompt often leads to a completely different outcome. State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region. In this paper, we pursue an intuitive prompt-toprompt editing framework, where the edits are controlled by text only. To this end, we analyze a text-conditioned model in depth and observe that the cross-attention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt. With this observation, we present several applications which monitor the image synthesis by editing the textual prompt only. This includes localized editing by replacing a word, global editing by adding a specification, and even delicately controlling the extent to which a word is reflected in the image. We present our results over diverse images and prompts, demonstrating high-quality synthesis and fidelity to the edited prompts.",
    "prev": "This leads to the emergence of the recently popular topic, subject-driven text-to-image generation [8,27].",
    "curr": "Text-Guided Image Editing Text-guided image editing [2,3,11,15,17,20] aims to edit an input image according to the given textual descriptions.",
    "next": "SDEdit [20] and Blended-Diffusion [1] blend the noisy input to the generated image in the diffusion denoising process.",
    "query": "Are there any research papers on text-guided image editing techniques that use cross-attention control for prompt-based editing without spatial masks?"
  },
  {
    "index": 763,
    "source_corpus_id": 203593355,
    "ref_id": "b22",
    "citation_corpus_id": 604334,
    "start": 1572,
    "end": 1593,
    "title": "Intriguing properties of neural networks",
    "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on the counter-intuitive properties of deep neural networks, such as their lack of unit interpretability and vulnerability to adversarial perturbations?"
  },
  {
    "index": 764,
    "source_corpus_id": 108296236,
    "ref_id": "b42",
    "citation_corpus_id": 1957433,
    "start": 2187,
    "end": 2212,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "INTRODUCTION\n\nLearned embeddings form the basis for many state-of-the-art learning systems.",
    "curr": "Word embeddings like word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), fastText (Bojanowski et al., 2017), and ELMo (Peters et al., 2018) are ubiquitous in natural language processing, where they are used for tasks like machine translation (Neubig et al., 2018), while graph embeddings (Nickel et al., 2016) like node2vec (Grover & Leskovec, 2016) are used to represent knowledge graphs and pre-trained image models (Simon et al., 2016) appear in many computer vision pipelines.",
    "next": "An effective embedding should capture the semantic structure of the data with high fidelity, in a way that is amenable to downstream tasks.",
    "query": "Are there any research papers on methods for learning word embeddings by combining global matrix factorization and local context window approaches, leveraging co-occurrence matrices to capture semantic structure in natural language processing tasks?"
  },
  {
    "index": 768,
    "source_corpus_id": 245704504,
    "ref_id": "b9",
    "citation_corpus_id": 13890001,
    "start": 18181,
    "end": 18200,
    "title": "Published as a conference paper at ICLR 2017 NEURAL PHOTO EDITING WITH INTROSPECTIVE AD- VERSARIAL NETWORKS",
    "abstract": "The increasingly photorealistic sample quality of generative image models suggests their feasibility in applications beyond image generation. We present the Neural Photo Editor, an interface that leverages the power of generative neural networks to make large, semantically coherent changes to existing images. To tackle the challenge of achieving accurate reconstructions without loss of feature quality, we introduce the Introspective Adversarial Network, a novel hybridization of the VAE and GAN. Our model efficiently captures long-range dependencies through use of a computational block based on weight-shared dilated convolutions, and improves generalization performance with Orthogonal Regularization, a novel weight regularization method. We validate our contributions on CelebA, SVHN, and CIFAR-100, and produce samples and reconstructions with high visual fidelity.",
    "prev": "GANs inversion and editing.",
    "curr": "Another mainstream approach to image editing involves GAN inversion (Zhu et al., 2016;Brock et al., 2017), where the input is first projected into the latent space of an unconditional GAN before synthesizing a new image from the modified latent code.",
    "next": "Several methods have been proposed in this direction, including fine-tuning network weights for each image (Bau et al., 2019a;Pan et al., 2020;Roich et al., 2021), choosing better or multiple layers to project and edit (Abdal et al., 2019;Gu et al., 2020;Wu et al., 2021), designing better encoders (Richardson et al., 2021;Tov et al., 2021), modeling image corruption and transformations (Anirudh et al., 2020;Huh et al., 2020), and discovering meaningful latent directions (Shen et al., 2020;Goetschalckx et al., 2019;Jahanian et al., 2020;Härkönen et al., 2020).",
    "query": "Are there any research papers on GAN inversion techniques for image editing, particularly those utilizing introspective adversarial networks and methods for neural photo editing?"
  },
  {
    "index": 771,
    "source_corpus_id": 220265858,
    "ref_id": "b38",
    "citation_corpus_id": 6359641,
    "start": 40818,
    "end": 40822,
    "title": "Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism",
    "abstract": "We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multiway, multilingual model on ten language pairs from WMT'15 simultaneously and observe clear performance improvements over models trained on only one language pair. In particular, we observe that the proposed model significantly improves the translation quality of low-resource language pairs.",
    "prev": "As a result, the run-time overhead is typically negligible, even for convolutional networks where masking and padding are heavily used.",
    "curr": "We chose multilingual neural machine translation (MT) [39,40,41] to validate our design for efficient training with GShard.",
    "next": "Multilingual MT, which is an inherently multi-task learning problem, aims at building a single neural network for the goal of translating multiple language pairs simultaneously.",
    "query": "Are there any research papers on multilingual neural machine translation approaches that use shared attention mechanisms for efficient training and improved performance across multiple language pairs?"
  },
  {
    "index": 773,
    "source_corpus_id": 3047732,
    "ref_id": "b34",
    "citation_corpus_id": 3281198,
    "start": 3782,
    "end": 3809,
    "title": "A UNIFIED PERSPECTIVE ON MULTI-DOMAIN AND MULTI-TASK LEARNING",
    "abstract": "In this paper, we provide a new neural-network based perspective on multi-task learning (MTL) and multi-domain learning (MDL). By introducing the concept of a semantic descriptor, this framework unifies MDL and MTL as well as encompassing various classic and recent MTL/MDL algorithms by interpreting them as different ways of constructing semantic descriptors. Our interpretation provides an alternative pipeline for zero-shot learning (ZSL), where a model for a novel class can be constructed without training data. Moreover, it leads to a new and practically relevant problem setting of zero-shot domain adaptation (ZSDA), which is the analogous to ZSL but for novel domains: A model for an unseen domain can be generated by its semantic descriptor. Experiments across this range of problems demonstrate that our framework outperforms a variety of alternatives.",
    "prev": "For example, one may want simultaneously predict a person's age (task one: multi-class classification or regression) as well as identify their gender (task two: binary classification) from a face image.",
    "curr": "Multi-Domain Learning: Each \"task\" corresponds to a dataset [Yang and Hospedales, 2015].",
    "next": "For example, one jointly can train a multi-class object recognition model for images captured by an HD camera (task/domain one) and for those captured by a webcam (task/domain two).",
    "query": "Are there any research papers on unified frameworks for multi-task and multi-domain learning using semantic descriptors, with applications in zero-shot learning and domain adaptation?"
  },
  {
    "index": 774,
    "source_corpus_id": 233474778,
    "ref_id": "b15",
    "citation_corpus_id": 14337532,
    "start": 27296,
    "end": 27323,
    "title": "SGDR: STOCHASTIC GRADIENT DESCENT WITH WARM RESTARTS",
    "abstract": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradientbased optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at",
    "prev": "Our proposed principles are preferable guidance for designing an efficient module for temporal modeling.",
    "curr": "We use SGD with momentum 0.9 and cosine learning rate schedule (Loshchilov & Hutter, 2017) to train the entire network.",
    "next": "The first 10 epochs are used for warm-up (Goyal et al., 2017a) to overcome early optimization difficulty.",
    "query": "Are there any research papers on warm restart techniques in stochastic gradient descent for improving training performance in deep neural networks?"
  },
  {
    "index": 775,
    "source_corpus_id": 247778993,
    "ref_id": "b4",
    "citation_corpus_id": 54458698,
    "start": 1952,
    "end": 1978,
    "title": "Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning",
    "abstract": "Text-based adventure games provide a platform on which to explore reinforcement learning in the context of a combinatorial action space, such as natural language. We present a deep reinforcement learning architecture that represents the game state as a knowledge graph which is learned during exploration. This graph is used to prune the action space, enabling more efficient exploration. The question of which action to take can be reduced to a question-answering task, a form of transfer learning that pre-trains certain parts of our architecture. In experiments using the TextWorld framework, we show that our proposed technique can learn a control policy faster than baseline alternatives. We have also open-sourced our code at https://github.com/rajammanabrolu/KG-DQN.",
    "prev": "Our experiments show that the proposed approach consistently improves existing methods, obtains good out-of-distribution generalization, and achieves new state-of-the-art results on widely used environments.",
    "curr": "INTRODUCTION\n\nText-based games (TBGs) have emerged as key benchmarks for studying how reinforcement learning (RL) agents can tackle the challenges of grounded language understanding, partial observability, large action spaces, and out-of-distribution generalization Ammanabrolu & Riedl, 2019).",
    "next": "While we have indeed made some progress on these fronts in recent years Adhikari et al., 2020;Murugesan et al., 2021b;a), these agents are still very inefficient and suffer from insufficient generalization to novel environments.",
    "query": "Are there any research papers on graph-based deep reinforcement learning methods for text-based games that improve exploration efficiency and out-of-distribution generalization?"
  },
  {
    "index": 779,
    "source_corpus_id": 14711954,
    "ref_id": "b2",
    "citation_corpus_id": 3130692,
    "start": 5513,
    "end": 5535,
    "title": "Learning to Compose Neural Networks for Question Answering",
    "abstract": "We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural module network, achieves state-of-theart results on benchmark datasets in both visual and structured domains.",
    "prev": "We show that our approach substantially outperforms purely unsupervised methods that do not provide the learner with any task-specific guidance about how hierarchies should be deployed, and further that the specific use of sketches to parameterize modular subpolicies makes better use of sketches than conditioning on them directly.",
    "curr": "The present work may be viewed as an extension of recent approaches for learning compositional deep architectures from structured program descriptors (Andreas et al., 2016;Reed & de Freitas, 2016).",
    "next": "Here we focus on learning in interactive environments.",
    "query": "Are there any research papers on methods for learning to compose neural networks using structured program descriptors and reinforcement learning for question answering tasks in visual and structured domains?"
  },
  {
    "index": 781,
    "source_corpus_id": 247849778,
    "ref_id": "b15",
    "citation_corpus_id": 236459945,
    "start": 2063,
    "end": 2081,
    "title": "Enhancing Content Preservation in Text Style Transfer Using Reverse Attention and Conditional Layer Normalization",
    "abstract": "Text style transfer aims to alter the style (e.g., sentiment) of a sentence while preserving its content. A common approach is to map a given sentence to content representation that is free of style, and the content representation is fed to a decoder with a target style. Previous methods in filtering style completely remove tokens with style at the token level, which incurs the loss of content information. In this paper, we propose to enhance content preservation by implicitly removing the style information of each token with reverse attention, and thereby retain the content. Furthermore, we fuse content information when building the target style representation, making it dynamic with respect to the content. Our method creates not only styleindependent content representation, but also content-dependent style representation in transferring style. Empirical results show that our method outperforms the state-of-the-art baselines by a large margin in terms of content preservation. In addition, it is also competitive in terms of style transfer accuracy and fluency.",
    "prev": "INTRODUCTION\n\nText style transfer (TST) models learn how to transfer the style of text from source to target while preserving the style-independent content (John et al., 2019;Fu et al., 2018).",
    "curr": "Existing TST methods perform well when transferring simple styles, such as sentiment; however, they tend to do a poor job on more abstract and subtle styles, such as formality and political stance (Lee et al., 2021;Fu et al., 2019b).",
    "next": "The lack of parallel datasets is one of the main bottlenecks for text style transfer tasks.",
    "query": "Are there any studies that explore techniques for enhancing content preservation in text style transfer, especially for abstract styles, using methods like reverse attention and conditional layer normalization?"
  },
  {
    "index": 783,
    "source_corpus_id": 12130431,
    "ref_id": "b18",
    "citation_corpus_id": 14124313,
    "start": 2824,
    "end": 2851,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "We believe that our method sheds new insights on how to make deep CNNs to be applicable on mobile or embedded devices.",
    "curr": "The code is available at https://github.com\n\nINTRODUCTION\n\nDeep convolutional neural networks (CNNs) have demonstrated record breaking results on a variety of computer vision tasks such as image classification (Krizhevsky et al., 2012;Simonyan & Zisserman, 2015), face recognition (Taigman et al., 2014;Sun et al., 2014), semantic segmentation (Long et al., 2015;Chen et al., 2015a) and object detection (Girshick, 2015;Ren et al., 2015).",
    "next": "Regardless of the availability of significantly improved training resources such as abundant annotated data, powerful computational platforms and diverse training frameworks, the promising results of deep CNNs are mainly attributed to the large number of learnable parameters, ranging from tens of millions to even hundreds of millions.",
    "query": "Are there any research papers on the impact of network depth in deep convolutional networks for large-scale image recognition?"
  },
  {
    "index": 785,
    "source_corpus_id": 204824061,
    "ref_id": "b3",
    "citation_corpus_id": 52967399,
    "start": 1224,
    "end": 1244,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "* Work done during an internship at Facebook AI Research.",
    "curr": "INTRODUCTION\n\nThe size of modern neural sequence models (Gehring et al., 2017;Vaswani et al., 2017;Devlin et al., 2019) can amount to billions of parameters (Radford et al., 2019).",
    "next": "For example, the winning entry of the WMT'19 news machine translation task in English-German used an ensemble totaling two billion parameters .",
    "query": "Are there any research papers on pre-training of bidirectional transformer models for natural language processing tasks with large parameter counts?"
  },
  {
    "index": 786,
    "source_corpus_id": 238419267,
    "ref_id": "b6",
    "citation_corpus_id": 11212020,
    "start": 8264,
    "end": 8287,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "THE PROBABILISTIC ATTENTION MECHANISM\n\nWe now give a high-level explanation of our results; the detailed formulations are in Section 2.",
    "curr": "Introduced in (Bahdanau et al., 2015) and later used to define the transformer architecture (Vaswani et al., 2017), in the NLP context, attention maps a matrix of queries Q, a matrix of keys K, and a matrix of values V to the quantity Softmax(QK )V , where the softmax function (defined below) is applied row-wise to QK .",
    "next": "Just as the authors of (Petersen & Voigtlaender, 2020;Zhou, 2020) focus on the simplified versions of practically implementable ConvNets in the study of approximation theory of deep ConvNets (e.g.",
    "query": "Are there any research papers on probabilistic attention mechanisms in neural machine translation and their application in encoder-decoder architectures for alignment and translation?"
  },
  {
    "index": 787,
    "source_corpus_id": 88517649,
    "ref_id": "b8",
    "citation_corpus_id": 52889459,
    "start": 2223,
    "end": 2226,
    "title": "LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS",
    "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by truncating the latent space. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128×128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.3 and Fréchet Inception Distance (FID) of 9.6, improving over the previous best IS of 52.52 and FID of 18.65.",
    "prev": "By capturing the mechanisms behind the data generation process, one can reason about data probabilistically, access and traverse the low-dimensional manifold the data is assumed to live on, and ultimately generate new data.",
    "curr": "It is therefore not surprising that learning generative models has gained momentum in applications like chemistry [16,25], NLP [8,46] and computer vision [9,48].",
    "next": "Variational Autoencoders (VAEs) [27,38] allow for a principled probabilistic way to model high-dimensional distributions.",
    "query": "Are there any research papers on large-scale training of generative adversarial networks for high-fidelity natural image synthesis with methods to improve stability and sample quality?"
  },
  {
    "index": 788,
    "source_corpus_id": 258741298,
    "ref_id": "b62",
    "citation_corpus_id": 202539551,
    "start": 2101,
    "end": 2123,
    "title": "Language Models as Knowledge Bases?",
    "abstract": "Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as \"fillin-the-blank\" cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-theart pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https: //github.com/facebookresearch/LAMA.",
    "prev": "nowledge cards serve as parametric repositories that are selected at inference time to generate background knowledge for the base LLM.We then propose three content selectors to dynamically select and retain information in documents generated by knowledge cards, specifically controlling for relevance, brevity, and factuality of outputs.Finally, we propose two complementary integration approaches to augment the base LLM with the (relevant, factual) knowledge curated from the specialized LMs.Through extensive experiments, we demonstrate that KNOWLEDGE CARD achieves state-of-the-art performance on six benchmark datasets.Ultimately, KNOWLEDGE CARD framework enables dynamic synthesis and updates of knowledge from diverse domains.Its modularity will ensure that relevant knowledge can be continuously updated through the collective efforts of the research community.",
    "curr": "INTRODUCTION\n\nLarge language models (LLMs) have demonstrated an impressive ability to encode world knowledge in model parameters (Petroni et al., 2019;Roberts et al., 2020).However, they still face various challenges in knowledge-intensive tasks and contexts: they suffer from hallucination (Kryściński et al., 2020;Pagnoni et al., 2021;Ji et al., 2023), struggle to encode long-tail facts (Kandpal et al., 2023;Mallen et al., 2023), and could not be easily updated with new and emerging knowledge (De Cao et al., 2021;Hase et al., 2021).Existing works propose addressing these limitations through retrieval augmentation or generated knowledge prompting.Retrieval-augmented LMs (Guu et al., 2020;Borgeaud et al., 2022;Shi et al., 2023) employ retrieval systems to fetch relevant documents from a general and fixed retrieval corpus (e.g., Wikipedia or the Pile (Gao et al., 2020)), leveraging external knowledge from non-parametric sources to aid LLM generation.Generated knowledge prompting approaches (Shin et al., 2020;Liu et al., 2022a;Sun et al., 2022) prompt LLMs to incorporate and generate contextual documents to encourage knowledge-aware gene",
    "next": null,
    "query": "Are there any research papers on language models functioning as knowledge bases, their ability to store relational knowledge, and their performance in open-domain question answering without fine-tuning?"
  },
  {
    "index": 789,
    "source_corpus_id": 252118863,
    "ref_id": "b12",
    "citation_corpus_id": 52967399,
    "start": 6941,
    "end": 6945,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Early practices revolve around contrastive learning [6-8, 18, 20] where the model output features of images transformed by different data augmentations are pulled together.",
    "curr": "With the development of Masked Language Modeling (MLM) in language pre-training [13], researchers also introduce the training strategy of masked reconstruction to visual pre-training.",
    "next": "BEiT [3] uses the DALL-E [32] to encode an image patch as the target for model reconstruction.",
    "query": "Are there any research papers on applying masked reconstruction strategies from natural language processing, such as those used in masked language modeling, to visual pre-training tasks?"
  },
  {
    "index": 792,
    "source_corpus_id": 264306022,
    "ref_id": "b3",
    "citation_corpus_id": 9128667,
    "start": 7926,
    "end": 7951,
    "title": "STOCHASTIC VARIATIONAL VIDEO PREDICTION",
    "abstract": "Predicting the future in real-world settings, particularly from raw sensory observations such as images, is exceptionally challenging. Real-world events can be stochastic and unpredictable, and the high dimensionality and complexity of natural images require the predictive model to build an intricate understanding of the natural world. Many existing methods tackle this problem by making simplifying assumptions about the environment. One common assumption is that the outcome is deterministic and there is only one plausible future. This can lead to low-quality predictions in real-world settings with stochastic dynamics. In this paper, we develop a stochastic variational video prediction (SV2P) method that predicts a different possible future for each sample of its latent variables. To the best of our knowledge, our model is the first to provide effective stochastic multi-frame prediction for real-world videos. We demonstrate the capability of the proposed method in predicting detailed future frames of videos on multiple real-world datasets, both action-free and action-conditioned. We find that our proposed method produces substantially improved video predictions when compared to the same model without stochasticity, and to other stochastic video prediction methods. Our SV2P implementation will be open sourced upon publication.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on stochastic variational methods for video prediction that address uncertainty in real-world scenarios?"
  },
  {
    "index": 793,
    "source_corpus_id": 204907203,
    "ref_id": "b7",
    "citation_corpus_id": 52967399,
    "start": 2655,
    "end": 2676,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Besides theft of intellectual property, extracted models may leak sensitive information about the training data (Tramèr et al., 2016) or be used to generate adversarial examples that evade the model served by the API (Papernot et al., 2017).",
    "curr": "With the recent success of contextualized pretrained representations for transfer learning, NLP models created by finetuning ELMo  and BERT (Devlin et al., 2019) have become increasingly popular .",
    "next": "Contextualized pretrained representations boost performance and reduce sample complexity (Yogatama et al., 2019), and typically require only a shallow task-specific network-sometimes just a single layer as in BERT.",
    "query": "Are there any research papers on the use of contextualized pretrained representations like BERT and ELMo for transfer learning in natural language processing tasks?"
  },
  {
    "index": 794,
    "source_corpus_id": 238408158,
    "ref_id": "b39",
    "citation_corpus_id": 3587087,
    "start": 1885,
    "end": 1904,
    "title": "Collective Entity Disambiguation with Structured Gradient Tree Boosting",
    "abstract": "We present a gradient-tree-boosting-based structured learning model for jointly disambiguating named entities in a document. Gradient tree boosting is a widely used machine learning algorithm that underlies many topperforming natural language processing systems. Surprisingly, most works limit the use of gradient tree boosting as a tool for regular classification or regression problems, despite the structured nature of language. To the best of our knowledge, our work is the first one that employs the structured gradient tree boosting (SGTB) algorithm for collective entity disambiguation. By defining global features over previous disambiguation decisions and jointly modeling them with local features, our system is able to produce globally optimized entity assignments for mentions in a document. Exact inference is prohibitively expensive for our globally normalized model. To solve this problem, we propose Bidirectional Beam Search with Gold path (BiBSG), an approximate inference algorithm that is a variant of the standard beam search algorithm. BiBSG makes use of global information from both past and future to perform better local search. Experiments on standard benchmark datasets show that SGTB significantly improves upon published results. Specifically, SGTB outperforms the previous state-of-the-art neural system by near 1% absolute accuracy on the popular AIDA-CoNLL dataset. 1",
    "prev": "INTRODUCTION\n\nWe consider the most general form of entity linking (EL) in which a system, given a document, must both extract entity mentions and link the mentions to their corresponding entries in a knowledge base (KB).",
    "curr": "EL is a foundational building block in automatic text understanding with applications to question answering (QA) (Ferrucci, 2012), information retrieval (Xiong et al., 2017;Hasibi et al., 2016;Balog et al., 2013;Reinanda et al., 2015), and commercial recommendation systems (Yang et al., 2018;Slawski, 2015).",
    "next": "The output space in EL is intractably large.",
    "query": "Are there any research papers on collective entity disambiguation using structured gradient tree boosting and structured learning approaches?"
  },
  {
    "index": 795,
    "source_corpus_id": 254096162,
    "ref_id": "b18",
    "citation_corpus_id": 235313882,
    "start": 10712,
    "end": 10731,
    "title": "NODE-GAM: NEURAL GENERALIZED ADDITIVE MODEL FOR INTERPRETABLE DEEP LEARNING",
    "abstract": "Deployment of machine learning models in real high-risk settings (e.g. healthcare) often depends not only on the model's accuracy but also on its fairness, robustness, and interpretability. Generalized Additive Models (GAMs) are a class of interpretable models with a long history of use in these high-risk domains, but they lack desirable features of deep learning such as differentiability and scalability. In this work, we propose a neural GAM (NODE-GAM) and neural GA 2 M (NODE-GA 2 M) that scale well and perform better than other GAMs on large datasets, while remaining interpretable compared to other ensemble and deep learning models. We demonstrate that our models find interesting patterns in the data. Lastly, we show that we improve model accuracy via self-supervised pre-training, an improvement that is not possible for non-differentiable GAMs.",
    "prev": "By using neural networks to learn the transformations from raw feature values to effects, Neural Additive Models (NAMs) maintain the interpretability of GAMs with more expressive effects (Agarwal et al., 2021).",
    "curr": "Pairwise interactions between effects can be included for more complex models (Yang et al., 2021;Chang et al., 2022), though complications arise involving degeneracy and the number of effect terms to inspect grows rapidly.",
    "next": "Regularization can be used to encourage sparsity in the ML model so that fewer components-weights in the network (Ng, 2004) or even features (Lemhadri et al., 2021)-conspire to produce the final prediction.",
    "query": "Are there any research papers on neural generalized additive models (NODE-GAM) that combine interpretability with deep learning techniques for high-risk applications like healthcare?"
  },
  {
    "index": 796,
    "source_corpus_id": 257482484,
    "ref_id": "b13",
    "citation_corpus_id": 233169183,
    "start": 3858,
    "end": 3877,
    "title": "PLASTICINELAB: A SOFT-BODY MANIPULATION BENCHMARK WITH DIFFERENTIABLE PHYSICS",
    "abstract": "Simulated virtual environments serve as one of the main driving forces behind developing and evaluating skill learning algorithms. However, existing environments typically only simulate rigid body physics. Additionally, the simulation process usually does not provide gradients that might be useful for planning and control optimizations. We introduce a new differentiable physics benchmark called PasticineLab, which includes a diverse collection of soft body manipulation tasks. In each task, the agent uses manipulators to deform the plasticine into a desired configuration. The underlying physics engine supports differentiable elastic and plastic deformation using the DiffTaichi system, posing many underexplored challenges to robotic agents. We evaluate several existing reinforcement learning (RL) methods and gradient-based methods on this benchmark. Experimental results suggest that 1) RL-based approaches struggle to solve most of the tasks efficiently; 2) gradient-based approaches, by optimizing open-loop control sequences with the built-in differentiable physics engine, can rapidly find a solution within tens of iterations, but still fall short on multi-stage tasks that require long-term planning. We expect that PlasticineLab will encourage the development of novel algorithms that combine differentiable physics and RL for more complex physics-based skill learning tasks. PlasticineLab is publicly available 1 . * This work was done during an internship at the MIT-IBM Watson AI Lab.",
    "prev": "INTRODUCTION\n\nDeformable object manipulation (DOM) is a crucial area of research with broad applications, from household (Maitin-Shepard et al., 2010;Miller et al., 2011;Ma et al., 2022) to industrial settings (Miller et al., 2012;.",
    "curr": "To aid in algorithm development and prototyping, several DOM benchmarks Huang et al., 2021) have been developed using deformable object simulators.",
    "next": "However, the high dimensional state and action spaces remain a significant challenge to DOM.",
    "query": "Are there any research papers on benchmarks for soft-body manipulation using differentiable physics and addressing challenges in high-dimensional state and action spaces?"
  },
  {
    "index": 797,
    "source_corpus_id": 238407774,
    "ref_id": "b44",
    "citation_corpus_id": 212859361,
    "start": 7588,
    "end": 7604,
    "title": "DROPEDGE: TOWARDS DEEP GRAPH CONVOLU- TIONAL NETWORKS ON NODE CLASSIFICATION",
    "abstract": "Over-fitting and over-smoothing are two main obstacles of developing deep Graph Convolutional Networks (GCNs) for node classification. In particular, over-fitting weakens the generalization ability on small dataset, while over-smoothing impedes model training by isolating output representations from the input features with the increase in network depth. This paper proposes DropEdge, a novel and flexible technique to alleviate both issues. At its core, DropEdge randomly removes a certain number of edges from the input graph at each training epoch, acting like a data augmenter and also a message passing reducer. Furthermore, we theoretically demonstrate that DropEdge either reduces the convergence speed of over-smoothing or relieves the information loss caused by it. More importantly, our DropEdge is a general skill that can be equipped with many other backbone models (e.g. GCN, ResGCN, GraphSAGE, and JKNet) for enhanced performance. Extensive experiments on several benchmarks verify that DropEdge consistently improves the performance on a variety of both shallow and deep GCNs. The effect of DropEdge on preventing over-smoothing is empirically visualized and validated as well. Codes are released on https://github.com/DropEdge/DropEdge.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on techniques to prevent over-smoothing and over-fitting in deep graph convolutional networks for node classification using edge dropping or data augmentation methods?"
  },
  {
    "index": 798,
    "source_corpus_id": 252918265,
    "ref_id": "b34",
    "citation_corpus_id": 3725815,
    "start": 6802,
    "end": 6821,
    "title": "Self-Attention with Relative Position Representations",
    "abstract": "Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al.(2017)achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graphlabeled inputs.",
    "prev": "(2006) (CTC) loss, identical SpecAugment (Park et al., 2019) parameters, and Adagrad optimizer (Duchi et al., 2011).",
    "curr": "The acoustic model is the same transformer architecture that was introduced in slimIPL, except that we encode positions with either absolute sinusoidal positional embedding (Vaswani et al., 2017) or the recently proposed CAPE (Likhomanenko et al., 2021b) instead of relative positional embedding (Shaw et al., 2018).",
    "next": "This allows us to speed up training (by 2-3x) and decrease the memory footprint significantly.",
    "query": "Are there any research papers on methods for improving training efficiency in transformer models through the use of relative position representations in sequence-to-sequence tasks?"
  },
  {
    "index": 799,
    "source_corpus_id": 240070972,
    "ref_id": "b30",
    "citation_corpus_id": 13900194,
    "start": 5143,
    "end": 5162,
    "title": "Certifying Some Distributional Robustness with Principled Adversarial Training",
    "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations. By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.* Equal contribution 1 Note that z, u ∈ R m , so trivially the dimensionality of the solution grows polynomially.2We assume that X is a subset of normed vector space.",
    "prev": "The classic threat model of adversarial robustness considers an inductive setting where a model is learned at the training time and fixed, and then at the test time, an attacker attempts to thwart the fixed model with adversarially perturbed input.",
    "curr": "This gives rise to the adversarial training Sinha et al., 2018;Carmon et al., 2019) to enhance adversarial robustness.",
    "next": "Going beyond the inductive threat model, there has been emerging interest in using transductive learning (Vapnik, 1998) 1 for adversarial robustness (Goldwasser et al., 2020;Wu et al., 2020b;Wang et al., 2021).",
    "query": "Are there any research papers on distributionally robust optimization methods for certifying adversarial robustness in neural networks?"
  },
  {
    "index": 800,
    "source_corpus_id": 235731899,
    "ref_id": "b9",
    "citation_corpus_id": 3708505,
    "start": 2281,
    "end": 2299,
    "title": "DEEP NEURAL NETWORKS AS GAUSSIAN PROCESSES",
    "abstract": "A deep fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP) in the limit of infinite network width. This correspondence enables exact Bayesian inference for neural networks on regression tasks by means of straightforward matrix computations. For single hiddenlayer networks, the covariance function of this GP has long been known. Recently, kernel functions for multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified the correspondence between using these kernels as the covariance function for a GP and performing fully Bayesian prediction with a deep neural network. In this work, we derive this correspondence and develop a computationally efficient pipeline to compute the covariance functions. We then use the resulting GP to perform Bayesian inference for deep neural networks on MNIST and CIFAR-10. We find that the GP-based predictions are competitive and can outperform neural networks trained with stochastic gradient descent. We observe that the trained neural network accuracy approaches that of the corresponding GP-based computation with increasing layer width, and that the GP uncertainty is strongly correlated with prediction error. We connect our observations to the recent development of signal propagation in random neural networks. * Both authors contributed equally to this work. † Work done as a member of the Google Brain Residency program (g.co/brainresidency). 1 Throughout this paper, we assume the conditions on the parameter distributions and nonlinearities are such that the Central Limit Theorem will hold; for instance, that the weight variance is scaled inversely proportional to the layer width.",
    "prev": "Neal (1996) showed that a shallow but infinitely-wide Bayesian Neural Network (BNN) with random weights and biases corresponds to a GP.",
    "curr": "This result was extended to fully-connected deep neural networks of any depth (Lee et al., 2018;Matthews et al., 2018), which are shown to converge to GPs as the width grows.",
    "next": "Similar results were later obtained for deep Convolutional Neural Networks (CNNs) (Novak et al., 2018;Garriga-Alonso et al., 2019) and attention networks (Hron et al., 2020).",
    "query": "Are there any research papers on the correspondence between deep neural networks and Gaussian processes in the context of Bayesian inference and parameter prior distributions?"
  },
  {
    "index": 801,
    "source_corpus_id": 54203451,
    "ref_id": "b7",
    "citation_corpus_id": 20472740,
    "start": 2023,
    "end": 2042,
    "title": "NATURAL LANGUAGE INFERENCE OVER INTERACTION SPACE",
    "abstract": "Natural Language Inference (NLI) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such architecture, Densely Interactive Inference Network (DIIN), demonstrates the state-of-the-art performance on large scale NLI copora and large-scale NLI alike corpus. It's noteworthy that DIIN achieve a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI; Williams et al. 2017) dataset with respect to the strongest published system.",
    "prev": "INTRODUCTION\n\nIn recent years, neural network based models have become the workhorse of natural language understanding and generation.",
    "curr": "They empower industrial systems in machine translation (Wu et al., 2016) and text generation (Kannan et al., 2016), also showing state-of-the-art performance on numerous benchmarks including Recognizing Textual Entailment (RTE) (Gong et al., 2017), Visual Question Answering (VQA) (Jiang et al., 2018), and Reading Comprehension (Wang et al., 2018).",
    "next": "Despite these successes, a growing body of literature suggests that these approaches do not generalize outside of the specific distributions on which they are trained, something that is necessary for a language understanding system to be widely deployed in the real world.",
    "query": "Are there any research papers on neural network architectures for natural language inference that leverage interaction space and dense attention mechanisms to improve generalization and performance on challenging datasets like MultiNLI?"
  },
  {
    "index": 802,
    "source_corpus_id": 221761540,
    "ref_id": "b26",
    "citation_corpus_id": 211146562,
    "start": 3602,
    "end": 3619,
    "title": "Published as a conference paper at ICLR 2020 DIVIDEMIX: LEARNING WITH NOISY LABELS AS SEMI-SUPERVISED LEARNING",
    "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at",
    "prev": "While increasing the size of data does improve the model's robustness to noise, our method can substantially boost the representation learning performance by addressing noise.",
    "curr": "There exists a large body of literature on learning with label noise (Jiang et al., 2018;Han et al., 2018;Guo et al., 2018;Tanaka et al., 2018;Arazo et al., 2019;Li et al., 2020a).",
    "next": "However, existing methods have several limitations that make them less effective for webly-supervised representation learning.",
    "query": "Are there any research papers on novel frameworks that combine learning with noisy labels and semi-supervised learning techniques for improving representation learning?"
  },
  {
    "index": 803,
    "source_corpus_id": 238408412,
    "ref_id": "b10",
    "citation_corpus_id": 52967399,
    "start": 2423,
    "end": 2444,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "7a.",
    "curr": "INTRODUCTION\n\nMultilingual language models (Devlin et al., 2019;Conneau & Lample, 2019;Conneau et al., 2020;Lewis et al., 2020a;Xue et al., 2021) have achieved impressive performance on a variety of multilingual natural language processing (NLP) tasks.",
    "next": "Training a model with multiple languages jointly can be understood as a multi-task learning (MTL) problem where each language serves as a distinct task to be learned (Wang et al., 2021).",
    "query": "Are there any research papers on pre-training techniques for multilingual natural language processing models using bidirectional transformers and multi-task learning approaches?"
  },
  {
    "index": 804,
    "source_corpus_id": 263909090,
    "ref_id": "b19",
    "citation_corpus_id": 253244266,
    "start": 11341,
    "end": 11358,
    "title": "A Systematic Investigation of Commonsense Knowledge in Large Language Models",
    "abstract": "Language models (LMs) trained on large amounts of data (e.g., Brown et al., 2020;Patwary et al., 2021)have shown impressive performance on many NLP tasks under the zeroshot and few-shot setup. Here we aim to better understand the extent to which such models learn commonsense knowledge -a critical component of many NLP applications. We conduct a systematic and rigorous zero-shot and few-shot commonsense evaluation of large pretrained LMs, where we: (i) carefully control for the LMs' ability to exploit potential surface cues and annotation artefacts, and (ii) account for variations in performance that arise from factors that are not related to commonsense knowledge. Our findings highlight the limitations of pre-trained LMs in acquiring commonsense knowledge without task-specific supervision; furthermore, using larger models or few-shot evaluation are insufficient to achieve human-level commonsense performance.",
    "prev": "Take the \"Take nap\" task as an example, the robot needs to understand that napping can be done on a bed, and the bed is typically located in a bedroom.",
    "curr": "Many works hold the belief that LLMs trained on large-scale data encode commonsense knowledge about the real-world (Davison et al., 2019;Li et al., 2022b;Bian et al., 2023).",
    "next": "Recently, several studies have investigated the integration of LLMs into task planning, which aims to address language ambiguities and provide robots with background knowledge Li et al., 2022a;Ahn et al., 2022).",
    "query": "Are there any studies that investigate the integration of large language models into robotic task planning, focusing on their ability to encode and apply commonsense knowledge for real-world understanding?"
  },
  {
    "index": 806,
    "source_corpus_id": 256358781,
    "ref_id": "b28",
    "citation_corpus_id": 236170938,
    "start": 2281,
    "end": 2300,
    "title": "EFFICIENT NEURAL CAUSAL DISCOVERY WITHOUT ACYCLICITY CONSTRAINTS",
    "abstract": "Learning the structure of a causal graphical model using both observational and interventional data is a fundamental problem in many scientific fields. A promising direction is continuous optimization for score-based methods, which, however, require constrained optimization to enforce acyclicity or lack convergence guarantees. In this paper, we present ENCO, an efficient structure learning method for directed, acyclic causal graphs leveraging observational and interventional data. ENCO formulates the graph search as an optimization of independent edge likelihoods, with the edge orientation being modeled as a separate parameter. Consequently, we provide for ENCO convergence guarantees when interventions on all variables are available, without having to constrain the score function with respect to acyclicity. In experiments, we show that ENCO can efficiently recover graphs with hundreds of nodes, an order of magnitude larger than what was previously possible, while handling deterministic variables and discovering latent confounders. * Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.In this work, we address both problems. By modeling the orientation of an edge as a separate parameter, we can define the score function without any acyclicity constraints or regularizers. This allows for unbiased low-variance gradient estimators that scale learning to much larger graphs. Yet, if we are able to intervene on all variables, the proposed optimization is guaranteed to converge to the correct, acyclic graph. Importantly, since such interventions might not always be available, we show that our algorithm performs robustly even when intervening on fewer variables and having small sample sizes. We call our method ENCO for Efficient Neural Causal Discovery.We make the following four contributions. Firstly, we propose ENCO, a causal structure learning method for observational and interventional data using continuous optimization. Different from recent methods, ENCO models the edge orientation as a separate parameter. Secondly, we derive unbiased, low-variance gradient estimators, which is crucial for scaling up the model to large numbers of variables. Thirdly, we show that ENCO is guaranteed to converge to the correct causal graph if interventions on all variables are available, despite not having any acyclicity constraints. Yet, we show in practice that the algorithm works on partial intervention sets as well. Fourthly, we extend ENCO to detecting latent confounders. In various experimental settings, ENCO recovers graphs accurately, making less than one error on graphs with 1,000 variables in less than nine hours of computation.",
    "prev": "lems in protein-signaling and transcriptional network discovery, that our approach lies on the Pareto frontier of two key metrics, the SID and SHD.",
    "curr": "INTRODUCTION\n\nIn many domains, including cell biology (Sachs et al., 2005), finance (Sanford & Moosa, 2012), and genetics (Zhang et al., 2013), the data generating process is thought to be represented by an underlying directed acylic graph (DAG).Many models rely on DAG assumptions, e.g., causal modeling uses DAGs to model distribution shifts, ensure predictor fairness among subpopulations, or learn agents more sample-efficiently (Kaddour et al., 2022).A key question, with implications ranging from better modeling to causal discovery, is how to recover this unknown DAG from observed data alone.While there are methods for identifying the underlying DAG if given additional interventional data (Eberhardt, 2007;Hauser & Bühlmann, 2014;Shanmugam et al., 2015;Kocaoglu et al., 2017;Brouillard et al., 2020;Addanki et al., 2020;Squires et al., 2020;Lippe et al., 2022), it is not always practical or ethical to obtain such data (e.g., if one aims to discover links between dietary choices and deadly diseases).",
    "next": "Learning DAGs from observational data alone is fundamentally difficult for two reasons.",
    "query": "Are there any research papers on methods for learning causal directed acyclic graphs from observational data without enforcing acyclicity constraints?"
  },
  {
    "index": 807,
    "source_corpus_id": 252531622,
    "ref_id": "b15",
    "citation_corpus_id": 220347682,
    "start": 16537,
    "end": 16566,
    "title": "In Search of Lost Domain Generalization",
    "abstract": "The goal of domain generalization algorithms is to predict well on distributions different from those seen during training. While a myriad of domain generalization algorithms exist, inconsistencies in experimental conditions-datasets, architectures, and model selection criteria-render fair and realistic comparisons difficult. In this paper, we are interested in understanding how useful domain generalization algorithms are in realistic settings. As a first step, we realize that model selection is non-trivial for domain generalization tasks. Contrary to prior work, we argue that domain generalization algorithms without a model selection strategy should be regarded as incomplete. Next, we implement DOMAINBED, a testbed for domain generalization including seven multi-domain datasets, nine baseline algorithms, and three model selection criteria. We conduct extensive experiments using DO-MAINBED and find that, when carefully implemented, empirical risk minimization shows state-of-the-art performance across all datasets. Looking forward, we hope that the release of DOMAINBED, along with contributions from fellow researchers, will streamline reproducible and rigorous research in domain generalization. * Alphabetical order, equal contribution.Preprint. Under review.",
    "prev": "As mentioned before, time series OOD algorithms are currently underexplored and there are only two recent strong approaches for comparison: GILE (Qian et al., 2021) and AdaRNN (Du et al., 2021).",
    "curr": "5 We further compare with 7 general OOD methods 6 from DomainBed (Gulrajani & Lopez-Paz, 2021): ERM, DANN (Ganin et al., 2016), CORAL (Sun & Saenko, 2016), Mixup (Zhang et al., 2018), GroupDRO (Sagawa et al., 2020), RSC (Huang et al., 2020), and ANDMask (Parascandolo et al., 2021).",
    "next": "More details of these methods are in Sec.",
    "query": "Are there any studies that compare domain generalization algorithms for time series data, particularly evaluating methods like GILE, AdaRNN, and general OOD approaches from DomainBed?"
  },
  {
    "index": 808,
    "source_corpus_id": 52135921,
    "ref_id": "b26",
    "citation_corpus_id": 11217889,
    "start": 3258,
    "end": 3283,
    "title": "Under review as a conference paper at ICLR 2018 CERTIFIED DEFENSES AGAINST ADVERSARIAL EX- AMPLES",
    "abstract": "While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no attack that perturbs each pixel by at most = 0.1 can cause more than 35% test error.",
    "prev": "It has been shown time and again ) that basically all defenses previously proposed did not increase model robustness but prevented existing attacks from finding minimal adversarial examples, the most common reason being masking of the gradients on which most attacks rely.",
    "curr": "The few verifiable defenses can only guarantee robustness within a small linear regime around the data points (Hein & Andriushchenko, 2017;Raghunathan et al., 2018).",
    "next": "The only defense currently considered effective  is a particular type of adversarial training .",
    "query": "Are there any research papers on certified defenses against adversarial examples using semidefinite relaxation or other methods that provide mathematical guarantees of robustness?"
  },
  {
    "index": 810,
    "source_corpus_id": 259252546,
    "ref_id": "b3",
    "citation_corpus_id": 225039882,
    "start": 8853,
    "end": 8856,
    "title": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.RELATED WORKTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.",
    "prev": ",32].Pioneering deep global motion deblurring works utilize CNNs as basic layers and achieve promising improvements in image quality.Among them, DeepDeblur [16], a multi-scale convolutional neural network, performs residual blocks to increase convergence speed.DeblurGAN [8] and DeblurGAN-v2 [9] introduce GANs and a perceptual loss to improve subjective quality.HINet [2] applies Instance Normalization to boost performance.Recently, a CNN-based local motion deblurring method, LBAG [10], bridges the gap between global and local motion deblurring by inserting gate modules at the end of MIMO-UNet architecture [3].It predicts differentiable blur masks to reduce sharp backgrounds from modifications and guide the network to deblur locally.Although the performance is significantly improved, CNN-based methods suffer from the content-independent interactions between images and convolution kernels, as well as the limitations of long-range dependency modeling.",
    "curr": "Given the Vision Transformer's (ViT) [4] ability to capture long-range dependencies, its application to global deblurring tasks has seen a surge of interest.For example, Uformer [24] employs windowbased self-attention with a learnable multi-scale restoration modulator to capture both local and global dependencies.Restormer [28] utilizes multi-head attention and a feed-forward network to achieve long-range pixel interactions.In this paper, we build a Transformer-based local motion deblurring framework, LMD-ViT, that adaptively selects windows relevant to blurry regions for window-based self-attention and feed-forward operations, simultaneously benefiting from long-range modeling.",
    "next": "Vision Transformer acceleration.Transformers have proven valuable in deblurring tasks, yet their direct application in local motion deblurring for high-resolution images presents challenges concerning computational efficiency.To solve the heavy computation problem of global self-attention in Transformers, researchers have presented several technique",
    "query": "Are there any research papers on Transformer-based approaches for local motion deblurring, particularly those addressing computational efficiency and utilizing window-based self-attention mechanisms?"
  },
  {
    "index": 811,
    "source_corpus_id": 108296442,
    "ref_id": "b8",
    "citation_corpus_id": 13276568,
    "start": 2297,
    "end": 2319,
    "title": "Learning language through pictures",
    "abstract": "We propose IMAGINET, a model of learning visually grounded representations of language from coupled textual and visual input. The model consists of two Gated Recurrent Unit networks with shared word embeddings, and uses a multi-task objective by receiving a textual description of a scene and trying to concurrently predict its visual representation and the next word in the sentence. Mimicking an important aspect of human language learning, it acquires meaning representations for individual words from descriptions of visual scenes. Moreover, it learns to effectively use sequential structure in semantic interpretation of multi-word phrases.",
    "prev": "It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "curr": "INTRODUCTION\n\nHumans are capable of learning visual concepts by jointly understanding vision and language (Fazly et al., 2010;Chrupała et al., 2015;Gauthier et al., 2018).",
    "next": "Consider the example shown in Figure 1-I.",
    "query": "Are there any research papers on models that integrate visual and textual data to learn language representations through joint understanding of vision and language?"
  },
  {
    "index": 812,
    "source_corpus_id": 54477714,
    "ref_id": "b13",
    "citation_corpus_id": 8495258,
    "start": 2224,
    "end": 2245,
    "title": "A Decomposable Attention Model for Natural Language Inference",
    "abstract": "We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.",
    "prev": "To generate a coherent piece, a model needs to reference elements that came before, sometimes in the distant past, repeating, varying, and further developing them to create contrast and surprise.",
    "curr": "Intuitively, self-attention (Parikh et al., 2016) appears to be a good match for this task.",
    "next": "Self-attention over its own previous outputs allows an autoregressive model to access any part of the previously generated output at every step of generation.",
    "query": "Are there any studies that explore decomposable attention models for natural language inference using self-attention mechanisms?"
  },
  {
    "index": 814,
    "source_corpus_id": 57189211,
    "ref_id": "b26",
    "citation_corpus_id": 3566136,
    "start": 26642,
    "end": 26670,
    "title": "Published as a conference paper at ICLR 2018 RELATIONAL NEURAL EXPECTATION MAXIMIZATION: UNSUPERVISED DISCOVERY OF OBJECTS AND THEIR INTERACTIONS",
    "abstract": "Common-sense physical reasoning is an essential ingredient for any intelligent agent operating in the real-world. For example, it can be used to simulate the environment, or to infer the state of parts of the world that are currently unobserved. In order to match real-world conditions this causal knowledge must be learned without access to supervised data. To address this problem we present a novel method that learns to discover objects and model their physical interactions from raw visual images in a purely unsupervised fashion. It incorporates prior knowledge about the compositional nature of human perception to factor interactions between object-pairs and learn efficiently. On videos of bouncing balls we show the superior modelling capabilities of our method compared to other unsupervised neural approaches that do not incorporate such prior knowledge. We demonstrate its ability to handle occlusion and show that it can extrapolate learned knowledge to scenes with different numbers of objects. * Work performed while at IDSIA.",
    "prev": "The insight that static observations are physically stable configurations of objects has been leveraged to improve 3D scene understanding algorithms.",
    "curr": "There has also been much attention on inferring object-factorized, or otherwise disentangled, representations of images (Eslami et al., 2016;Greff et al., 2017;van Steenkiste et al., 2018).",
    "next": "In contrast to works which aim to discover objects in a completely unsupervised manner, we focus on using object representations learned with minimal supervision, in the form of segmentation masks, for downstream tasks.",
    "query": "Are there any research papers on unsupervised methods for discovering object interactions and their physical properties in visual scenes using relational models and minimal supervision?"
  },
  {
    "index": 817,
    "source_corpus_id": 256274796,
    "ref_id": "b4",
    "citation_corpus_id": 1740355,
    "start": 3268,
    "end": 3287,
    "title": "Under review as a conference paper at ICLR 2017 DESIGNING NEURAL NETWORK ARCHITECTURES USING REINFORCEMENT LEARNING",
    "abstract": "At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We propose a meta-modeling approach based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using Q-learning with angreedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks. , et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015. A high-throughput screening approach to discovering good forms of biologically inspired visual representation.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on using reinforcement learning to automatically design neural network architectures for image classification tasks?"
  },
  {
    "index": 818,
    "source_corpus_id": 2703040,
    "ref_id": "b19",
    "citation_corpus_id": 3144218,
    "start": 4782,
    "end": 4803,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "This process is then repeated multiple times like in classical CNNs to build a deep graph representation.",
    "curr": "Other notable works on graph neural networks include (Li et al., 2015;Schütt et al., 2017;Battaglia et al., 2016;Kipf & Welling, 2017).",
    "next": "Very recently, (Gilmer et al., 2017) showed that many of these approaches can be seen to be specific instances of a general message passing formalism, and coined the term message passing neural networks (MPNNs) to refer to them collectively.",
    "query": "Are there any research papers on semi-supervised classification using graph convolutional networks or message passing neural networks?"
  },
  {
    "index": 819,
    "source_corpus_id": 250334789,
    "ref_id": "b32",
    "citation_corpus_id": 3526391,
    "start": 2934,
    "end": 2953,
    "title": "ENHANCING THE RELIABILITY OF OUT-OF-DISTRIBUTION IMAGE DETECTION IN NEURAL NETWORKS",
    "abstract": "We consider the problem of detecting out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in-and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach (Hendrycks & Gimpel, 2017) by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10 and Tiny-ImageNet) when the true positive rate is 95%.A seemingly straightforward approach of detecting out-of-distribution images is to enlarge the training set of both in-and out-of-distribution examples. However, the number of out-of-distribution examples can be infinitely many, making the re-training approach computationally expensive and intractable. Moreover, to ensure that a neural network accurately classifies in-distribution samples into correct classes while correctly detecting out-of-distribution samples, one might need to employ exceedingly large neural network architectures, which further complicates the training process.Hendrycks & Gimpel (2017) proposed a baseline method to detect out-of-distribution examples without further re-training networks. The method is based on an observation that a well-trained neural network tends to assign higher softmax scores to in-distribution examples than out-of-distribution examples. In this paper, we go further. We observe that after using temperature scaling in the softmax function(Hinton et al., 2015;Pereyra et al., 2017)  and adding small controlled perturbations to inputs, . Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. 2015.",
    "prev": "INTRODUCTION\n\nLearning in the presence of Out-Of-Distribution (OOD) data has been a challenging task in machine learning, as the deployed classifier tends to fail if the unseen data drawn from unknown distributions are not properly handled (Hendrycks & Gimpel, 2017;Pan & Yang, 2009).",
    "curr": "Such a critical problem ubiquitously exists when deep models meet domain shift (Ganin et al., 2016;Tzeng et al., 2017) and unseen-class data (Hendrycks & Gimpel, 2017;Scheirer et al., 2012), which has drawn a lot of attention in some important fields such as OOD detection (Hein et al., 2019;Hendrycks & Gimpel, 2017;Lee et al., 2018;Liang et al., 2018;2023a;, Open-Set Domain Adaptation (DA) Saito et al., 2018), and Open-Set Semi-Supervised Learning (SSL) (Huang et al., 2021b;a;Oliver et al., 2018;Saito et al., 2021;.",
    "next": "In the above fields, OOD data can be divided into two types, namely benign OOD data 1 and malign OOD data.",
    "query": "Are there any research papers on methods for improving the reliability of out-of-distribution image detection in neural networks using temperature scaling and input perturbations?"
  },
  {
    "index": 820,
    "source_corpus_id": 246634143,
    "ref_id": "b38",
    "citation_corpus_id": 14124313,
    "start": 4373,
    "end": 4401,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "• Experimental results show that our efficient models outperform the previous efficient models and yield faster model speeds with further robustness ( §5).",
    "curr": "INTRODUCTION\n\nImage classification has been advanced with deep convolutional neural networks (Simonyan & Zisserman, 2015;Huang et al., 2017;He et al., 2016b) with the common design paradigm of the network building blocks with trainable spatial convolutions inside.",
    "next": "Such trainable layers with learnable parameters effectively grasp attentive signals to distinguish input but are computationally heavy.",
    "query": "Are there any research papers on deep convolutional networks for image recognition that focus on increasing network depth and improving computational efficiency?"
  },
  {
    "index": 821,
    "source_corpus_id": 252070677,
    "ref_id": "b37",
    "citation_corpus_id": 2131938,
    "start": 2278,
    "end": 2282,
    "title": "Information Extraction over Structured Data: Question Answering with Freebase",
    "abstract": "Answering natural language questions using the Freebase knowledge base has recently been explored as a platform for advancing the state of the art in open domain semantic parsing. Those efforts map questions to sophisticated meaning representations that are then attempted to be matched against viable answer candidates in the knowledge base. Here we show that relatively modest information extraction techniques, when paired with a webscale corpus, can outperform these sophisticated approaches by roughly 34% relative gain.",
    "prev": "INTRODUCTION\n\nKnowledge graphs (KGs) represent a large amount of entities and their relationships via a collection of factual triplets, (ℎ, , ), where each triplet expresses the relationship between a head entity ℎ and a tail entity .",
    "curr": "Large-scale KGs [1,19,26,32] can provide powerful knowledge inference capabilities for many intelligent applications, including question answering [38], web search [8] and recommendation systems [33].",
    "next": "As KGs are often built semiautomatically from unstructured data, it has become a well-known fact that most existing KGs are far from complete [6].",
    "query": "Are there any research papers on information extraction techniques for improving question answering over knowledge graphs, particularly addressing challenges related to KG incompleteness?"
  },
  {
    "index": 822,
    "source_corpus_id": 54472058,
    "ref_id": "b1",
    "citation_corpus_id": 11212020,
    "start": 1821,
    "end": 1843,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "INTRODUCTION\n\nDue to the power law distribution of word frequencies, rare words are extremely common in any language (Zipf, 1935).",
    "curr": "Yet, the majority of language generation tasks-including machine translation (Sutskever et al., 2014;Bahdanau et al., 2014;Luong et al., 2015), summarization (Rush et al., 2015;See et al., 2017;Paulus et al., 2018), dialogue generation (Vinyals & Le, 2015), question answering (Yin et al., 2015), speech recognition (Graves et al., 2013;Xiong et al., 2017), and others-generate words by sampling from a multinomial distribution over a closed output vocabulary.",
    "next": "This is done by computing scores for each candidate word and normalizing them to probabilities using a softmax layer.",
    "query": "Are there any research papers on neural machine translation methods that use encoder-decoder architectures with soft alignment techniques?"
  },
  {
    "index": 823,
    "source_corpus_id": 260497139,
    "ref_id": "b0",
    "citation_corpus_id": 18828233,
    "start": 5003,
    "end": 5005,
    "title": "TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.",
    "prev": "Negative Logarithm Alternative It is worth mentioning that in the above formulation we adopt − log(D r (x)) as an alternative of log(1−D r (x)) for the generator's loss [12].",
    "curr": "The rationale is that, when the discriminator perfectly distinguishes fake samples, the log(1−D r (x)) loss function of the generator may suffer from the gradient vanishing problem [12,1].",
    "next": "The motivation of using the − log(D r (x)) is that: though giving a difference gradient scale, it always preserves the same gradient direction as log(1−D r (x)).",
    "query": "Are there any research papers on theoretical approaches to improving training stability in generative adversarial networks by modifying loss functions to address gradient vanishing?"
  },
  {
    "index": 826,
    "source_corpus_id": 56895416,
    "ref_id": "b13",
    "citation_corpus_id": 49411844,
    "start": 3015,
    "end": 3033,
    "title": "DARTS: Differentiable Architecture Search",
    "abstract": "This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques.",
    "prev": "However, as NAS is modeled as a Markov Decision Process, credits are assigned to structural decisions with temporal-difference (TD) learning (Sutton et al., 1998), whose efficiency and interpretability suffer from delayed rewards (Arjona-Medina et al., 2018).",
    "curr": "To get rid of the architecture sampling process, DARTS (Liu et al., 2019) proposes deterministic attention on operations to analytically calculate expectation at each layer and remove operations with relatively weak attention after the convergence of the parent network.",
    "next": "Due to the pervasive non-linearity in neural operations, the performance of the child network is always inconsistent and parameter retraining comes up as necessary.",
    "query": "Are there any research papers on differentiable methods for neural architecture search that utilize continuous relaxation and gradient-based optimization to enhance scalability and efficiency compared to non-differentiable approaches?"
  },
  {
    "index": 827,
    "source_corpus_id": 235390444,
    "ref_id": "b13",
    "citation_corpus_id": 4986726,
    "start": 2046,
    "end": 2071,
    "title": "CENSORING REPRESENTATIONS WITH AN ADVERSARY",
    "abstract": "In practice, there are often explicit constraints on what representations or decisions are acceptable in an application of machine learning. For example it may be a legal requirement that a decision must not favour a particular group. Alternatively it can be that that representation of data must not have identifying information. We address these two related issues by learning flexible representations that minimize the capability of an adversarial critic. This adversary is trying to predict the relevant sensitive variable from the representation, and so minimizing the performance of the adversary ensures there is little or no information in the representation about the sensitive variable. We demonstrate this adversarial approach on two problems: making decisions free from discrimination and removing private information from images. We formulate the adversarial model as a minimax problem, and optimize that minimax objective using a stochastic gradient alternate min-max optimizer. We demonstrate the ability to provide discriminant free representations for standard test problems, and compare with previous state of the art methods for fairness, showing statistically significant improvement across most cases. The flexibility of this method is shown via a novel problem: removing annotations from images, from separate training examples of annotated and unannotated images, and with no a priori knowledge of the form of annotation provided to the model.",
    "prev": "Concretely, the goal is to ensure that representations have two properties: (i) they are informative for various prediction tasks of interest, (ii) sensitive attributes of the original data (e.g., race) cannot be recovered from the representations.",
    "curr": "Perhaps the most prominent approach for learning fair representations is adversarial training (Edwards & Storkey, 2016;Madras et al., 2018;Xie et al., 2017;Song et al., 2019;Roy & Boddeti, 2019), which jointly trains an encoder trying to transform data into a fair representation with an adversary attempting to recover sensitive attributes from the representation.",
    "next": "However, several recent lines of work (Feng et al., 2019;Moyer et al., 2018;Elazar & Goldberg, 2018;Xu et al., 2020;Gupta et al., 2021;Song & Shmatikov, 2020) have noticed that these approaches do not produce truly fair representations: stronger adversaries can in fact recover sensitive attributes.",
    "query": "Are there any research papers on adversarial training methods for learning fair machine learning representations that prevent the recovery of sensitive attributes?"
  },
  {
    "index": 828,
    "source_corpus_id": 222291521,
    "ref_id": "b18",
    "citation_corpus_id": 3144218,
    "start": 3067,
    "end": 3071,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "For example, in drug discovery, when viewing molecules as graphs with atoms as nodes and chemical bonds as edges, biochemists are interested in identifying the subgraphs that mostly represent certain properties of the molecules, namely the functional groups [17,11].",
    "curr": "In graph representation learning, the predictive subgraph highlights the vital substructure for graph classification, and provides an alternative way for yielding graph representation besides mean/sum aggregation [19,30,32] and pooling aggregation [35,21,4].",
    "next": "In graph attack and defense, it is vital to purify a perturbed graph and mine the robust structures for classification [16].",
    "query": "Are there any research papers on semi-supervised classification using graph convolutional networks and techniques for identifying predictive subgraphs in molecular and graph-structured data?"
  },
  {
    "index": 829,
    "source_corpus_id": 13352766,
    "ref_id": "b2",
    "citation_corpus_id": 1740355,
    "start": 8386,
    "end": 8429,
    "title": "Under review as a conference paper at ICLR 2017 DESIGNING NEURAL NETWORK ARCHITECTURES USING REINFORCEMENT LEARNING",
    "abstract": "At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We propose a meta-modeling approach based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using Q-learning with angreedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks. , et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015. A high-throughput screening approach to discovering good forms of biologically inspired visual representation.",
    "prev": "This allows our approach to find the opti-mal architecture for the given dataset and constraints instead of being limited to that of the original model.",
    "curr": "Knowledge Distillation: Knowledge distillation is the task of training a smaller network (a \"student\") to mimic a \"teacher\" network, performing comparably to the input network (a \"teacher\") (Bucilu, Caruana, and Niculescu-Mizil 2006), (Ba and Caruana 2014), (Hinton, Vinyals, and Dean 2015), (Romero et al.",
    "next": "2014), (Urban et al.",
    "query": "Are there any research papers on using reinforcement learning for automated neural network architecture design in image classification tasks?"
  },
  {
    "index": 830,
    "source_corpus_id": 222208633,
    "ref_id": "b20",
    "citation_corpus_id": 3608234,
    "start": 5721,
    "end": 5740,
    "title": "Published as a conference paper at ICLR 2018 GENERATING WIKIPEDIA BY SUMMARIZING LONG SEQUENCES",
    "abstract": "We show that generating English Wikipedia articles can be approached as a multidocument summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoderdecoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations. * Joint first-authors. Ordered randomly. † Work done as a member of the Google Brain Residency (g.co/brainresidency)",
    "prev": "The most straightforward paradigm is restricting the attention pattern to be fixed local windows.",
    "curr": "Most works (Liu et al., 2018a;Parmar et al., 2018;Child et al., 2019;Huang et al., 2019;Ho et al., 2019;Hu et al., 2019;Parmar et al., 2019;Qiu et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020) follow this paradigm.",
    "next": "Although restricting the attention pattern to a local neighborhood can decrease the complexity, it loses global information.",
    "query": "Are there any research papers on attention mechanisms for long sequence abstractive summarization that address the trade-off between local window restrictions and global information retention?"
  },
  {
    "index": 832,
    "source_corpus_id": 231847109,
    "ref_id": "b9",
    "citation_corpus_id": 220249831,
    "start": 4943,
    "end": 4962,
    "title": "Heteroskedastic and Imbalanced Deep Learning with Adaptive Regularization",
    "abstract": "Real-world large-scale datasets are heteroskedastic and imbalanced -labels have varying levels of uncertainty and label distributions are long-tailed. Heteroskedasticity and imbalance challenge deep learning algorithms due to the difficulty of distinguishing among mislabeled, ambiguous, and rare examples. Addressing heteroskedasticity and imbalance simultaneously is under-explored. We propose a data-dependent regularization technique for heteroskedastic datasets that regularizes different regions of the input space differently. Inspired by the theoretical derivation of the optimal regularization strength in a one-dimensional nonparametric classification setting, our approach adaptively regularizes the data points in higher-uncertainty, lower-density regions more heavily. We test our method on several benchmark tasks, including a real-world heteroskedastic and imbalanced dataset, WebVision. Our experiments corroborate our theory and demonstrate a significant improvement over other methods in noise-robust deep learning.",
    "prev": "Robust SSL (Oliver et al., 2018;Guo Labeled set Unlabeled set \"elephant\" \"cheetah\" \"octopus\" class 1 class 2 class 3 seen classes novel classes + seen classes seen class seen class seen class novel class novel class novel class\n\nOpen-world SSL Figure 1: In the open-world SSL, the unlabeled dataset may contain classes that have never been encountered in the labeled set.",
    "curr": "Given unlabeled test set, the model needs to either assign instances to one of the classes previously seen in the labeled set, or form a novel class and assign instances to Chen et al., 2020b;Guo et al., 2020;Yu et al., 2020) assumes class distribution mismatch between labeled and unlabeled data, but in this setting the model only needs to be able to recognize (reject) instances belonging to novel classes in the unlabeled data as out-of-distribution instances.",
    "next": "In contrast, instead of rejecting instances belonging to novel classes, open-world SSL aims at discovering individual novel classes and then assigning instances to them.",
    "query": "Are there any studies that explore methods for open-world semi-supervised learning with adaptive regularization techniques to address heteroskedastic and imbalanced data distributions?"
  },
  {
    "index": 834,
    "source_corpus_id": 68137503,
    "ref_id": "b31",
    "citation_corpus_id": 3708505,
    "start": 2925,
    "end": 2942,
    "title": "DEEP NEURAL NETWORKS AS GAUSSIAN PROCESSES",
    "abstract": "A deep fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP) in the limit of infinite network width. This correspondence enables exact Bayesian inference for neural networks on regression tasks by means of straightforward matrix computations. For single hiddenlayer networks, the covariance function of this GP has long been known. Recently, kernel functions for multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified the correspondence between using these kernels as the covariance function for a GP and performing fully Bayesian prediction with a deep neural network. In this work, we derive this correspondence and develop a computationally efficient pipeline to compute the covariance functions. We then use the resulting GP to perform Bayesian inference for deep neural networks on MNIST and CIFAR-10. We find that the GP-based predictions are competitive and can outperform neural networks trained with stochastic gradient descent. We observe that the trained neural network accuracy approaches that of the corresponding GP-based computation with increasing layer width, and that the GP uncertainty is strongly correlated with prediction error. We connect our observations to the recent development of signal propagation in random neural networks. * Both authors contributed equally to this work. † Work done as a member of the Google Brain Residency program (g.co/brainresidency). 1 Throughout this paper, we assume the conditions on the parameter distributions and nonlinearities are such that the Central Limit Theorem will hold; for instance, that the weight variance is scaled inversely proportional to the layer width.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on the equivalence between deep neural networks and Gaussian processes, focusing on Bayesian inference and computational methods for regression tasks?"
  },
  {
    "index": 835,
    "source_corpus_id": 222140859,
    "ref_id": "b9",
    "citation_corpus_id": 52967399,
    "start": 2031,
    "end": 2052,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "INTRODUCTION\n\nSelf-supervised representation learning pre-trains good feature extractors from massive unlabeled data, which show promising transferability to various downstream tasks.",
    "curr": "Recent success includes large-scale pre-trained language models (e.g., BERT, RoBERTa, and GPT-3 (Devlin et al., 2019;Liu et al., 2019;Brown et al., 2020)), which have advanced state of the art over a wide range of NLP tasks such as NLI and QA, even surpassing human performance.",
    "next": "Specifically, in the computer vision domain, many studies have shown that self-supervised representation learning is essentially solving the problem of maximizing the mutual information (MI) I(X; T ) between the input X and the representation T (van den Oord et al., 2018;Belghazi et al., 2018;Hjelm et al., 2019;.",
    "query": "Are there any research papers on self-supervised representation learning methods for natural language processing tasks using pre-trained language models like BERT?"
  },
  {
    "index": 837,
    "source_corpus_id": 235669672,
    "ref_id": "b23",
    "citation_corpus_id": 1487550,
    "start": 19543,
    "end": 19559,
    "title": "UNSUPERVISED WORD SENSE DISAMBIGUATION RIVALING SUPERVISED METHODS",
    "abstract": "This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints -that words tend to have one sense per discourse and one sense per collocation -exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%.",
    "prev": "We use 5 iterations, t 1 = 0.8, and t 2 = 1.2.",
    "curr": "• Self-training (Yarowsky, 1995;McClosky et al., 2006).",
    "next": "A classical semi-supervised method -each iteration, we train on pseudo-labeled data (initialized to be the original labeled dataset) and add highly confident predictions to the training set using the prediction as the label.",
    "query": "Are there any research papers on unsupervised word sense disambiguation methods using iterative bootstrapping or self-training techniques?"
  },
  {
    "index": 838,
    "source_corpus_id": 256697539,
    "ref_id": "b17",
    "citation_corpus_id": 238354201,
    "start": 24567,
    "end": 24592,
    "title": "MOBILEVIT: LIGHT-WEIGHT, GENERAL-PURPOSE, AND MOBILE-FRIENDLY VISION TRANSFORMER",
    "abstract": "Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision transformers (ViTs) have been adopted. Unlike CNNs, ViTs are heavyweight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. Mobile-ViT presents a different perspective for the global processing of information with transformers. Our results show that MobileViT significantly outperforms CNNand ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: https://github.com/apple/ml-cvnets.",
    "prev": "We tested the inference time and peak memory consumption on CPU (Intel(R) Xeon(R) Gold 6246R CPU with 3.40 GHz clock), GPU (Nvidia A100), and on CPU with XNNPACK optimization.",
    "curr": "For comparison, we also tested LSTM and MobileViT (Mehta & Rastegari, 2022) networks.",
    "next": "MODELS AND TRAINING PROCESS\n\nGhostNet Our classification network consist of 3 ghost bottlenecks and ends with a convolutional layer and average pooling.",
    "query": "Are there any studies that explore lightweight vision transformers combining CNN and ViT architectures for mobile-friendly applications with efficient inference and parameter optimization?"
  },
  {
    "index": 841,
    "source_corpus_id": 56657805,
    "ref_id": "b7",
    "citation_corpus_id": 3530344,
    "start": 3867,
    "end": 3885,
    "title": "Published as a conference paper at ICLR 2018 REINFORCEMENT LEARNING ON WEB INTERFACES USING WORKFLOW-GUIDED EXPLORATION",
    "abstract": "Reinforcement learning (RL) agents improve through trial-and-error, but when reward is sparse and the agent cannot discover successful action sequences, learning stagnates. This has been a notable problem in training deep RL agents to perform web-based tasks, such as booking flights or replying to emails, where a single mistake can ruin the entire sequence of actions. A common remedy is to \"warmstart\" the agent by pre-training it to mimic expert demonstrations, but this is prone to overfitting. Instead, we propose to constrain exploration using demonstrations. From each demonstration, we induce high-level \"workflows\" which constrain the allowable actions at each time step to be similar to those in the demonstration (e.g., \"Step 1: click on a textbox; Step 2: enter some text\"). Our exploration policy then learns to identify successful workflows and samples actions that satisfy these workflows. Workflows prune out bad exploration directions and accelerate the agent's ability to discover rewards. We use our approach to train a novel neural policy designed to handle the semi-structured nature of websites, and evaluate on a suite of web tasks, including the recent World of Bits benchmark. We achieve new state-of-the-art results, and show that workflow-guided exploration improves sample efficiency over behavioral cloning by more than 100x. * First three authors contributed equally",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on reinforcement learning methods for web-based tasks that use workflow-guided exploration to improve sample efficiency and avoid overfitting compared to behavioral cloning?"
  },
  {
    "index": 847,
    "source_corpus_id": 5037032,
    "ref_id": "b32",
    "citation_corpus_id": 14724343,
    "start": 202899,
    "end": 202920,
    "title": "THIRD-PERSON IMITATION LEARNING",
    "abstract": "Reinforcement learning (RL) makes it possible to train agents capable of achieving sophisticated goals in complex and uncertain environments. A key difficulty in reinforcement learning is specifying a reward function for the agent to optimize. Traditionally, imitation learning in RL has been used to overcome this problem. Unfortunately, hitherto imitation learning methods tend to require that demonstrations are supplied in the first-person: the agent is provided with a sequence of states and a specification of the actions that it should have taken. While powerful, this kind of imitation learning is limited by the relatively hard problem of collecting first-person demonstrations. Humans address this problem by learning from third-person demonstrations: they observe other humans perform tasks, infer the task, and accomplish the same task themselves. In this paper, we present a method for unsupervised third-person imitation learning. Here third-person refers to training an agent to correctly achieve a simple goal in a simple environment when it is provided a demonstration of a teacher achieving the same goal but from a different viewpoint; and unsupervised refers to the fact that the agent receives only these third-person demonstrations, and is not provided a correspondence between teacher states and student states. Our methods primary insight is that recent advances from domain confusion can be utilized to yield domain agnostic features which are crucial during the training process. To validate our approach, we report successful experiments on learning from third-person demonstrations in a pointmass domain, a reacher domain, and inverted pendulum.",
    "prev": "(2017) observe a sequence of images from the expert demonstration for performing rope manipulations.Sermanet et al.",
    "curr": "(2017;2018) imitate humans with robots by self-supervised learning but require expert supervision at training time.Third person imitation learning (Stadie et al., 2017) and the concurrent work of imitation-from-observation (Liu et al., 2018) learn to translate expert observations into agent observations such that they can do policy optimization to minimize the distance between the agent trajectory and the translated demonstration, but they require demonstrations for learning.Visual servoing is a standard problem in robotics (Koichi & Tom, 1993) that seeks to take actions that align the agent's observation with a target configuration of carefully-designed visual features (Wilson et al., 1996;Yoshimi & Allen, 1994) or raw pixel intensities (Caron et al., 2013).Classical methods rely on fixed features or policies, but more recently end-to-end learning has improved results (Lampe & Riedmiller, 2013;Lee et al., 2017).et al.",
    "next": "(1995) jointly learn forward and inverse dynamics model but do not optimize for consistency between the forward and inverse dynamics.We empirically show that learning models by our forward consistency loss significantly improves task p",
    "query": "Are there any studies on unsupervised third-person imitation learning in robotics that use domain confusion techniques to align agent observations with expert demonstrations?"
  },
  {
    "index": 849,
    "source_corpus_id": 257365203,
    "ref_id": "b52",
    "citation_corpus_id": 237291550,
    "start": 3654,
    "end": 3673,
    "title": "SIMVLM: SIMPLE VISUAL LANGUAGE MODEL PRE- TRAINING WITH WEAK SUPERVISION",
    "abstract": "With recent progress in joint modeling of visual and textual representations, Vision-Language Pretraining (VLP) has achieved impressive performance on many multimodal downstream tasks. However, the requirement for expensive annotations including clean image captions and regional labels limits the scalability of existing approaches, and complicates the pretraining procedure with the introduction of multiple dataset-specific objectives. In this work, we relax these constraints and present a minimalist pretraining framework, named Simple Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training complexity by exploiting large-scale weak supervision, and is trained end-to-end with a single prefix language modeling objective. Without utilizing extra data or task-specific customization, the resulting model significantly outperforms previous pretraining methods and achieves new state-of-the-art results on a wide range of discriminative and generative vision-language benchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE (+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score). Furthermore, we demonstrate that SimVLM acquires strong generalization and transfer ability, enabling zero-shot behavior including open-ended visual question answering and cross-modality transfer.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on simple visual language model pre-training using weak supervision and achieving state-of-the-art results on vision-language benchmarks?"
  },
  {
    "index": 850,
    "source_corpus_id": 232233563,
    "ref_id": "b17",
    "citation_corpus_id": 28202810,
    "start": 1892,
    "end": 1914,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "Although RL has achieved impressive achievements on various tasks (Silver et al., 2017;Mnih et al., 2015;Berner et al., 2019), it is very expensive to provide dense rewards for every task we want the robot to learn.",
    "curr": "Intrinsically motivated reinforcement learning encourages the agent to explore by providing an \"internal motivation\" instead, such as curiosity (Schmidhuber, 1991;Pathak et al., 2017;Burda et al., 2018), diversity (Gregor et al., 2016;Haarnoja et al., 2018;Eysenbach et al., 2019) and empowerment (Klyubin et al., 2005;Salge et al., 2014;Mohamed & Rezende, 2015).",
    "next": "Those internal motivations can be computed on the fly when the agent is interacting with the environment, without any human engineered reward.",
    "query": "Are there any research papers on intrinsic motivation approaches in reinforcement learning that utilize curiosity, diversity, or empowerment as internal reward mechanisms?"
  },
  {
    "index": 851,
    "source_corpus_id": 263829780,
    "ref_id": "b17",
    "citation_corpus_id": 247628243,
    "start": 2571,
    "end": 2592,
    "title": "EVALUATING DISTRIBUTIONAL DISTORTION IN NEURAL LANGUAGE MODELING",
    "abstract": "A fundamental characteristic of natural language is the high rate at which speakers produce novel expressions. Because of this novelty, a heavy-tail of rare events accounts for a significant amount of the total probability mass of distributions in language(Baayen, 2001). Standard language modeling metrics such as perplexity quantify the performance of language models (LM) in aggregate. As a result, we have relatively little understanding of whether neural LMs accurately estimate the probability of sequences in this heavy-tail of rare events. To address this gap, we develop a controlled evaluation scheme which uses generative models trained on natural data as artificial languages from which we can exactly compute sequence probabilities. Training LMs on generations from these artificial languages, we compare the sequence-level probability estimates given by LMs to the true probabilities in the target language. Our experiments reveal that LSTM and Transformer language models (i) systematically underestimate the probability of sequences drawn from the target language, and (ii) do so more severely for lessprobable sequences. Investigating where this probability mass went, (iii) we find that LMs tend to overestimate the probability of ill-formed (perturbed) sequences. In addition, we find that this underestimation behaviour (iv) is weakened, but not eliminated by greater amounts of training data, and (v) is exacerbated for target distributions with lower entropy.",
    "prev": "INTRODUCTION\n\nThe dominant paradigm of natural language generation systems hinges on probabilistic neural language models (Radford et al., 2019;Zhang et al., 2022), which permit evaluating the probability of any given text sequence as well as generating novel ones using various decoding strategies upon learned distributions (Holtzman et al., 2019;Meister et al., 2023b).Language modeling, the process of aligning model distribution with that of human language, is usually formulated as a sequence prediction task in which maximum likelihood estimation (MLE) is typically adopted as the training objective owing to its simplicity and intuitiveness.",
    "curr": "However, various text degeneration phenomena with incoherent and nonsensical (LeBrun et al., 2021;Holtzman et al., 2019) content are still widely observed in text generated from language models pre-trained on massive amounts of human data.This indicates that the model distribution Q θ (parametrized by θ) learned by MLE still differs substantially from the human language distribution P , despite having a seemingly low training loss (Meister et al., 2023b).From a distributional view, training with MLE is equivalent to minimizing the forward cross-entropy between P and Q θ :\nCE(P, Q θ ) = −E x∼P [log Q θ (x)]\n(1)\n\nWe argue that the forward cross-entropy has inherent limitations as a metric for matching model distribution and that of human language.Firstly, forward cross-entropy is recall-prioritized (Meister et al., 2023a).At each time step, it focuses exclusively on increasing the model likelihood of the 1 arXiv:2310.04691v3[cs.CL] 2 Nov 2023\n\nPreprint ground-truth next token.This can result in poor precision of the learned model distribution when training data is noisy or slow conve",
    "next": null,
    "query": "Are there any studies that investigate distributional distortion in neural language models and evaluate methods for assessing their ability to estimate probabilities of rare or ill-formed sequences?"
  },
  {
    "index": 853,
    "source_corpus_id": 211082893,
    "ref_id": "b29",
    "citation_corpus_id": 205514,
    "start": 3422,
    "end": 3444,
    "title": "DEEP MULTI-SCALE VIDEO PREDICTION BEYOND MEAN SQUARE ERROR",
    "abstract": "Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical flow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still, many vision applications could benefit from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectory. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset.",
    "prev": "Videos of real-world interactions are plentiful and readily available, and a large generative model can be trained on large unlabeled datasets containing many video sequences, thereby learning about a wide range of real-world phenoma.",
    "curr": "Such a model could be useful for learning representations for further downstream tasks (Mathieu et al., 2016), or could even be used directly in applications where predicting the future enables effective decision making and control, such as robotics (Finn et al., 2016).",
    "next": "A central challenge in video prediction is that the future is highly uncertain: a short sequence of observations of the present can imply many possible futures.",
    "query": "Are there any research papers on deep video prediction methods beyond mean square error, using multi-scale architectures, adversarial training, and image gradient difference loss for applications in robotics and future frame prediction?"
  },
  {
    "index": 855,
    "source_corpus_id": 51952942,
    "ref_id": "b1",
    "citation_corpus_id": 65455367,
    "start": 8889,
    "end": 8892,
    "title": "ON THE CONVERGENCE OF ADAM AND BEYOND",
    "abstract": "Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous analysis of ADAM algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with \"long-term memory\" of past gradients, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.",
    "prev": "The most well-known first-order algorithms are gradient descent (GD) for deterministic optimization [6,7] and stochastic gradient descent (SGD) for stochastic optimization [8,9], where the former determines ∆ t using the full (batch) gradient of an objective function, and the latter uses a simpler but more computationally-efficient stochastic (unbiased) gradient estimate.",
    "curr": "• Different from recently developed theoretical analysis of AMSGrad [2], which has been focused on diminishing momentum controlling parameter, our convergence analysis is applicable to the more popular constant momentum parameter setting.",
    "next": "• Our work provides theoretical support for a generic class of adaptive momentum based methods, including existing algorithms such as Adam, AMSGrad, AdaGrad, as well as their new variants such as AdaGrad with momentum.",
    "query": "Are there any research papers on the convergence analysis of adaptive momentum-based optimization algorithms such as Adam, AMSGrad, and AdaGrad, and their variants?"
  },
  {
    "index": 856,
    "source_corpus_id": 244709059,
    "ref_id": "b28",
    "citation_corpus_id": 225094135,
    "start": 49901,
    "end": 49923,
    "title": "Published as a conference paper at ICLR 2021 LEARNING TO REPRESENT ACTION VALUES AS A HYPERGRAPH ON THE ACTION VERTICES",
    "abstract": "Action-value estimation is a critical component of many reinforcement learning (RL) methods whereby sample complexity relies heavily on how fast a good estimator for action value can be learned. By viewing this problem through the lens of representation learning, good representations of both state and action can facilitate action-value estimation. While advances in deep learning have seamlessly driven progress in learning state representations, given the specificity of the notion of agency to RL, little attention has been paid to learning action representations. We conjecture that leveraging the combinatorial structure of multi-dimensional action spaces is a key ingredient for learning good representations of action. To test this, we set forth the action hypergraph networks framework-a class of functions for learning action representations in multi-dimensional discrete action spaces with a structural inductive bias. Using this framework we realise an agent class based on a combination with deep Q-networks, which we dub hypergraph Q-networks. We show the effectiveness of our approach on a myriad of domains: illustrative prediction problems under minimal confounding effects, Atari 2600 games, and discretised physical control benchmarks.Published as a conference paper at ICLR 2021Our results advocate for the general usefulness of leveraging the combinatorial structure of multidimensional discrete action spaces, especially in problems with larger action spaces.",
    "prev": "This paradigm transforms the value decomposition to the structured prediction problem.",
    "curr": "A future work is integrating prior knowledge of the decomposition structure as many previous works for structured prediction (Chen et al., 2020;Tavakoli et al., 2021).",
    "next": "Figure 1 :\n1Learning curves on a suite of MuJoCo benchmark tasks with episodic rewards.",
    "query": "Are there any research papers on leveraging hypergraph structures or combinatorial action space representations for action-value estimation in reinforcement learning with structured prediction methods?"
  },
  {
    "index": 857,
    "source_corpus_id": 251253049,
    "ref_id": "b58",
    "citation_corpus_id": 222140788,
    "start": 8761,
    "end": 8780,
    "title": "Published as a conference paper at ICLR 2021 DENOISING DIFFUSION IMPLICIT MODELS",
    "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples 10× to 50× faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error.",
    "prev": "(2021) improve inversion by conditioning the denoising process on noised low-pass filter data from the target image.",
    "curr": "demonstrate that the DDIM (Song et al., 2020) sampling process can be inverted in a closed-form manner, extracting a latent noise map that will produce a given real image.",
    "next": "In DALL-E 2 (Ramesh et al., 2022), they build on this method and demonstrate that it can be used to induce changes in the image, such as cross-image interpolations or semantic editing.",
    "query": "Are there any research papers on methods for inverting denoising diffusion implicit models (DDIMs) to enable semantic image editing and cross-image interpolation through latent space manipulations?"
  },
  {
    "index": 858,
    "source_corpus_id": 52947979,
    "ref_id": "b12",
    "citation_corpus_id": 17306137,
    "start": 2373,
    "end": 2396,
    "title": "PIXELVAE: A LATENT VARIABLE MODEL FOR NATURAL IMAGES",
    "abstract": "Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64 × 64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.",
    "prev": "Implementation of DReG estimators and code to reproduce experiments: sites.google.com/view/ dregs.",
    "curr": "INTRODUCTION\n\nFollowing the influential work by (Kingma & Welling, 2013;Rezende et al., 2014), deep generative models with latent variables have been widely used to model data such as natural images (Rezende & Mohamed, 2015;Gulrajani et al., 2016), speech and music time-series (Chung et al., 2015;Fraccaro et al., 2016;Krishnan et al., 2015), and video (Babaeizadeh et al., 2017;Ha & Schmidhuber, 2018;Denton & Fergus, 2018).",
    "next": "The power of these models lies in combining learned nonlinear function approximators with a principled probabilistic approach, resulting in expressive models that can capture complex distributions.",
    "query": "Are there any research papers on combining variational autoencoders with autoregressive decoders to improve natural image modeling?"
  },
  {
    "index": 859,
    "source_corpus_id": 262824542,
    "ref_id": "b30",
    "citation_corpus_id": 3608234,
    "start": 6444,
    "end": 6448,
    "title": "Published as a conference paper at ICLR 2018 GENERATING WIKIPEDIA BY SUMMARIZING LONG SEQUENCES",
    "abstract": "We show that generating English Wikipedia articles can be approached as a multidocument summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoderdecoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations. * Joint first-authors. Ordered randomly. † Work done as a member of the Google Brain Residency (g.co/brainresidency)",
    "prev": "Experimental methodology\n\nThis section details our experimental set-up (Section 2.1) and useful tools employed by our analysis: (i) measuring the relationship between learning rate and loss across scales (Section 2.2) and (ii) examining scaling trends for model characteristics (Section 2.3).",
    "curr": "Experimental set-up\n\nWe train small Transformer models [45] with a similar experimental set-up as GPT-2 [38] implemented in Flax [20]: the models are decoder-only [31] and trained with an auto-regressive loss (refer to Section A for more infrastructure details).While we experimentally manipulate many of the following hyperparameters, this section provides their default values, which we use unless otherwise specified.",
    "next": "By default, we use AdamW [33] with β 1 = 0.9, β 2 = 0.95, ϵ = 1e-8, and gradient clipping at global norm 1.The default warmup is 5e3 steps, and the default number of total steps is 1e5.We use a linear schedule for warmup and and a cosine-decay [32] schedule for the remainder, with minimum learning rate 1e-5.We use an independent weight decay of 1e-4 and auxiliary z-loss [6] with coefficient 1e-4.Sections 3.2.2 and 3.1.2respectively provide additional information and ablations on decoupled weight decay and z-loss.",
    "query": "Are there any research papers on using decoder-only Transformer models for long-sequence summarization in Wikipedia generation, including experimental setups and training methodologies?"
  },
  {
    "index": 861,
    "source_corpus_id": 255186293,
    "ref_id": "b8",
    "citation_corpus_id": 218486942,
    "start": 28207,
    "end": 28229,
    "title": "GoEmotions: A Dataset of Fine-Grained Emotions",
    "abstract": "Understanding emotion expressed in language has a wide range of applications, from building empathetic chatbots to detecting harmful online behavior. Advancement in this area can be improved using large-scale datasets with a fine-grained typology, adaptable to multiple downstream tasks. We introduce GoEmotions, the largest manually annotated dataset of 58k English Reddit comments, labeled for 27 emotion categories or Neutral. We demonstrate the high quality of the annotations via Principal Preserved Component Analysis. We conduct transfer learning experiments with existing emotion benchmarks to show that our dataset generalizes well to other domains and different emotion taxonomies. Our BERTbased model achieves an average F1-score of .46 across our proposed taxonomy, leaving much room for improvement. 1",
    "prev": "CIFAR-100 is a single-label image recognition task, which we perform in the challenging zero-shot classification setting using CLIP (Radford et al., 2021).",
    "curr": "In addition, we conduct a natural language processing experiment using the Go Emotions (Demszky et al., 2020) dataset and a fine-tuned BERT model (Devlin et al., 2018), where the goal is to recognize emotion in text and a single instance may have multiple labels.",
    "next": "Finally, we evaluate the interaction between our method and subgroup fairness using the UCI Nursery dataset (Dua and Graff, 2017), where applicants are ranked for admissions to school.",
    "query": "Are there any research papers on the GoEmotions dataset and methods for multi-label emotion recognition in text using BERT-based models?"
  },
  {
    "index": 862,
    "source_corpus_id": 256503523,
    "ref_id": "b17",
    "citation_corpus_id": 203836948,
    "start": 2454,
    "end": 2474,
    "title": "INDUCTIVE MATRIX COMPLETION BASED ON GRAPH NEURAL NETWORKS",
    "abstract": "We propose an inductive matrix completion model without using side information. By factorizing the (rating) matrix into the product of low-dimensional latent embeddings of rows (users) and columns (items), a majority of existing matrix completion methods are transductive, since the learned embeddings cannot generalize to unseen rows/columns or to new matrices. To make matrix completion inductive, most previous works use content (side information), such as user's age or movie's genre, to make predictions. However, high-quality content is not always available, and can be hard to extract. Under the extreme setting where not any side information is available other than the matrix to complete, can we still learn an inductive matrix completion model? In this paper, we propose an Inductive Graph-based Matrix Completion (IGMC) model to address this problem. IGMC trains a graph neural network (GNN) based purely on 1-hop subgraphs around (user, item) pairs generated from the rating matrix and maps these subgraphs to their corresponding ratings. It achieves highly competitive performance with state-of-the-art transductive baselines. In addition, IGMC is inductive -it can generalize to users/items unseen during the training (given that their interactions exist), and can even transfer to new tasks. Our transfer learning experiments show that a model trained out of the MovieLens dataset can be directly used to predict Douban movie ratings with surprisingly good performance. Our work demonstrates that: 1) it is possible to train inductive matrix completion models without using side information while achieving similar or better performances than state-of-the-art transductive methods; 2) local graph patterns around a (user, item) pair are effective predictors of the rating this user gives to the item; and 3) Long-range dependencies might not be necessary for modeling recommender systems.",
    "prev": "Introduction\n\nLink prediction is a crucial task in graph machine learning.",
    "curr": "It has various real-world applications, such as recommender systems (Zhang & Chen, 2020), knowledge graph completion (Zhu et al., 2021), and drug interaction prediction (Souri et al., 2022).",
    "next": "Graph Neural Networks have been used in link prediction.",
    "query": "Are there any research papers on inductive matrix completion using graph neural networks for link prediction tasks such as recommender systems, knowledge graph completion, and drug interaction prediction without relying on side information?"
  },
  {
    "index": 863,
    "source_corpus_id": 227209335,
    "ref_id": "b14",
    "citation_corpus_id": 52908831,
    "start": 24092,
    "end": 24116,
    "title": "FFJORD: FREE-FORM CONTINUOUS DYNAMICS FOR SCALABLE REVERSIBLE GENERATIVE MODELS",
    "abstract": "A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling. * Equal contribution. Order determined by coin toss.",
    "prev": "4).",
    "curr": "(Dinh et al., 2016) 3.49 -iResNet  3.45 -Glow (Kingma & Dhariwal, 2018) 3.35 -MintNet (Song et al., 2019b) 3.32 -Residual Flow  3.28 46.37 FFJORD (Grathwohl et al., 2018) 3  Exact likelihood computation Leveraging the connection to neural ODEs, we can compute the density defined by Eq.",
    "next": "(12) via the instantaneous change of variables formula .",
    "query": "Are there any research papers on generative models with exact likelihood computation using neural ODEs and Hutchinson's trace estimator for scalable reversible models?"
  },
  {
    "index": 867,
    "source_corpus_id": 234357892,
    "ref_id": "b16",
    "citation_corpus_id": 209461008,
    "start": 2979,
    "end": 2983,
    "title": "Published as a conference paper at ICLR 2020 COMPUTATION REALLOCATION FOR OBJECT DETECTION",
    "abstract": "The allocation of computation resources in the backbone is a crucial issue in object detection. However, classification allocation pattern is usually adopted directly to object detector, which is proved to be sub-optimal. In order to reallocate the engaged computation resources in a more efficient way, we present CR-NAS (Computation Reallocation Neural Architecture Search) that can learn computation reallocation strategies across different feature resolution and spatial position diectly on the target detection dataset. A two-level reallocation space is proposed for both stage and spatial reallocation. A novel hierarchical search procedure is adopted to cope with the complex search space. We apply CR-NAS to multiple backbones and achieve consistent improvements. Our CR-ResNet50 and CR-MobileNetV2 outperforms the baseline by 1.9% and 1.7% COCO AP respectively without any additional computation budget. The models discovered by CR-NAS can be equiped to other powerful detection neck/head and be easily transferred to other dataset, e.g. PASCAL VOC, and other vision tasks, e.g. instance segmentation. Our CR-NAS can be used as a plugin to improve the performance of various networks, which is demanding.",
    "prev": "Without considering the prior of faces, the backbone, neck and head design of TinaFace is thus redundant and sub-optimal.",
    "curr": "Since directly taking the backbone of the classification network for object detection is sub-optimal, the recent CR-NAS [17] reallocates the computation across different resolutions.",
    "next": "It is based on the observation that the allocation of computation across different resolutions has a great impact on the Effective Receptive Field (ERF) and affects the detection performance.",
    "query": "Are there any research papers on computation reallocation techniques for object detection using neural architecture search methods?"
  },
  {
    "index": 870,
    "source_corpus_id": 59608630,
    "ref_id": "b18",
    "citation_corpus_id": 3462549,
    "start": 14591,
    "end": 14610,
    "title": "FEW-SHOT AUTOREGRESSIVE DENSITY ESTIMATION: TOWARDS LEARNING TO LEARN DISTRIBUTIONS",
    "abstract": "Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet. However, such models require many thousands of gradient-based weight updates and unique image examples for training. Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks. In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset. Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision. Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.",
    "prev": "The concept of 'learning to learn' is not tied to a specific task and thus meta-learning algorithms have been successfully applied to a wide range of challenges like RL (Wang et al., 2016;Finn et al., 2017), program induction (Devlin et al., 2017) few-shot classification (Koch et al., 2015;Vinyals et al., 2016) and scene understanding .",
    "curr": "Some meta learning models generate predictions in an autoregressive fashion by predicting the next target point from the entire prior sequence of consecutive observations (Reed et al., 2018;Mishra et al., 2018).",
    "next": "Algorithms of this kind have delivered state-of-the art results in a range of tasks such as supervised learning to classification.",
    "query": "Are there any research papers on the application of meta-learning techniques to autoregressive models for few-shot density estimation in vision tasks?"
  },
  {
    "index": 871,
    "source_corpus_id": 263605885,
    "ref_id": "b8",
    "citation_corpus_id": 247315430,
    "start": 7124,
    "end": 7141,
    "title": "Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text",
    "abstract": "Modern neural language models can produce remarkably fluent and grammatical text. So much, in fact, that recent work by Clark et al.  (2021)  has reported that conventional crowdsourcing can no longer reliably distinguish between machine-authored (GPT-3) and humanauthored writing. As errors in machine generations become ever subtler and harder to spot, it poses a new challenge to the research community for robust machine text evaluation.We propose a new framework called SCARE-CROW for scrutinizing machine text via crowd annotation. To support the broad range of real machine errors that can be identified by laypeople, the ten error categories of SCARECROWsuch as redundancy , commonsense errors , and incoherence -are identified through several rounds of crowd annotation experiments without a predefined ontology.",
    "prev": "z f 8 G g Y e l r w A N 6 z n 7 j T 0 o 2 d G d K y t P y X S O f u 0 o u a F m y 3 n M w u O E / d c m 5 F / 0 w Y V j p N h L U t T I Z R i M W h c K e o t m l l N R 9 K C Q D X 1 g A s r / a 5 U T L j l A v 2 H t L w J j y + l / w Y X n 6 P 4 M G L f D j o n p 0 s 7 1 s l 7 8 o H s k p g c k x N y R s 5 J n w j y K y D B R t A K f o e r 4 W a 4 t U g N g 2 X N W / J H h O 8 e A K i X u X s = < / l a t e x i t > p ✓,µ (x) / p ✓ (x)e Eµ(x)   Figure 1: The decoding distribution p θ,µ induced by DAEMON scales the input LM distribution p θ with a sequence-level energy function E µ , which leads to a more accurate recovery of the underlying data distribution p d .",
    "curr": "Although pre-trained on large corpora of human texts with scaled up sizes, existing autoregressive language models (LMs) (Radford et al., 2019;Brown et al., 2020;Zhang et al., 2022) are still struggling to produce humanlike texts measured in various aspects, such as repetition, coherence, and consistency (Pillutla et al., 2021;Dou et al., 2022).Existing decoding methods are mainly driven to address two main mis-specifications of an LM's distribution: (i) The long tail of the distribution is unreliable (Holtzman et al., 2020), such that sampling from these low-probability regions often produces low-quality contents that are incoherent.",
    "next": "(ii) The mode of the distribution is degenerated (Welleck et al., 2020), where samples with high probabilities exhibit low diversity with repetitive patterns.As a result, sampling-based decoding methods (Fan et al., 2018;Holtzman et al., 2020;Meister et al., 2022) use various truncation strategies to avoid sampling from the unreliable long tail of the distribution, while recent search-based methods (Li et al., 2022;Su et al., 2022) incorporate additional contrastive objectives to avoid the collapse of degenerated repetitions.Since these two mis-specifications reside at opposing extremes of the probability spectrum, current decoding methods inevitably concentrate on Preprint.",
    "query": "Are there any studies that explore methods for evaluating the quality of machine-generated text, particularly focusing on issues like repetition, coherence, and consistency in autoregressive language models?"
  },
  {
    "index": 873,
    "source_corpus_id": 259243562,
    "ref_id": "b31",
    "citation_corpus_id": 6212000,
    "start": 1292,
    "end": 1310,
    "title": "UNDERSTANDING DEEP LEARNING REQUIRES RE- THINKING GENERALIZATION",
    "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on the generalization of deep learning models that challenge traditional explanations involving regularization and model properties, focusing on experiments showing networks can fit random data and the role of expressivity in generalization?"
  },
  {
    "index": 874,
    "source_corpus_id": 263608822,
    "ref_id": "b43",
    "citation_corpus_id": 254877499,
    "start": 1940,
    "end": 1961,
    "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions",
    "abstract": "Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, what to retrieve depends on what has already been derived, which in turn may depend on what was previously retrieved. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-ofdistribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning. 1 .",
    "prev": "at retrieval augmentation can sometimes have a negative effect on performance.In this work, we present a thorough analysis on five open-domain question answering benchmarks, characterizing cases when retrieval reduces accuracy.We then propose two methods to mitigate this issue.First, a simple baseline that filters out retrieved passages that do not entail question-answer pairs according to a natural language inference (NLI) model.This is effective in preventing performance reduction, but at a cost of also discarding relevant passages.Thus, we propose a method for automatically generating data to fine-tune the language model to properly leverage retrieved passages, using a mix of relevant and irrelevant contexts at training time.We empirically show that even 1,000 examples suffice to train the model to be robust to irrelevant contexts while maintaining high performance on examples with relevant ones.",
    "curr": "INTRODUCTION\n\nLarge Language Models (LLMs) (Brown et al., 2020;Chowdhery et al., 2022;Touvron et al., 2023) are the foundation on top of which modern language systems are built.However, open-domain question answering (ODQA; Chen et al.",
    "next": "2017) and other knowledge-intensive tasks (Thorne et al., 2018;Petroni et al., 2021) require vast amounts of up-to-date factual knowledge about rare entities that even very large models cannot memorize (Roberts et al., 2020;Dhingra et al., 2022).A dominant approach for combating this issue has been Retrieval Augmented Language Models (RALMs), which incorporate a retrieval mechanism to reduce the need for storing information in the LLM parameters (Guu et al., 2020;Lewis et al., 2020b;Izacard et al., 2022;Rubin & Berant, 2023).Furthermore, RALMs have also been shown to improve ODQA performance in an in-context setting (without any training), simply by prepending retrieved sentences to the input question (Ram et al., 2023).Nevertheless, retrievers are not perfect and past work has shown that noisy retrieval can negatively affect LLM performance (Petroni et al.,",
    "query": "Are there any research papers on methods that interleave retrieval with chain-of-thought reasoning for multi-step question answering to address knowledge gaps in large language models?"
  },
  {
    "index": 875,
    "source_corpus_id": 261530996,
    "ref_id": "b24",
    "citation_corpus_id": 3338083,
    "start": 3076,
    "end": 3080,
    "title": "Published as a conference paper at ICLR 2018 SYNTHESIZING REALISTIC NEURAL POPULATION ACTIVITY PATTERNS USING GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "The ability to synthesize realistic patterns of neural activity is crucial for studying neural information processing. Here we used the Generative Adversarial Networks (GANs) framework to simulate the concerted activity of a population of neurons. We adapted the Wasserstein-GAN variant to facilitate the generation of unconstrained neural population activity patterns while still benefiting from parameter sharing in the temporal domain. We demonstrate that our proposed GAN, which we termed Spike-GAN, generates spike trains that match accurately the first-and second-order statistics of datasets of tens of neurons and also approximates well their higher-order statistics. We applied Spike-GAN to a real dataset recorded from salamander retina and showed that it performs as well as state-ofthe-art approaches based on the maximum entropy and the dichotomized Gaussian frameworks. Importantly, Spike-GAN does not require to specify a priori the statistics to be matched by the model, and so constitutes a more flexible method than these alternative approaches. Finally, we show how to exploit a trained Spike-GAN to construct 'importance maps' to detect the most relevant statistical structures present in a spike train. Spike-GAN provides a powerful, easy-to-use technique for generating realistic spiking neural activity and for describing the most relevant features of the large-scale neural population recordings studied in modern systems neuroscience.",
    "prev": "That is, (P) minimizes f over x ∈ X and y ∈ Y (the upper-level problem) when y must be one of the minimizers of g(x, ·) over y ∈ Y (the lower-level problem).",
    "curr": "Scalable optimization methods for solving (P) are in high demand to handle increasingly large-scale applications in machine-learning, including meta-learning [50], hyper-parameter optimization [20,3], model selection [37,24], adversarial networks [25,23], game theory [55], and reinforcement learning [34,56].",
    "next": "There is particular interest in developing (stochastic) gradient-descent-based methods due to their simplicity and scalability to large-scale problems [22,13,28,33,12,15,27,54,30,62].",
    "query": "Are there any research papers on scalable optimization methods for solving bilevel problems in machine learning applications such as meta-learning, hyper-parameter optimization, and reinforcement learning?"
  },
  {
    "index": 876,
    "source_corpus_id": 257766959,
    "ref_id": "b12",
    "citation_corpus_id": 249209614,
    "start": 3570,
    "end": 3573,
    "title": "CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers",
    "abstract": "Large-scale pretrained transformers have created milestones in text (GPT-3) and text-to-image (DALL-E and CogView) generation.Its application to video generation is still facing many challenges: The potential huge computation cost makes the training from scratch unaffordable; The scarcity and weak relevance of text-video datasets hinder the model understanding complex movement semantics.In this work, we present 9B-parameter transformer CogVideo, trained by inheriting a pretrained text-to-image model, CogView2.We also propose multi-frame-rate hierarchical training strategy to better align text and video clips.As (probably) the first open-source large-scale pretrained text-to-video model, CogVideo outperforms all publicly available models at a large margin in machine and human evaluations.* Equal contribution.Preprint.Under review.",
    "prev": "Despite its potential benefits, text-conditioned video prediction (TVP) is a challenging task because it requires a deep understanding of the initial frames, the natural language instruction, and the grounding between language and images, while predicting based upon all the information above.",
    "curr": "The traditional text-conditioned video generation task [35,36,13,12,26] does not condition on initial frames and thus a model could seemingly perform well if it only generates a few prototypical videos corresponding to the input text.",
    "next": "The TVP task is much more challenging as the initial frames are given and generating prototypical videos is no longer a solution.",
    "query": "Are there any research papers on large-scale pretraining methods for text-to-video generation using transformers, addressing challenges in computational cost and dataset scarcity, and employing multi-frame-rate training strategies?"
  },
  {
    "index": 878,
    "source_corpus_id": 257254919,
    "ref_id": "b3",
    "citation_corpus_id": 21529792,
    "start": 2126,
    "end": 2142,
    "title": "Published as a conference paper at ICLR 2018 LEARNING ROBUST REWARDS WITH ADVERSARIAL INVERSE REINFORCEMENT LEARNING",
    "abstract": "Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose AIRL, a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that AIRL greatly outperforms prior methods in these transfer settings.",
    "prev": "1\n\nINTRODUCTION\n\nInverse Reinforcement Learning (IRL) techniques have been developed to robustly extract behaviors from expert demonstration and solve the problems of classical Imitation Learning (IL) methods (Ng et al., 1999;Ziebart et al., 2008).",
    "curr": "Among the recent methods for IRL, the Adversarial Imitation Learning (AIL) approach (Ho & Ermon, 2016;Fu et al., 2018;Peng et al., 2021), which casts the optimization over rewards and policies into an adversarial setting, have been proven particularly successful.",
    "next": "These methods, inspired by Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), alternate between learning a discriminator, and improving the agent's policy w.r.t.",
    "query": "Are there any research papers on adversarial inverse reinforcement learning methods for learning robust rewards in high-dimensional or dynamic environments?"
  },
  {
    "index": 885,
    "source_corpus_id": 1638605,
    "ref_id": "b0",
    "citation_corpus_id": 13239813,
    "start": 3632,
    "end": 3657,
    "title": "Automatic Text Scoring Using Neural Networks",
    "abstract": "Automated Text Scoring (ATS) provides a cost-effective and consistent alternative to human marking. However, in order to achieve good performance, the predictive features of the system need to be manually engineered by human experts. We introduce a model that forms word representations by learning the extent to which specific words contribute to the text's score. Using Long-Short Term Memory networks to represent the meaning of texts, we demonstrate that a fully automated framework is able to achieve excellent results over similar approaches. In an attempt to make our results more interpretable, and inspired by recent advances in visualizing neural networks, we introduce a novel method for identifying the regions of the text that the model has found more discriminative.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on automatic text scoring using neural networks and methods for identifying discriminative text regions in natural language processing?"
  },
  {
    "index": 887,
    "source_corpus_id": 257078711,
    "ref_id": "b8",
    "citation_corpus_id": 246430387,
    "start": 27784,
    "end": 27801,
    "title": "COUNTERFACTUAL PLANS UNDER DISTRIBUTIONAL AMBIGUITY",
    "abstract": "Counterfactual explanations are attracting significant attention due to the flourishing applications of machine learning models in consequential domains. A counterfactual plan consists of multiple possibilities to modify a given instance so that the model's prediction will be altered. As the predictive model can be updated subject to the future arrival of new data, a counterfactual plan may become ineffective or infeasible with respect to the future values of the model parameters. In this work, we study the counterfactual plans under model uncertainty, in which the distribution of the model parameters is partially prescribed using only the first-and second-moment information. First, we propose an uncertainty quantification tool to compute the lower and upper bounds of the probability of validity for any given counterfactual plan. We then provide corrective methods to adjust the counterfactual plan to improve the validity measure. The numerical experiments validate our bounds and demonstrate that our correction increases the robustness of the counterfactual plans in different real-world datasets.arXiv:2201.12487v2 [cs.LG] 10 Apr 2022Published as a conference paper at ICLR 2022 of the applicant so that the modification is actionable. Further, the plan should consist of a diverse range of recourses to accommodate the different tastes and preferences of the population. Russell (2019) propose a mixed-integer programming method to generate a counterfactual plan for a linear classifier, in which the diversity is imposed using a rule-based approach. InDandl et al. (2020), the authors propose a model-agnostic approach using a multi-objective evolutionary algorithm to construct a diverse counterfactual plan. More recently,Mothilal et al. (2020)use the determinantal point process to measure the diversity of a plan. The authors then formulate an optimization problem to find the counterfactual plan that minimizes the weighted sum of three terms representing validity, proximity, and diversity.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any studies on counterfactual plans under model uncertainty, particularly methods for quantifying uncertainty and improving robustness through validity and diversity measures?"
  },
  {
    "index": 889,
    "source_corpus_id": 252280353,
    "ref_id": "b15",
    "citation_corpus_id": 220347682,
    "start": 16868,
    "end": 16897,
    "title": "In Search of Lost Domain Generalization",
    "abstract": "The goal of domain generalization algorithms is to predict well on distributions different from those seen during training. While a myriad of domain generalization algorithms exist, inconsistencies in experimental conditions-datasets, architectures, and model selection criteria-render fair and realistic comparisons difficult. In this paper, we are interested in understanding how useful domain generalization algorithms are in realistic settings. As a first step, we realize that model selection is non-trivial for domain generalization tasks. Contrary to prior work, we argue that domain generalization algorithms without a model selection strategy should be regarded as incomplete. Next, we implement DOMAINBED, a testbed for domain generalization including seven multi-domain datasets, nine baseline algorithms, and three model selection criteria. We conduct extensive experiments using DO-MAINBED and find that, when carefully implemented, empirical risk minimization shows state-of-the-art performance across all datasets. Looking forward, we hope that the release of DOMAINBED, along with contributions from fellow researchers, will streamline reproducible and rigorous research in domain generalization. * Alphabetical order, equal contribution.Preprint. Under review.",
    "prev": "Time series OOD algorithms are currently less studied and there are only two recent strong approaches for comparison: GILE (Qian et al., 2021) and AdaRNN (Du et al., 2021).",
    "curr": "5 We further compare with 7 general OOD methods 6 from DomainBed (Gulrajani & Lopez-Paz, 2021): ERM, DANN (Ganin et al., 2016), CORAL (Sun & Saenko, 2016), Mixup (Zhang et al., 2018), GroupDRO (Sagawa et al., 2020), RSC (Huang et al., 2020), and ANDMask (Parascandolo et al., 2021).",
    "next": "More details of these methods are in Sec.",
    "query": "Are there any studies that evaluate domain generalization algorithms for time series data, particularly comparing methods like GILE, AdaRNN, and general OOD techniques from DomainBed?"
  },
  {
    "index": 891,
    "source_corpus_id": 208202099,
    "ref_id": "b32",
    "citation_corpus_id": 53792719,
    "start": 4767,
    "end": 4787,
    "title": "HIERARCHICAL VISUOMOTOR CONTROL OF HUMANOIDS",
    "abstract": "We aim to build complex humanoid agents that integrate perception, motor control, and memory. In this work, we partly factor this problem into low-level motor control from proprioception and high-level coordination of the low-level skills informed by vision. We develop an architecture capable of surprisingly flexible, task-directed motor control of a relatively high-DoF humanoid body by combining pre-training of low-level motor controllers with a high-level, task-focused controller that switches among low-level sub-policies. The resulting system is able to control a physically-simulated humanoid body to solve tasks that require coupling visual perception from an unstabilized egocentric RGB camera during locomotion in the environment. Supplementary video link 1 * Joint first authors.",
    "prev": "The virtual rodent affords a new opportunity to directly compare principles of artificial control to biological data from real-world rodents, which are more experimentally accessible than humans.",
    "curr": "We draw inspiration from emerging deep reinforcement learning algorithms which now allow artificial agents to perform complex and adaptive movement in physical environments with sensory information that is increasingly similar to that available to animals (Peng et al., 2016;Heess et al., 2017;Merel et al., 2019a;.",
    "next": "Similarly, our virtual rodent exists in a physical world, equipped with a set of actuators that must be coordinated for it to behave effectively.",
    "query": "Are there any research papers on hierarchical visuomotor control in humanoid agents using reinforcement learning and integrating visual perception with motor control?"
  },
  {
    "index": 892,
    "source_corpus_id": 245634209,
    "ref_id": "b13",
    "citation_corpus_id": 202230734,
    "start": 4254,
    "end": 4275,
    "title": "A BASELINE FOR FEW-SHOT IMAGE CLASSIFICATION",
    "abstract": "Fine-tuning a deep network trained with the standard cross-entropy loss is a strong baseline for few-shot learning. When fine-tuned transductively, this outperforms the current state-of-the-art on standard datasets such as Mini-Imagenet, Tiered-Imagenet, CIFAR-FS and FC-100 with the same hyper-parameters. The simplicity of this approach enables us to demonstrate the first few-shot learning results on the Imagenet-21k dataset. We find that using a large number of meta-training classes results in high few-shot accuracies even for a large number of few-shot classes. We do not advocate our approach as the solution for few-shot learning, but simply use the results to highlight limitations of current benchmarks and few-shot protocols. We perform extensive studies on benchmark datasets to propose a metric that quantifies the \"hardness\" of a few-shot episode. This metric can be used to report the performance of few-shot algorithms in a more systematic way.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on using fine-tuning with cross-entropy loss as a baseline for few-shot image classification and methods to quantify the hardness of few-shot learning episodes?"
  },
  {
    "index": 893,
    "source_corpus_id": 263835408,
    "ref_id": "b14",
    "citation_corpus_id": 44134226,
    "start": 9895,
    "end": 9913,
    "title": "Hierarchical Neural Story Generation",
    "abstract": "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",
    "prev": "Greedy decoding, which simply selects the most probable token under IP θ (x i |x 1:i−1 ), is a special case of sampling when the temperature τ = 0.",
    "curr": "Variants of sampling-based decoding include top-p sampling (Holtzman et al., 2020) and top-k sampling (Fan et al., 2018), which restrict the sampling to the most probable tokens.",
    "next": "FIXED GENERATION CONFIGURATION FOR SAFETY EVALUATION\n\nWe notice that open-source LLMs are usually evaluated for alignment using only default generation methods, and this potentially leaves them vulnerable to misalignment when alternative strategies are used.",
    "query": "Are there any studies that explore hierarchical story generation techniques using decoding strategies like top-p sampling, model fusion, and multi-scale self-attention mechanisms?"
  },
  {
    "index": 896,
    "source_corpus_id": 219965999,
    "ref_id": "b33",
    "citation_corpus_id": 211532691,
    "start": 5026,
    "end": 5030,
    "title": "RIDE: REWARDING IMPACT-DRIVEN EXPLORATION FOR PROCEDURALLY-GENERATED ENVIRONMENTS",
    "abstract": "Exploration in sparse reward environments remains one of the key challenges of model-free reinforcement learning. Instead of solely relying on extrinsic rewards provided by the environment, many state-of-the-art methods use intrinsic rewards to encourage exploration. However, we show that existing methods fall short in procedurally-generated environments where an agent is unlikely to visit a state more than once. We propose a novel type of intrinsic reward which encourages the agent to take actions that lead to significant changes in its learned state representation. We evaluate our method on multiple challenging procedurally-generated tasks in MiniGrid, as well as on tasks with high-dimensional observations used in prior work. Our experiments demonstrate that this approach is more sample efficient than existing exploration methods, particularly for procedurally-generated MiniGrid environments. Furthermore, we analyze the learned behavior as well as the intrinsic reward received by our agent. In contrast to previous approaches, our intrinsic reward does not diminish during the course of training and it rewards the agent substantially more for interacting with objects that it can control. * Work done during an internship at Facebook AI Research.",
    "prev": "We evaluate AMIGO on MiniGrid [6], a suite of fast-to-run procedurally-generated environments with a symbolic/discrete (expressed in terms of objects like walls, doors, keys, chests and balls) observation space which isolates the problem of exploration from that of visual perception.",
    "curr": "Furthermore, [34] found that MiniGrid presents a particular challenge for existing state-of-the-art intrinsic motivation approaches.",
    "next": "Here, AMIGO sets a new state-of-the-art on some of the hardest MiniGrid environments [6], being the only method based on intrinsic motivation capable of successfully obtaining extrinsic reward on some of them.",
    "query": "Are there any research papers on intrinsic motivation methods for enhancing exploration in procedurally-generated environments, such as MiniGrid, using impact-driven reward mechanisms?"
  },
  {
    "index": 902,
    "source_corpus_id": 229212743,
    "ref_id": "b20",
    "citation_corpus_id": 6706414,
    "start": 42302,
    "end": 42327,
    "title": "Published as a conference paper at ICLR 2015 EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES",
    "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
    "prev": "Following the protocol in (Yu et al., 2019), we evaluate the robustness against five types of image perturbation: cropping and resizing, blurring with Gaussian kernel, JPEG compression, additive Gaussian noise, and random combination of them.We consider two versions of our model: the original version and the immunized version.An immunized model indicates that during training we augment generated images with the corresponding perturbation in random strengths before feeding them to the fingerprint decoder.",
    "curr": "It is worth noting that none of the encoder, decoder, and training data are accessible to the public.Therefore, the robustness against perturbation has to be experimented with the black-box assumption, as protocoled in (Yu et al., 2019).In other words, white-box perturbations such as adversarial image modifications (Goodfellow et al., 2015) and fingerprint overwriting, which requires access to the encoder, decoder, and/or training data, are not applicable in our scenario.",
    "next": "We plot in Figure 5 the comparisons of fingerprint detection accuracy among our original/immunized models and the models of (Yu et al., 2021) w.r.t.",
    "query": "Are there any studies that explore methods for enhancing adversarial robustness in machine learning models under black-box assumptions without access to encoder, decoder, or training data?"
  },
  {
    "index": 904,
    "source_corpus_id": 256615829,
    "ref_id": "b26",
    "citation_corpus_id": 221739314,
    "start": 2570,
    "end": 2586,
    "title": "Contextualized Perturbation for Textual Adversarial Attack",
    "abstract": "Adversarial examples expose the vulnerabilities of natural language processing (NLP) models, and can be used to evaluate and improve their robustness. Existing techniques of generating such examples are typically driven by local heuristic rules that are agnostic to the context, often resulting in unnatural and ungrammatical outputs. This paper presents CLARE, a ContextuaLized AdversaRial Example generation model that produces fluent and grammatical outputs through a mask-then-infill procedure. CLARE builds on a pre-trained masked language model and modifies the inputs in a contextaware manner. We propose three contextualized perturbations, Replace, Insert and Merge, that allow for generating outputs of varied lengths. CLARE can flexibly combine these perturbations and apply them at any position in the inputs, and is thus able to attack the victim model more effectively with fewer edits. Extensive experiments and human evaluation demonstrate that CLARE outperforms the baselines in terms of attack success rate, textual similarity, fluency and grammaticality. Young. 2016. Counter-fitting word vectors to linguistic constraints. In Proc. of NAACL.Daniel Naber et al. 2003. A rule-based style and grammar checker. Citeseer.",
    "prev": "INTRODUCTION\n\nDeep Neural Networks (DNNs) have obtained great progress in the field of natural language processing (NLP) but are vulnerable to adversarial attacks, leading to security and safety concerns, and research on defense algorithms against such attacks is urgently needed.",
    "curr": "Specifically, the most common attack for NLP is word-level attack (Wang et al., 2019b;Garg & Ramakrishnan, 2020;Zang et al., 2020;Li et al., 2021), which is usually implemented by adding, deleting or substituting words within a sentence.",
    "next": "Such an attack often brings catastrophic performance degradation to DNN-based models.",
    "query": "Are there any research papers on context-aware methods for generating effective textual adversarial attacks in natural language processing models?"
  },
  {
    "index": 907,
    "source_corpus_id": 219965949,
    "ref_id": "b31",
    "citation_corpus_id": 14307651,
    "start": 13721,
    "end": 13725,
    "title": "THE CONCRETE DISTRIBUTION: A CONTINUOUS RELAXATION OF DISCRETE RANDOM VARIABLES",
    "abstract": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce CONCRETE random variables-CONtinuous relaxations of disCRETE random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.",
    "prev": "As also mentioned by Tran et al.",
    "curr": "[52], previous work on Gumbel-softmax distributions indicate that the gradient approximation works well when the number of classes is lower than 200 [32,23].",
    "next": "Finally, the question remains if a sequence of additive bijectors of the form of Eq.",
    "query": "Are there any research papers on continuous relaxations of discrete random variables using reparameterization techniques, such as the Concrete distribution, for tasks like density estimation and structured prediction with neural networks?"
  },
  {
    "index": 908,
    "source_corpus_id": 57375714,
    "ref_id": "b12",
    "citation_corpus_id": 9996719,
    "start": 6983,
    "end": 7005,
    "title": "Published as a conference paper at ICLR 2016 GENERATING IMAGES FROM CAPTIONS WITH ATTENTION",
    "abstract": "Motivated by the recent progress in generative models, we introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. After training on Microsoft COCO, we compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on generating images from natural language captions using attention-based models?"
  },
  {
    "index": 910,
    "source_corpus_id": 12200521,
    "ref_id": "b14",
    "citation_corpus_id": 252796,
    "start": 13724,
    "end": 13745,
    "title": "Building a Large Annotated Corpus of English: The Penn Treebank",
    "abstract": "There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenomena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large corpora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valuable for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investigation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models.In this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus 1 consisting of over 4.5 million words of American English. During the first three-year phase of the Penn Treebank Project (1989)(1990)(1991)(1992), this corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure. These materials are available to members of the Linguistic Data Consortium; for details, see Section 5.1.The paper is organized as follows. Section 2 discusses the POS tagging task. After outlining the considerations that informed the design of our POS tagset and presenting the tagset itself, we describe our two-stage tagging process, in which text is first assigned POS tags automatically and then corrected by human annotators. Section 3 briefly presents the results of a comparison between entirely manual and semi-automated tagging, with the latter being shown to be superior on three counts: speed, consistency, and accuracy. In Section 4, we turn to the bracketing task. Just as with the tagging task, we have partially automated the bracketing task: the output of",
    "prev": "Experiments and Discussion\n\nWe evaluate zoneout's performance on the following tasks:\n\n• Classification of hand-written digits on permuted sequential MNIST [Le et al., 2015].",
    "curr": "• Word-level language modeling on the Penn Treebank corpus [Marcus et al., 1993].",
    "next": "• Character-level language modeling on the Penn Treebank corpus\n\nWe first investigate zoneout with a shared zoneout mask on cells and hiddens on the above tasks and compare its performance with other regularizers.",
    "query": "Are there any research papers on the application of zoneout in language modeling tasks, such as word-level and character-level models on the Penn Treebank corpus?"
  },
  {
    "index": 912,
    "source_corpus_id": 11480374,
    "ref_id": "b9",
    "citation_corpus_id": 7228830,
    "start": 5988,
    "end": 6007,
    "title": "Adversarial Examples for Evaluating Reading Comprehension Systems",
    "abstract": "Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear.To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75% F1 score to 36%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7%. We hope our insights will motivate the development of new models that understand language more precisely.",
    "prev": "The ensemble model achieves an exact match (EM) score of 78.8% and F1 score of 85.9%.",
    "curr": "Furthermore, we have tested FusionNet against adversarial SQuAD datasets (Jia & Liang, 2017).",
    "next": "Results show that FusionNet outperforms existing state-of-the-art architectures in both datasets: on AddSent, FusionNet increases the best F1 metric from 46.6% to 51.4%; on AddOneSent, FusionNet boosts the best F1 metric from 56.0% to 60.7%.",
    "query": "Are there any research papers on adversarial evaluation methods for reading comprehension systems using adversarial examples?"
  },
  {
    "index": 915,
    "source_corpus_id": 851777,
    "ref_id": "b7",
    "citation_corpus_id": 11383178,
    "start": 20317,
    "end": 20337,
    "title": "IMPORTANCE WEIGHTED AUTOENCODERS",
    "abstract": "The variational autoencoder (VAE; ) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.",
    "prev": "RECENT METHODS IN VARIATIONAL INFERENCE\n\nWe demonstrate Edward's flexibility for experimenting with complex inference algorithms.",
    "curr": "We consider the VAE setup from Figure 2 and  ≤ 85.8 Hierarchical variational model  ≤ 85.4 Importance-weighted auto-encoders (K = 50) (Burda et al., 2016) ≤ 86.3 HVM with IWAE objective (K = 5) ≤ 85.2 Rényi divergence (α = −1) (Li & Turner, 2016) ≤ 140.5 Table 1: Inference methods for a probabilistic decoder on binarized MNIST.",
    "next": "The Edward PPL is a convenient research platform, making it easy to both develop and experiment with many algorithms.",
    "query": "Are there any research papers on improved variational inference techniques for generative models, such as importance-weighted autoencoders and hierarchical variational methods?"
  },
  {
    "index": 916,
    "source_corpus_id": 247158860,
    "ref_id": "b10",
    "citation_corpus_id": 67915085,
    "start": 9066,
    "end": 9086,
    "title": "STOCHASTIC OPTIMIZATION OF SORTING NETWORKS VIA CONTINUOUS RELAXATIONS",
    "abstract": "Sorting input objects is an important step in many machine learning pipelines. However, the sorting operator is non-differentiable with respect to its inputs, which prohibits end-to-end gradient-based optimization. In this work, we propose NeuralSort, a general-purpose continuous relaxation of the output of the sorting operator from permutation matrices to the set of unimodal row-stochastic matrices, where every row sums to one and has a distinct arg max. This relaxation permits straight-through optimization of any computational graph involve a sorting operation. Further, we use this relaxation to enable gradient-based stochastic optimization over the combinatorially large space of permutations by deriving a reparameterized gradient estimator for the Plackett-Luce family of distributions over permutations. We demonstrate the usefulness of our framework on three tasks that require learning semantic orderings of high-dimensional objects, including a fully differentiable, parameterized extension of the k-nearest neighbors algorithm. ;Chierichetti et al., 2018). By learning distributions over unobserved permutations, we can account for the uncertainty in these permutations in a principled manner. However, the challenge with stochastic optimization over discrete distributions lies in gradient estimation with respect to the distribution parameters. Vanilla REINFORCE estimators are impractical for most cases, or necessitate custom control variates for low-variance gradient estimation (Glasserman, 2013).In this regard, we consider the Plackett-Luce (PL) family of distributions over permutations (Plackett, 1975; Luce, 1959). A common modeling choice for ranking models, the PL distribution is parameterized by n scores, with its support defined over the symmetric group consisting of n! permutations. We derive a reparameterizable sampler for stochastic optimization with respect to this distribution, based on Gumbel perturbations to the n (log-)scores. However, the reparameterized sampler requires sorting these perturbed scores, and hence the gradients of a downstream learning objective with respect to the scores are not defined. By using NeuralSort instead, we can approximate the objective and obtain well-defined reparameterized gradient estimates for stochastic optimization.Finally, we apply NeuralSort to tasks that require us to learn semantic orderings of complex, highdimensional input data. First, we consider sorting images of handwritten digits, where the goal is to learn to sort images by their unobserved labels. Our second task extends the first one to quantile regression, where we want to estimate the median (50-th percentile) of a set of handwritten numbers. In addition to identifying the index of the median image in the sequence, we need to learn to map the inferred median digit to its scalar representation. In the third task, we propose an algorithm that learns a basis representation for the k-nearest neighbors (kNN) classifier in an end-to-end procedure. Because the choice of the k nearest neighbors requires a non-differentiable sorting, we use NeuralSort to obtain an approximate, differentiable surrogate. On all tasks, we observe significant empirical improvements due to NeuralSort over the relevant baselines and competing relaxations to permutation matrices.PRELIMINARIESAn n-dimensional permutation z = [z 1 , z 2 , . . . , z n ] T is a list of unique indices {1, 2, . . . , n}. Every permutation z is associated with a permutation matrix P z ∈ {0, 1} n×n with entries given as: P z [i, j] = 1 if j = z i 0 otherwise. multimodal graph matching: Sparse coding meets graph matching. In Advances in Neural Information Processing Systems, 2013.",
    "prev": "Differentiable sorting & ranking.",
    "curr": "Differentiable sorting and ranking algorithms (Adams & Zemel, 2011;Grover et al., 2018;Blondel et al., 2020;Petersen et al., 2021) can be used in training neural networks with sorting and ranking supervision.",
    "next": "Recent approach (Blondel et al., 2020) proposes to construct differentiable sorting and ranking operators as projections onto the permutahedron, i.e., the convex hull of permutations, and using a reduction to isotonic optimization.",
    "query": "Are there any research papers on differentiable sorting and ranking methods using continuous relaxations for gradient-based optimization in neural networks?"
  },
  {
    "index": 917,
    "source_corpus_id": 53576131,
    "ref_id": "b3",
    "citation_corpus_id": 38407095,
    "start": 5929,
    "end": 5942,
    "title": "Europarl: A Parallel Corpus for Statistical Machine Translation",
    "abstract": "We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web 1 . This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT). We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead.",
    "prev": "• We systematically analyze what information is captured by units in representation across multiple settings by varying network architectures, tasks, and datasets.",
    "curr": "We use VD- CNN (Conneau et al., 2017) for sentiment and topic classification tasks on Yelp Reviews, AG News (Zhang et al., 2015), and DBpedia ontology dataset (Lehmann et al., 2015) and ByteNet (Kalchbrenner et al., 2016) for translation tasks on Europarl (Koehn, 2005) and News Commentary (Tiedemann, 2012) datasets.",
    "next": "• We also analyze how aligned natural language concepts evolve as they get represented in deeper layers.",
    "query": "Are there any research papers on analyzing neural network representations across different architectures and tasks, such as sentiment classification and machine translation, using models like VD-CNN and ByteNet?"
  },
  {
    "index": 921,
    "source_corpus_id": 219636236,
    "ref_id": "b6",
    "citation_corpus_id": 52889459,
    "start": 27586,
    "end": 27590,
    "title": "LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS",
    "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by truncating the latent space. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128×128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.3 and Fréchet Inception Distance (FID) of 9.6, improving over the previous best IS of 52.52 and FID of 18.65.",
    "prev": "CLUE's latent space optimization mechanism allows it to cope well with high dimensional data.",
    "curr": "Future work can leverage recent advances in scalable BNNs [37] and generative modeling [38] to explore using CLUE for more complex data, such as natural images and natural language.",
    "next": "Broader Impact\n\nAs machine learning models are deployed in high-stakes scenarios, there has been a call for algorithmic transparency into models' behavior.",
    "query": "Are there any research papers on large-scale GAN training for high-fidelity natural image synthesis using orthogonal regularization and truncation tricks?"
  },
  {
    "index": 925,
    "source_corpus_id": 249953817,
    "ref_id": "b7",
    "citation_corpus_id": 52967399,
    "start": 2663,
    "end": 2666,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Predicting visual observations for embodied agents is however challenging and computationally demanding: the model needs to capture the complexity and inherent stochasticity of future events while maintaining an inference speed that supports the robot's actions.",
    "curr": "Therefore, recent advances in autoregressive generative models, which leverage Transformers [7] for building neural architectures and learn good representations via self-supervised generative pretraining [8], have not benefited video prediction or robotic applications.",
    "next": "We in particular identify three technical challenges.",
    "query": "Are there any studies that explore the challenges of applying autoregressive generative models with Transformers for video prediction in robotic applications?"
  },
  {
    "index": 927,
    "source_corpus_id": 20140417,
    "ref_id": "b0",
    "citation_corpus_id": 11383178,
    "start": 1933,
    "end": 1953,
    "title": "IMPORTANCE WEIGHTED AUTOENCODERS",
    "abstract": "The variational autoencoder (VAE; ) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.",
    "prev": "We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.",
    "curr": "INTRODUCTION\n\nWe build upon AESMC , a method for model learning that itself builds on variational auto-encoders (VAEs) (Kingma & Welling, 2014;Rezende et al., 2014) and importance weighted auto-encoders (IWAEs) (Burda et al., 2016).",
    "next": "AESMC is similarly based on maximizing a lower bound to the log marginal likelihood, but uses SMC (Doucet & Johansen, 2009) as the underlying marginal likelihood estimator instead of importance sampling (IS).",
    "query": "Are there any research papers on importance weighted autoencoders and their use of sequential Monte Carlo for improved log-likelihood estimation in generative models?"
  },
  {
    "index": 930,
    "source_corpus_id": 238856821,
    "ref_id": "b48",
    "citation_corpus_id": 227230646,
    "start": 2994,
    "end": 3011,
    "title": "Distill and Replay for Continual Language Learning",
    "abstract": "Accumulating knowledge to tackle new tasks without necessarily forgetting the old ones is a hallmark of human-like intelligence. But the current dominant paradigm of machine learning is still to train a model that works well on static datasets. When learning tasks in a stream where data distribution may fluctuate, fitting on new tasks often leads to forgetting on the previous ones. We propose a simple yet effective framework that continually learns natural language understanding tasks with one model. Our framework distills knowledge and replays experience from previous tasks when fitting on a new task, thus named DnR (distill and replay). The framework is based on language models and can be smoothly built with different language model architectures. Experimental results demonstrate that DnR outperfoms previous state-of-the-art models in continually learning tasks of the same type but from different domains, as well as tasks of radically different types. With the distillation method, we further show that it's possible for DnR to incrementally compress the model size while still outperforming most of the baselines. We hope that DnR could promote the empirical application of continual language learning, and contribute to building human-level language intelligence minimally bothered by catastrophic forgetting. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/.",
    "prev": "More recent methods attempt to learn from different types of tasks.",
    "curr": "These include LAMOL (Sun et al., 2019) and its improvements (Chuang et al., 2020;Sun et al., 2020;Kanwatchara et al., 2021).",
    "next": "Despite the effectiveness of these methods in LLL, there are several limitations.",
    "query": "Are there any research papers on methods for continual language learning that use distillation and replay to prevent catastrophic forgetting?"
  },
  {
    "index": 931,
    "source_corpus_id": 220042361,
    "ref_id": "b0",
    "citation_corpus_id": 202541346,
    "start": 4131,
    "end": 4152,
    "title": "Published as a conference paper at ICLR 2020 PREDICTION, CONSISTENCY, CURVATURE: REPRESEN- TATION LEARNING FOR LOCALLY-LINEAR CONTROL",
    "abstract": "Many real-world sequential decision-making problems can be formulated as optimal control with high-dimensional observations and unknown dynamics. A promising approach is to embed the high-dimensional observations into a lowerdimensional latent representation space, estimate the latent dynamics model, then utilize this model for control in the latent space. An important open question is how to learn a representation that is amenable to existing control algorithms? In this paper, we focus on learning representations for locally-linear control algorithms, such as iterative LQR (iLQR). By formulating and analyzing the representation learning problem from an optimal control perspective, we establish three underlying principles that the learned representation should comprise: 1) accurate prediction in the observation space, 2) consistency between latent and observation space dynamics, and 3) low curvature in the latent space transitions. These principles naturally correspond to a loss function that consists of three terms: prediction, consistency, and curvature (PCC). Crucially, to make PCC tractable, we derive an amortized variational bound for the PCC loss function. Extensive experiments on benchmark domains demonstrate that the new variational-PCC learning algorithm benefits from significantly more stable and reproducible training, and leads to superior control performance. Further ablation studies give support to the importance of all three PCC components for learning a good latent space for control.Published as a conference paper at ICLR 2020 the process of encoding, transitioning via the latent dynamics, and then decoding, to adhere to the true observation dynamics. The second is consistency: given the ability to encode a observation trajectory sampled from the true environment, we expect the latent dynamics to be consistent with the encoded trajectory. Finally, curvature: in order to learn a latent space that is specifically amenable to LLC algorithms, we expect the (learned) latent dynamics to exhibit low curvature in order to minimize the approximation error of its first-order Taylor expansion employed by LLC algorithms. Our contributions are thus as follows: (1) We propose the Prediction, Consistency, and Curvature (PCC) framework for learning a latent space that is amenable to LLC algorithms and show that the elements of PCC arise systematically from bounding the suboptimality of the solution of the LLC algorithm in the latent space. (2) We design a latent variable model that adheres to the PCC framework and derive a tractable variational bound for training the model.(3)To the best of our knowledge, our proposed curvature loss for the transition dynamics (in the latent space) is novel. We also propose a direct amortization of the Jacobian calculation in the curvature loss to help training with curvature loss more efficiently. (4) Through extensive experimental comparison, we show that the PCC model consistently outperforms E2C (Watter et al., 2015)  and RCE (Banijamali et al., 2018)   on a number of control-from-images tasks, and verify via ablation, the importance of regularizing the model to have consistency and low-curvature. . Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems, pp. 4754-4765, 2018. Roy De Maesschalck, Delphine Jouan-Rimbaud, and Désiré L Massart. The mahalanobis distance. Chemometrics and intelligent laboratory systems, 50(1):1-18, 2000. Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In . Visual foresight: Model-based deep reinforcement learning for vision-based robotic control. arXiv preprint arXiv:1812.00568, 2018. Bernard Espiau, François Chaumette, and Patrick Rives. A new approach to visual servoing in robotics. , et al. Model-based reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019. 9 Published as a conference paper at ICLR 2020 Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. She. Singularity-avoiding swing-up control for underactuated three-link gymnast robot using virtual coupling between control torques. International Journal of Robust and Nonlinear Control, 25(2):207-221, 2015. Weiwei Li and Emanuel Todorov. Iterative linear quadratic regulator design for nonlinear biological movement systems. In ICINCO (1), pp. 222-229, 2004. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.Kevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012.Erik Ordentlich and Marcelo J Weinberger. A distribution dependent refinement of pinsker's inequality.",
    "prev": "To address this issue, a class of algorithms have been developed that are based on learning a lowdimensional latent (embedding) space and a latent model (dynamics), and then using this model to control the system in the latent space.",
    "curr": "This class has been referred to as learning control-lable embedding (LCE) and includes recently developed algorithms, such as E2C [Watter et al., 2015], RCE [Banijamali et al., 2018], SOLAR [Zhang et al., 2019], PCC [Levine et al., 2020], Dreamer [Hafner et al., 2020], and PC3 .",
    "next": "The following two properties are extremely important in designing LCE models and algorithms.",
    "query": "Are there any research papers on methods for learning controllable latent representations in control systems using prediction, consistency, and curvature principles for locally-linear control algorithms?"
  },
  {
    "index": 932,
    "source_corpus_id": 264590171,
    "ref_id": "b14",
    "citation_corpus_id": 246634193,
    "start": 1799,
    "end": 1818,
    "title": "Published as a conference paper at ICLR 2023 CORRUPTED IMAGE MODELING FOR SELF-SUPERVISED VISUAL PRE-TRAINING",
    "abstract": "We introduce Corrupted Image Modeling (CIM) for self-supervised visual pretraining. CIM uses an auxiliary generator with a small trainable BEiT (Bao et al.,  2021)  to corrupt the input image instead of using artificial [MASK] tokens, where some patches are randomly selected and replaced with plausible alternatives sampled from the BEiT output distribution. Given this corrupted image, an enhancer network learns to either recover all the original image pixels, or predict whether each visual token is replaced by a generator sample or not. The generator and the enhancer are simultaneously trained and synergistically updated. After pre-training, the enhancer can be used as a high-capacity visual encoder for downstream tasks. CIM is a general and flexible visual pre-training framework that is suitable for various network architectures. For the first time, CIM demonstrates that both ViT and CNN can learn rich visual representations using a unified, non-Siamese framework. Experimental results show that our approach achieves compelling results in vision benchmarks, such as ImageNet classification and ADE20K semantic segmentation.",
    "prev": " capture and learn structural information about objects and scenes.The intermediate feature representations obtained from MIM are suitable for fine-tuning on downstream tasks.In this paper, we propose an Image Modeling framework based on random orthogonal projection instead of binary masking as in MIM.Our proposed Random Orthogonal Projection Image Modeling (ROPIM) reduces spatially-wise token information under guaranteed bound on the noise variance and can be considered as masking entire spatial image area under locally varying masking degrees.Since ROPIM uses a random subspace for the projection that realizes the masking step, the readily available complement of the subspace can be used during unmasking to promote recovery of removed information.In this paper, we show that using random orthogonal projection leads to superior performance compared to crop-based masking.We demonstrate state-of-the-art results on several popular benchmarks.",
    "curr": "INTRODUCTION\n\nROPIM MAE (He et al., 2022) CIM (Fang et al., 2023) BEiT (Bao et al., 2022) MoCov3 (Chen et al., 2021b) MFM (Xie et.al., 2023) CAN (Mishra et al., 2022) GPL-MAE (Jiang et al., 2023) GPL-ROPIM Fast methods Best-performing Masked Image Modeling (MIM) (Bao et al., 2022;He et al., 2022;Xie et al., 2022) has achieved promising performance by pretraining backbones that are then fine-tuned on different downstream tasks such as image classification or semantic segmentation.",
    "next": "Most MIM techniques follow the general paradigm of self-prediction, i.e., they randomly mask out some regions in the input data and then learn to recover the missing data.Current MIM methods (Bao et al., 2022;He et al., 2022;Xie et al., 2022) mainly apply masking in the spatial domain by randomly excluding image patches.Since raw image pixels are highly correlated within their spatial neighbourhood, a high masking ratio (60%-75%) leads to high quality features (He et al., 2022;Xie et al., 2022).",
    "query": "Are there any research papers on self-supervised visual pre-training methods that use corrupted image modeling with generator-enhancer networks for feature learning?"
  },
  {
    "index": 933,
    "source_corpus_id": 252907467,
    "ref_id": "b29",
    "citation_corpus_id": 233296246,
    "start": 7882,
    "end": 7904,
    "title": "Consistent Accelerated Inference via Confident Adaptive Transformers",
    "abstract": "We develop a novel approach for confidently accelerating inference in the large and expensive multilayer Transformers that are now ubiquitous in natural language processing (NLP). Amortized or approximate computational methods increase efficiency, but can come with unpredictable performance costs. In this work, we present CATs-Confident Adaptive Transformers-in which we simultaneously increase computational efficiency, while guaranteeing a specifiable degree of consistency with the original model with high confidence. Our method trains additional prediction heads on top of intermediate layers, and dynamically decides when to stop allocating computational effort to each input using a meta consistency classifier. To calibrate our early prediction stopping rule, we formulate a unique extension of conformal prediction. We demonstrate the effectiveness of this approach on four classification and regression tasks. 1 * The first two authors contributed equally. 1 https://github.com/TalSchuster/CATs",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on methods for accelerating inference in Transformers while ensuring consistency with the original model using adaptive computation and conformal prediction?"
  },
  {
    "index": 935,
    "source_corpus_id": 258865836,
    "ref_id": "b9",
    "citation_corpus_id": 52967399,
    "start": 2062,
    "end": 2083,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "* Equal contribution.",
    "curr": "INTRODUCTION\n\nThe goal of natural language processing (NLP) is to develop computational models that can understand and generate human language.By capturing the statistical patterns and structures of text-based natural language, language models can predict and generate coherent and meaningful sequences of words.Combined with the Transformer model architecture (Vaswani et al., 2017), large language models (LLMs) trained on web-scale amounts of text, with proportionate compute and size, have demonstrated remarkable success in NLP tasks (Devlin et al., 2019;Brown et al., 2020;Chowdhery et al., 2022;Zhang et al., 2022a;Scao et al., 2022;Zeng et al., 2023).However, transfering these abilities to spoken human language remains a challenging frontier.Spoken dialog systems remain a cascade of separately trained automatic speech recognition (ASR), natural language understanding (NLU) and generation (NLG), and text-to-speech (TTS) systems (Gorin et al., 1997;Jokinen & McTear, 2009), with LLMs now playing the role of a combined NLU and NLG system.However, such cascades introduce latency and additional mechanisms for propagating and rendering non-verbal cues like speaker identity and prosody.Recently, spoken language models (Lakhotia et al., 2021;Kharitonov et al., 2022) and other generative audio models (Dhariwal et al., 2020;Hawthorne et al., 2022;Borsos et al., 2022;Agostinelli et al., 2023) have emerged as a promising avenue for generative speech modeling.These works quantize audio representations (Hsu et al., 2021;Chung et al., 2021;Zeghidour",
    "next": null,
    "query": "Are there any studies that explore methods for integrating large language models with spoken language processing to address challenges in end-to-end speech understanding and generation?"
  },
  {
    "index": 936,
    "source_corpus_id": 252967732,
    "ref_id": "b9",
    "citation_corpus_id": 67855286,
    "start": 9462,
    "end": 9481,
    "title": "ANTISYMMETRICRNN: A DYNAMICAL SYSTEM VIEW ON RECURRENT NEURAL NETWORKS",
    "abstract": "Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing gradients. In this paper, we draw connections between recurrent networks and ordinary differential equations. A special form of recurrent networks called the AntisymmetricRNN is proposed under this theoretical framework, which is able to capture long-term dependencies thanks to the stability property of its underlying differential equation. Existing approaches to improving RNN trainability often incur significant computation overhead. In comparison, AntisymmetricRNN achieves the same goal by design. We showcase the advantage of this new architecture through extensive simulations and experiments. AntisymmetricRNN exhibits much more predictable dynamics. It outperforms regular LSTM models on tasks requiring long-term memory and matches the performance on tasks where short-term dependencies dominate despite being much simpler. advocate going beyond initialization and forcing the weight matrices to be orthogonal throughout the entire learning process. However, some of these approaches come with significant computational overhead and reportedly hinder representation power of these models(Vorontsov et al., 2017). Moreover, orthogonal weight matrices alone do not prevent exploding and vanishing gradients, due to the nonlinear nature of deep neural networks as shown in(Pennington et al., 2017).Here we offer a new perspective on the trainability of RNNs from the dynamical system viewpoint. While exploding gradient is a manifestation of the instability of the underlying dynamical system, vanishing gradient results from a lossy system, properties that have been widely studied in the dynamical system literature(Haber & Ruthotto, 2017;Laurent & von Brecht, 2017). The main contributions of the work are:• We draw connections between RNNs and the ordinary differential equation theory and design new recurrent architectures by discretizing ODEs.",
    "prev": "In this way, the time variable is discretized and the ODE solution is computed by the successive application of an iterated map that operates on the discrete set of points between 0 and T , with a step size > 0.",
    "curr": "Crucially, as already observed for feed-forward and recurrent neural models (Haber & Ruthotto, 2017;Chang et al., 2019), each step of the ODE discretization process can be equated to one layer of a DGN network.",
    "next": "The whole neural architecture contains as many layers as the integration steps in the numerical method (i.e., L = T / ), and each layer = 1, ..., L computes nodes' states x u which approximates x u ( ).",
    "query": "Are there any studies that explore the connection between recurrent neural networks and ordinary differential equations through discretization methods, such as those used in the AntisymmetricRNN architecture for improving long-term dependency modeling?"
  },
  {
    "index": 937,
    "source_corpus_id": 250144675,
    "ref_id": "b2",
    "citation_corpus_id": 208637407,
    "start": 2711,
    "end": 2713,
    "title": "Published as a conference paper at ICLR 2020 AUGMIX: A SIMPLE DATA PROCESSING METHOD TO IMPROVE ROBUSTNESS AND UNCERTAINTY",
    "abstract": "Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AUGMIX, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AUGMIX significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.",
    "prev": "Introduction\n\nData augmentation is a crucial component of machine learning tasks as it prevents overfitting caused by a lack of training data and improves task performance without additional inference costs.",
    "curr": "Many data augmentation methods have been proposed across a broad range of research fields, including image recognition [1,2,3,4,5,6,7,8], image processing [9,10], language processing [11,12,13,14], and speech recognition [15,16].",
    "next": "In image recognition, each augmentation algorithm has become an essential component of the modern training recipe through various combinations [17,18,19,20].",
    "query": "Are there any research papers on data augmentation methods that improve robustness and uncertainty estimation in image classification?"
  },
  {
    "index": 941,
    "source_corpus_id": 257102638,
    "ref_id": "b21",
    "citation_corpus_id": 52901777,
    "start": 4009,
    "end": 4031,
    "title": "Bias-Reduced Uncertainty Estimation for Deep Neural Classifiers",
    "abstract": "We consider the problem of uncertainty estimation in the context of (non-Bayesian) deep neural classification. In this context, all known methods are based on extracting uncertainty signals from a trained network optimized to solve the classification problem at hand. We demonstrate that such techniques tend to introduce biased estimates for instances whose predictions are supposed to be highly confident. We argue that this deficiency is an artifact of the dynamics of training with SGD-like optimizers, and it has some properties similar to overfitting. Based on this observation, we develop an uncertainty estimation algorithm that selectively estimates the uncertainty of highly confident points, using earlier snapshots of the trained model, before their estimates are jittered (and way before they are ready for actual classification). We present extensive experiments indicating that the proposed algorithm provides uncertainty estimates that are consistently better than all known methods.",
    "prev": "A subset of ViT models is superior to all other models for all aspects of uncertainty estimation (\"ViT\" in the legend, marked as a red triangle facing upwards); the performance of EfficientNet-V2 and GENet models is worse.",
    "curr": "Among the various metrics proposed for evaluating the performance of uncertainty estimation are: Area Under the Receiver Operating Characteristic (AUROC or AUC), Area Under the Risk-Coverage curve (AURC) (Geifman et al., 2018), selective risk or coverage for a selective accuracy constraint (SAC), Negative Log-likelihood (NLL), Expected Calibration Error (ECE), which is often used for evaluating a model's calibration (see Section 2) and Brier score (Brier, 1950).",
    "next": "All these metrics are well known and are often used for comparing the uncertainty estimation performance of models (Moon et al., 2020;Nado et al., 2021;Maddox et al., 2019;Lakshminarayanan et al., 2017).",
    "query": "Are there any studies that explore bias-reduced uncertainty estimation methods for deep neural classifiers, particularly those addressing overfitting-like artifacts in confidence estimates?"
  },
  {
    "index": 943,
    "source_corpus_id": 14911774,
    "ref_id": "b14",
    "citation_corpus_id": 6308361,
    "start": 25501,
    "end": 25526,
    "title": "ReferItGame: Referring to Objects in Photographs of Natural Scenes",
    "abstract": "In this paper we introduce a new game to crowd-source natural language referring expressions. By designing a two player game, we can both collect and verify referring expressions directly within the game. To date, the game has produced a dataset containing 130,525 expressions, referring to 96,654 distinct objects, in 19,894 photographs of natural scenes. This dataset is larger and more varied than previous REG datasets and allows us to study referring expressions in real-world scenes. We provide an in depth analysis of the resulting dataset. Based on our findings, we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets.",
    "prev": "However, a very interesting effect of supervision is that it improves the interpretability of the code even when agents must communicate about images that do not contain objects in the supervised category set.",
    "curr": "This emerged in a follow-up experiment in which, during training, the sender was again exposed (with equal probability) to the same supervised classification task as above, but now the agents played the referential game on a different dataset of images derived from ReferItGame (Kazemzadeh et al., 2014).",
    "next": "In its general format, the ReferItGame contains annotations of bounding boxes in real images with referring expressions produced by humans when playing the game.",
    "query": "Are there any research papers on methods for generating referring expressions in natural scenes using the ReferItGame dataset and crowd-sourced annotations?"
  },
  {
    "index": 944,
    "source_corpus_id": 228063969,
    "ref_id": "b11",
    "citation_corpus_id": 53477919,
    "start": 8724,
    "end": 8741,
    "title": "Optimal Control Via Neural Networks: A Convex Approach",
    "abstract": "Control of complex systems involves both system identification and controller design.Deep neural networks have proven to be successful in many identification tasks, such as classification, prediction, and end-to-end system modeling.However, from the controller design perspective, these networks are difficult to work with because they are typically nonlinear and nonconvex.Therefore many systems are still optimized and controlled based on simple linear models despite their poor identification performance.In this paper we address this problem by explicitly constructing deep neural networks that are convex with respect to their inputs.We show that these input convex networks can be trained to obtain accurate models of complex physical systems.In particular, we design input convex recurrent neural networks to capture temporal behavior of dynamical systems.Then optimal controllers based on these networks can be designed by solving convex optimization problems.Results on both toy models and real-world image denoising and building energy optimization problems demonstrate the modeling accuracy and control efficiency of the proposed approach.",
    "prev": "This is indeed quite different from the setting considered in (Pilanci & Ergen, 2020) and demands a different treatment.",
    "curr": "It could also be useful to mention that there are works in (Amos et al., 2017;Chen et al., 2019) that customize the network architecture for convex inference, but they still require non-convex training.",
    "next": "In recent years, deep learning has been widely deployed in inverse problems to either learn effective priors for iterative algorithms (Bora et al., 2017;Heckel & Hand, 2018), or to directly learn the inversion map using feed-forward networks (Jin et al., 2017;Zhang et al., 2017).",
    "query": "Are there any research papers on convex neural networks for optimal control, focusing on system identification and controller design with convex optimization techniques?"
  },
  {
    "index": 946,
    "source_corpus_id": 259075723,
    "ref_id": "b6",
    "citation_corpus_id": 222378211,
    "start": 4845,
    "end": 4847,
    "title": "R-GAP: RECURSIVE GRADIENT ATTACK ON PRIVACY A PREPRINT",
    "abstract": "Federated learning frameworks have been regarded as a promising approach to break the dilemma between demands on privacy and the promise of learning from large collections of distributed data. Many such frameworks only ask collaborators to share their local update of a common model, i.e. gradients with respect to locally stored data, instead of exposing their raw data to other collaborators. However, recent optimization-based gradient attacks show that raw data can often be accurately recovered from gradients. It has been shown that minimizing the Euclidean distance between true gradients and those calculated from estimated data is often effective in fully recovering private data. However, there is a fundamental lack of theoretical understanding of how and when gradients can lead to unique recovery of original data. Our research fills this gap by providing a closed-form recursive procedure to recover data from gradients in deep neural networks. We demonstrate that gradient attacks consist of recursively solving a sequence of systems of linear equations. Furthermore, our closed-form approach works as well as or even better than optimization-based approaches at a fraction of the computation, we name it Recursive Gradient Attack on Privacy (R-GAP). Additionally, we propose a rank analysis method, which can be used to estimate a network architecture's risk of a gradient attack. Experimental results demonstrate the validity of the closed-form attack and rank analysis, while demonstrating its superior computational properties and lack of susceptibility to local optima vis a vis optimization-based attacks. Source code is available for download from https://github.com/JunyiZhu-AI/R-GAP.",
    "prev": "In FL, a server trains a shared model by applying aggregated gradient updates, received from a large number of clients.",
    "curr": "Gradient leakage attacks A long line of work [5,6,7,8,9,10], primarily on images, has shown that even passive servers can reconstruct client data from gradients, breaking the key privacy promise of FL.",
    "next": "However, these attacks are only applicable to naive FL deployments [11]-in real-life settings with no unrealistic assumptions, they are limited to small batch sizes with no secure aggregation [12].",
    "query": "Are there any research papers on gradient-based attacks in federated learning and methods for recovering private data from aggregated gradients?"
  },
  {
    "index": 949,
    "source_corpus_id": 53216818,
    "ref_id": "b21",
    "citation_corpus_id": 997870,
    "start": 21913,
    "end": 21917,
    "title": "DIVIDE-AND-CONQUER REINFORCEMENT LEARNING",
    "abstract": "Standard model-free deep reinforcement learning (RL) algorithms sample a new initial state for each trial, allowing them to optimize policies that can perform well even in highly stochastic environments. However, problems that exhibit considerable initial state variation typically produce high-variance gradient estimates for model-free RL, making direct policy or value function optimization challenging. In this paper, we develop a novel algorithm that instead optimizes an ensemble of policies, each on a different \"slice\" of the initial state space, and gradually unifies them into a single policy that can succeed on the whole state space. This approach, which we term divide-and-conquer RL, is able to solve complex tasks where conventional deep RL methods are ineffective. Our results show that divide-and-conquer RL greatly outperforms conventional policy gradient methods on challenging grasping, manipulation, and locomotion tasks, and exceeds the performance of a variety of prior methods. Videos of policies learned by our algorithm can be viewed here.",
    "prev": "A large number of time steps are not needed to manipulate the object, and a strong signal about progress is readily received.",
    "curr": "However, since the targets can change rapidly, the variance in gradient estimates can be very high for function approximation methods [22].",
    "next": "Trajectory optimization is particularly well suited for such types of problems, since it can efficiently compute near-optimal actions conditioned on the instance, facilitating function approximation.",
    "query": "Are there any research papers on methods for reducing gradient variance in reinforcement learning when dealing with dynamic environments and rapidly changing targets using divide-and-conquer strategies?"
  },
  {
    "index": 951,
    "source_corpus_id": 238857090,
    "ref_id": "b21",
    "citation_corpus_id": 204960716,
    "start": 37833,
    "end": 37852,
    "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance. 1",
    "prev": "For direct comparison with existing methods that do not take direct advantage of this assumption, we trained a new Language Processing module that does not make use of templates but makes use of the subtasks sequences annotations ALFRED provides.",
    "curr": "5 Fine-tuning a pre-trained BART (Lewis et al., 2020) model, we directly learned a mapping from a high-level instruction to a sequence of subtasks (e.g.",
    "next": "\"Drop a clean pan on the table\" → \"(PickupObject, Pan), (PutObject, Sink), ...\").",
    "query": "Are there any research papers on using BART for fine-tuning models that map high-level instructions to sequences of subtasks in natural language processing tasks?"
  },
  {
    "index": 953,
    "source_corpus_id": 219558836,
    "ref_id": "b4",
    "citation_corpus_id": 52967399,
    "start": 2234,
    "end": 2255,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Under review.",
    "curr": "Introduction\n\nPre-trained transformer-based masked language models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and ALBERT (Lan et al., 2020) have had a dramatic impact on the NLP landscape in the recent year.",
    "next": "The standard recipe of using such models typically involves training a pre-trained model for few epochs on a supervised downstream dataset.",
    "query": "Are there any research papers on pre-trained transformer-based language models and their applications in natural language processing tasks?"
  },
  {
    "index": 961,
    "source_corpus_id": 256389917,
    "ref_id": "b14",
    "citation_corpus_id": 209977508,
    "start": 1969,
    "end": 1990,
    "title": "META-Q-LEARNING",
    "abstract": "This paper introduces Meta-Q-Learning (MQL), a new off-policy algorithm for meta-Reinforcement Learning (meta-RL). MQL builds upon three simple ideas. First, we show that Q-learning is competitive with state-of-the-art meta-RL algorithms if given access to a context variable that is a representation of the past trajectory. Second, a multi-task objective to maximize the average reward across the training tasks is an effective method to meta-train RL policies. Third, past data from the meta-training replay buffer can be recycled to adapt the policy on a new task using off-policy updates. MQL draws upon ideas in propensity estimation to do so and thereby amplifies the amount of available data for adaptation. Experiments on standard continuous-control benchmarks suggest that MQL compares favorably with the state of the art in meta-RL.",
    "prev": "re but also data that evolved in a similar fashion in the past.The time-varying propensity score is quite general: we demonstrate different ways of implementing it and evaluate it on a variety of problems ranging from supervised learning (e.g., image classification problems) where data undergoes a sequence of gradual shifts, to reinforcement learning tasks (e.g., robotic manipulation and continuous control) where data shifts as the policy or the task changes.",
    "curr": "INTRODUCTION\n\nMachine learning models are not expected to perform well when the test data is from a different distribution than the training data.There are many techniques to mitigate the consequent deterioration in performance (Heckman, 1979;Shimodaira, 2000;Huang et al., 2006;Bickel et al., 2007;Sugiyama et al., 2007b;2008;Gretton et al., 2008).These techniques use the propensity score between the train and test data distributions to reweigh the data-and they work well (Agarwal et al., 2011;Wen et al., 2014;Reddi et al., 2015b;Fakoor et al., 2020c;a;Tibshirani et al., 2019) when dealing with a single training and test dataset.But when machine learning models are deployed for real-world problems, they do not just undergo one distribution shift1 (from train to test), but instead suffer many successive distribution shifts (Lu et al., 2019) e.g., search queries to an online retailer or a movie recommendation service evolve as fashion and tastes of the population evolve, etc.Even for problems in healthcare, there are many situations where data drifts gradually, e.g., the different parts of the brain atrophy slowly as the brain ages for both healthy subjects and those with dementia; tracking these changes and distinguishing between them is very useful for staging the disease and deciding treatments (Saito et al., 2022;Wang et al., 2023).Viruses can mutate and drift over time, which can make them resistant to existing treatments (Russell, 2016;Callaway, 2020;Harvey et al., 2021).",
    "next": "To build a method that can account for changes in data,",
    "query": "Are there any research papers on methods for handling multiple successive distribution shifts in machine learning using propensity estimation or similar techniques, particularly in meta-reinforcement learning contexts?"
  },
  {
    "index": 962,
    "source_corpus_id": 199528271,
    "ref_id": "b9",
    "citation_corpus_id": 52967399,
    "start": 3151,
    "end": 3172,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "For example, when training typical Transformers based neural machine translation models on the De-En IWSLT'14 dataset, removing the warmup stage increases the training loss from 3 to around 10, as shown in Figure 1.",
    "curr": "Similar phenomena are observed in other scenarios like BERT (a bidirectional transformer language model) pre-training (Devlin et al., 2019).",
    "next": "Due to the lack of the theoretical underpinnings, there is neither guarantee that warmup would bring consistent improvements for various machine learning settings nor guidance on how we should In this paper, we conduct both empirical and theoretical analysis of the convergence issue to identify its origin.",
    "query": "Are there any studies that investigate the impact of warmup stages on training stability and convergence in transformer-based models for language understanding, particularly in pre-training scenarios like BERT?"
  },
  {
    "index": 963,
    "source_corpus_id": 238353966,
    "ref_id": "b0",
    "citation_corpus_id": 49310753,
    "start": 36797,
    "end": 36823,
    "title": "Published as a conference paper at ICLR 2018 MAXIMUM A POSTERIORI POLICY OPTIMISATION",
    "abstract": "We introduce a new algorithm for reinforcement learning called Maximum aposteriori Policy Optimisation (MPO) based on coordinate ascent on a relativeentropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings. , et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.",
    "prev": "Batch normalization and its variant are introduced in deep deterministic policy gradient (DDPG) (Lillicrap et al., 2015) and twin delayed DDPG (Fujimoto et al., 2018) (Bhatt et al., 2020).",
    "curr": "Layer normalization is introduced in the implementation of maximum a posteriori policy optimisation(Abdolmaleki et al., 2018; Hoffman et al.,  2020).",
    "next": "It is also introduced in SAC extensions(Ma et al., 2020; Zhang et al., 2021)\n\nFigure 4 :Figure 5 :\n45Average return and average/standard deviation of estimation bias for DroQ with different dropout rates.",
    "query": "Are there any research papers on the application of normalization techniques such as layer normalization in reinforcement learning algorithms like Maximum a Posteriori Policy Optimisation (MPO) and Soft Actor-Critic (SAC) extensions?"
  },
  {
    "index": 965,
    "source_corpus_id": 202539918,
    "ref_id": "b28",
    "citation_corpus_id": 14091946,
    "start": 1524,
    "end": 1544,
    "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies",
    "abstract": "The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any studies that investigate the effectiveness of LSTMs in capturing syntax-sensitive grammatical dependencies and the impact of different training objectives on their ability to model such structures?"
  },
  {
    "index": 967,
    "source_corpus_id": 263608898,
    "ref_id": "b19",
    "citation_corpus_id": 233296292,
    "start": 6518,
    "end": 6536,
    "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
    "abstract": "This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using \"entailment\" pairs as positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show-both theoretically and empirically-that contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available. 1",
    "prev": "(2020); Kumar et al.",
    "curr": "(2022) for CL.In this paper, we use SimCLR (Chen et al., 2020) and SimSiam (Chen & He, 2021) for CIFAR10 dataset and SimCSE (Gao et al., 2021) for BIASBIOS dataset to learn CL representations.",
    "next": "Several works have theoretically studied the success of self-supervised learning (Arora et al., 2019;HaoChen et al., 2021;Lee et al., 2020;Tian et al., 2021;Tosh et al., 2021).Our theoretical analysis of CL loss is partly motivated by Fang et al.",
    "query": "Are there any research papers on contrastive learning methods for improving sentence embeddings, such as SimCSE, and their theoretical and empirical analysis in both unsupervised and supervised settings?"
  },
  {
    "index": 968,
    "source_corpus_id": 259076379,
    "ref_id": "b4",
    "citation_corpus_id": 52967399,
    "start": 2249,
    "end": 2256,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Specifically, we provide an efficient GPU inference algorithm for SpQR which yields faster inference than 16-bit baselines at similar accuracy, while enabling memory compression gains of more than 4x.",
    "curr": "Introduction\n\nPretrained large language models (LLMs) improved rapidly from task-specific performance [WSM + 18, DCLT19,RWC + 19], to performing well on general tasks if prompted with instructions [BMR + 20,WBZ + 21,Ope23].",
    "next": "While the improved performance can be attributed to scaling in training data and parameters [KMH + 20,CND + 22] recent trends focused on smaller models trained on more data, that are easier to use at inference time [HBM + 22, BSA + 23, TLI + 23].",
    "query": "Are there any research papers on the pretraining of deep bidirectional transformers for language understanding and their impact on natural language processing task performance?"
  },
  {
    "index": 970,
    "source_corpus_id": 227161986,
    "ref_id": "b36",
    "citation_corpus_id": 210064473,
    "start": 18945,
    "end": 18963,
    "title": "SPACE: UNSUPERVISED OBJECT-ORIENTED SCENE REPRESENTATION VIA SPATIAL ATTENTION AND DECOMPOSITION",
    "abstract": "The ability to decompose complex multi-object scenes into meaningful abstractions like objects is fundamental to achieve higher-level cognition. Previous approaches for unsupervised object-oriented scene representation learning are either based on spatial-attention or scene-mixture approaches and limited in scalability which is a main obstacle towards modeling real-world scenes. In this paper, we propose a generative latent variable model, called SPACE, that provides a unified probabilistic modeling framework that combines the best of spatial-attention and scene-mixture approaches. SPACE can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. Previous models are good at either of these, but not both. SPACE also resolves the scalability problems of previous methods by incorporating parallel spatial-attention and thus is applicable to scenes with a large number of objects without performance degradations. We show through experiments on Atari and 3D-Rooms that SPACE achieves the above properties consistently in comparison to SPAIR, IODINE, and GENESIS. Results of our experiments can be found on our project website: https://sites.google.com/view/space-project-page * Visiting Student at Rutgers University. Authors named inside {} equally contributed. Correspondance to",
    "prev": "An alternative to inferring object keypoints (and corresponding representations) is to learn entire object representations.",
    "curr": "Recent approaches can be broadly categorized into spatial mixture models (Greff et al., 2017;, sequential attention models (Eslami et al., 2016;Kosiorek et al., 2018) or hybrid models that are combinations of both (Lin et al., 2020).",
    "next": "While these methods have shown promising results, they have yet to scale to more complex visual settings.",
    "query": "Are there any studies on unsupervised methods for decomposing complex scenes into object representations using spatial attention and decomposition, with a focus on scalability in visual settings?"
  },
  {
    "index": 972,
    "source_corpus_id": 208006294,
    "ref_id": "b2",
    "citation_corpus_id": 11212020,
    "start": 2671,
    "end": 2694,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": " and metrics evaluation on both LSTM models and BERT Transformer models on multiple datasets show that our algorithms outperform prior hierarchical explanation algorithms.Our algorithms apply to hierarchical visualization of compositional semantics, extraction of classification rules and improving human trust of models.",
    "curr": "Introduction\n\nRecent advances in deep neural networks have led to impressive results on a range of natural language processing (NLP) tasks, by learning latent, compositional vector representations of text data (Peters et al., 2018;Devlin et al., 2018;Liu et al., 2019b).However, interpretability of the predictions given by these complex, \"black box\" models has always been a limiting factor for use cases that require explanations of the features involved in modeling (e.g., words and phrases) (Guidotti et al., 2018;Ribeiro et al., 2016).Prior efforts on enhancing model interpretability have focused on either constructing models with intrinsically interpretable structures (Bahdanau et al., 2015;Liu et al., 2019a), or developing post-hoc explanation algorithms which can explain model predictions without elucidating the mechanisms by which model works (Mohseni et al., 2018;Guidotti et al., 2018).Among these work, post-hoc explanation has come to the fore as they can operate over a variety of trained models while not affecting predictive performance of models.",
    "next": "Towards post-hoc explanation, a major line of work, additive feature attribution methods (Lundberg and Lee, 2017;Ribeiro et al., 2016;Binder et al., 2016;Shrikumar et al., 2017), explain a model prediction by assigning importance scores to individual input variables.However, these methods may not work for explaining compositional semantics in natural language (e.g., phrases or clauses), as the importance of a phrase often is non-linear combination of the importance of the words in the phrase.Contextual decomposition (CD) (Murdoch et al., 2018) and its hierarchical extension (Singh et al., 2019) go beyond the additive assu",
    "query": "Are there any research papers on post-hoc explanation methods for improving model interpretability in natural language processing, particularly those addressing compositional semantics through hierarchical decomposition techniques?"
  },
  {
    "index": 973,
    "source_corpus_id": 251252927,
    "ref_id": "b17",
    "citation_corpus_id": 3144218,
    "start": 2452,
    "end": 2456,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "A variety of applications characterized by this type of graph-structured data include works in the areas of social analysis [5], recommendation systems [56], computer vision [42], study of the properties of chemical compounds [26,48], statistical physics [8,10], and financial forensics [51,57].",
    "curr": "The most popular learning models for relational data use graph convolutions [33], where the idea is to aggregate the attributes of the set of neighbours of a node instead of only utilizing its own attributes.",
    "next": "Despite several empirical studies of various GCN-type models [13,38] that demonstrate that graph convolutions can improve the performance of traditional classification methods, such as a multi-layer perceptron (MLP), there has been limited progress in the theoretical understanding of the benefits of graph convolutions in multi-layer networks in terms of improving node classification tasks.",
    "query": "Are there any research papers on semi-supervised classification using graph convolutional networks and their theoretical benefits in node classification tasks?"
  },
  {
    "index": 974,
    "source_corpus_id": 3475375,
    "ref_id": "b0",
    "citation_corpus_id": 18828233,
    "start": 6604,
    "end": 6630,
    "title": "TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.",
    "prev": "(2012); Li et al.",
    "curr": "(2015) or the Wasserstein metric Arjovsky and Bottou (2017).",
    "next": "All of these divergence measures yield a minimax objective.",
    "query": "Are there any studies that explore principled methods for training generative adversarial networks, addressing instability and saturation issues, and utilizing divergence measures or the Wasserstein metric?"
  },
  {
    "index": 976,
    "source_corpus_id": 256194054,
    "ref_id": "b52",
    "citation_corpus_id": 3162051,
    "start": 12877,
    "end": 12897,
    "title": "mixup: BEYOND EMPIRICAL RISK MINIMIZATION",
    "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks. * Alphabetical order.",
    "prev": "In this case, the optimization objective for the student network is a weighted combination of dual losses: a standard supervised cross-entropy loss and a \"distribution-matching\" objective that aims to mimic the teacher's output.",
    "curr": "However, in our case there are no class-labels for the patches generated from a single image, so we solely use the second objective formulated as a Kullback-Leibler (KL) divergence between the student output p s and the teacher's output p t :\nL KL = c ∈ C −p t c log p s c + p t c log p t c(1)\nwhere c are the teachers' classes and the outputs of both student and teacher are temperature τ flattened probabilities, p = softmax(l/τ ), that are generated from logits l.\n\nFor training, we follow (Beyer et al., 2022) in employing a function matching strategy, where the teacher and student models are fed consistently augmented instances, that include heavy augmentations, such as MixUp (Zhang et al., 2018) or CutMix .",
    "next": "However, in contrast to (Beyer et al., 2022), we neither have access to TPUs nor can train 10K epochs on ImageNet-sized datasets.",
    "query": "Are there any research papers on the use of mixup for improving generalization and robustness in neural networks through convex combinations of examples and labels?"
  },
  {
    "index": 977,
    "source_corpus_id": 249431433,
    "ref_id": "b13",
    "citation_corpus_id": 14124313,
    "start": 2098,
    "end": 2126,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "INTRODUCTION\n\nThe success of deep learning in computer vision is largely driven by Convolutional Neural Networks (CNNs).",
    "curr": "Starting from the milestone work AlexNet (Krizhevsky et al., 2012), CNNs keep pushing the frontier of computer vision (Simonyan & Zisserman, 2015;He et al., 2016;Tan & Le, 2019).",
    "next": "Interestingly, the recently emerged Vision Transformer (ViT) (Dosovitskiy et al., 2020) challenges the leading position of CNNs.",
    "query": "Are there any research papers on the impact of deep convolutional neural networks on large-scale image recognition tasks?"
  },
  {
    "index": 978,
    "source_corpus_id": 261823404,
    "ref_id": "b14",
    "citation_corpus_id": 225039882,
    "start": 7252,
    "end": 7279,
    "title": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.RELATED WORKTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.",
    "prev": "RELATED WORK\n\nIn this section, we introduce the recent related improvements in 3D generative models including GAN-based and diffusion-based methods, as well as transformer structure.",
    "curr": "TRANSFORMER\n\nIn recent years, Transformer (Vaswani et al., 2017) has seen rapid progress in many fields including image recognition (Dosovitskiy et al., 2020a;Touvron et al., 2021), object detection (Carion et al., 2020;Zhu et al., 2020), tracking (Cao et al., 2021;2022;2023), segmentation (Zheng et al., 2021;Strudel et al., 2021), and image generation (Van den Oord et al., 2016;Jiang et al., 2021;Mo et al., 2023).Some works (Chen et al., 2020;Child et al., 2019) prove the remarkable of transformer when predicting the pixels autoregressively.Based on the masked token, MaskGIT.",
    "next": "(Chang et al., 2022) achieve promising generation performance.DiT (Peebles & Xie, 2022) adopts the transformer as the backbone of diffusion models of images.Based on the 2D version, Mo et al.",
    "query": "Are there any research papers on the application of transformer models for image recognition tasks using large-scale pre-training and image patch processing?"
  },
  {
    "index": 981,
    "source_corpus_id": 258887825,
    "ref_id": "b23",
    "citation_corpus_id": 3488815,
    "start": 1233,
    "end": 1237,
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. * Authors ordered alphabetically.",
    "prev": "Under review.",
    "curr": "Introduction\n\nMany noteworthy modern machine learning problems, such as generative adversarial networks (GANs) [13], adversarial training [24], and sharpness-aware minimization (SAM) [10], are instances of minimax problems, formulated as min x max y f (x, y).",
    "next": "First-order methods, such as gradient descent ascent (GDA) [1] and extragradient (EG) [20], are workhorses of minimax optimization in modern machine learning, but they still remain remarkably unreliable.",
    "query": "Are there any research papers on methods for improving adversarial robustness in deep learning using minimax optimization techniques such as gradient descent ascent and extragradient?"
  },
  {
    "index": 982,
    "source_corpus_id": 263829263,
    "ref_id": "b10",
    "citation_corpus_id": 189928186,
    "start": 2333,
    "end": 2337,
    "title": "Theoretical Limitations of Self-Attention in Neural Sequence Models",
    "abstract": "Transformers are emerging as the new workhorse of NLP, showing great success across tasks. Unlike LSTMs, transformers process input sequences entirely through self-attention. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited. In this work, we mathematically investigate the computational power of self-attention to model formal languages. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of selfattention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.156",
    "prev": "it complexity class TC 0 , i.e., AC 0 -circuits extended by majority gates.We first show a negative result that there is an AC 0 -language that cannot be recognized by an UHAT encoder.On the positive side, we show that UHAT encoders can recognize a rich fragment of AC 0 -languages, namely, all languages definable in first-order logic with arbitrary unary numerical predicates.This logic, includes, for example, all regular languages from AC 0 .We then show that AHAT encoders can recognize all languages of our logic even when we enrich it with counting terms.We apply these results to derive new results on the expressive power of UHAT and AHAT up to permutation of letters (a.k.a.Parikh images).",
    "curr": "Introduction\n\nTransformers have revolutionized natural language processing by facilitating the efficient and effective modeling of intricate contextual relationships within text [19].This remarkable capability has sparked numerous investigations into the potential boundaries of transformers' power [11,22,17,21,12,6,5,7].One natural method for addressing this question is to explore the classes of formal languages that these architectures can recognize.This approach provides an insight into their strengths and limitations.The response to this question naturally relies on the specific features allowed within transformer encoders.These encompass the interplay between encoders and decoders, the kind of functions used for positional encodings and attention mechanisms, and considerations of fixed or unbounded precision, among other factors.",
    "next": "While the capacity of transformers that incorporate both encoders and decoders to recognize languages is well understood today (indeed, such architectures are Turing-complete and can thus recognize any computable language [17]), the expressive power of transformer encoders has not been fully elucidated to date.Unique Hard Attention Transformers (UHAT) are a class of transformer encoders that has been a subject of many recent papers.As was shown by [12]",
    "query": "Are there any research papers on the theoretical limitations of self-attention mechanisms in transformers regarding their ability to model formal languages and hierarchical structures?"
  },
  {
    "index": 983,
    "source_corpus_id": 3568073,
    "ref_id": "b23",
    "citation_corpus_id": 6628106,
    "start": 28804,
    "end": 28823,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "The upsampling and downsampling operations in Table 2 correspond to 2 × 2 element replication and average pooling, respectively.",
    "curr": "We train the networks using Adam (Kingma & Ba, 2015) with α = 0.001, β 1 = 0, β 2 = 0.99, and = 10 −8 .",
    "next": "We do not use any learning rate decay or rampdown, but for visualizing generator output at any given point during the training, we use an exponential running average for the weights of the generator with decay 0.999.",
    "query": "Are there any research papers on the Adam optimization algorithm for stochastic gradient-based training of neural networks?"
  },
  {
    "index": 985,
    "source_corpus_id": 212414722,
    "ref_id": "b15",
    "citation_corpus_id": 210064473,
    "start": 56249,
    "end": 56267,
    "title": "SPACE: UNSUPERVISED OBJECT-ORIENTED SCENE REPRESENTATION VIA SPATIAL ATTENTION AND DECOMPOSITION",
    "abstract": "The ability to decompose complex multi-object scenes into meaningful abstractions like objects is fundamental to achieve higher-level cognition. Previous approaches for unsupervised object-oriented scene representation learning are either based on spatial-attention or scene-mixture approaches and limited in scalability which is a main obstacle towards modeling real-world scenes. In this paper, we propose a generative latent variable model, called SPACE, that provides a unified probabilistic modeling framework that combines the best of spatial-attention and scene-mixture approaches. SPACE can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. Previous models are good at either of these, but not both. SPACE also resolves the scalability problems of previous methods by incorporating parallel spatial-attention and thus is applicable to scenes with a large number of objects without performance degradations. We show through experiments on Atari and 3D-Rooms that SPACE achieves the above properties consistently in comparison to SPAIR, IODINE, and GENESIS. Results of our experiments can be found on our project website: https://sites.google.com/view/space-project-page * Visiting Student at Rutgers University. Authors named inside {} equally contributed. Correspondance to",
    "prev": "ng bilinear interpolation.This attention mechanism is motivated by the observation that only part of the image contains information for tracking an object and an inductive bias that objects cannot move a large distance within a short time span (i.e., objects do not teleport).",
    "curr": "Posterior Discovery.The posterior discovery also consists of proposal and rejection phases.The main difference is that we now compute the proposal in spatially-parallel manner by conditioning on the observations x ≤t , i.e., q(z D t |z P t , x ≤t ) = HW h,w=1 q(z D t,h,w |z P t , x ≤t ).Here, the observation x ≤t is encoded into the feature map of dimensionality H×W ×D using a Convolutional LSTM (Xingjian et al., 2015).Then, from each feature we obtain zD t,h,w .Importantly, this is done over all the feature cells h, w in parallel.A similar approach is used in SPAIR (Crawford & Pineau, 2019b), but it infers the object latent representations sequentially and thus is difficult to scale to a large number of objects (Lin et al., 2020).Even if this spatially-parallel proposal plays a key role in making our model scalable, we also observe another challenge due to this high capacity of the discovery module.The problem is that the discovery module tends to dominate the propagation module and thus most of the objects in an image are explained by the discovery module, i.e., objects are rediscovered at every time-step while nothing is propagated.We call this problem propagation collapse.",
    "next": "Why would the model tend to explain an image through discovery while suppressing propagation?First, the model does not care where-either from discovery or propagation-an object is sourced from as long as it can make an accurate reconstruction.Second, the propagation step performs a much harder task than the discovery.For the propagation to properly predict, tracker n needs to learn to find the matching object from an image containing many objects.Although the propagation attention plays an important role in balancing the discovery and pr",
    "query": "Are there any research papers on unsupervised object-oriented scene decomposition using spatial attention and parallel processing to address scalability and propagation collapse challenges?"
  },
  {
    "index": 986,
    "source_corpus_id": 262054258,
    "ref_id": "b32",
    "citation_corpus_id": 253107613,
    "start": 3698,
    "end": 3718,
    "title": "IN-CONTEXT REINFORCEMENT LEARNING WITH ALGORITHM DISTILLATION",
    "abstract": "We propose Algorithm Distillation (AD), a method for distilling reinforcement learning (RL) algorithms into neural networks by modeling their training histories with a causal sequence model. Algorithm Distillation treats learning to reinforcement learn as an across-episode sequential prediction problem. A dataset of learning histories is generated by a source RL algorithm, and then a causal transformer is trained by autoregressively predicting actions given their preceding learning histories as context. Unlike sequential policy prediction architectures that distill post-learning or expert sequences, AD is able to improve its policy entirely in-context without updating its network parameters. We demonstrate that AD can reinforcement learn in-context in a variety of environments with sparse rewards, combinatorial task structure, and pixel-based observations, and find that AD learns a more data-efficient RL algorithm than the one that generated the source data. * Equal contribution. Corresponding authors: mlaskin, luyuwang, vminh@deepmind.com.",
    "prev": null,
    "curr": "nt years, large pre-trained Transformers (Vaswani et al., 2017), so-called foundation models (Bommasani et al., 2021), have proven to be highly successful across a wide range of predictive tasks (Bubeck et al., 2023;Rae et al., 2021) and are thus promising candidates for use with arithmetic coding.Indeed, Transformer-based compression with arithmetic coding has produced state-of-theart results both in the online (Bellard, 2021;Mao et al., 2022) and offline settings (Valmeekam et al., 2023).In the online setting, a pseudo-randomly initialized model is directly trained on the stream of data that is to be compressed, while the offline setting, which we consider in our work, trains the model on an external dataset before employing it to compress a (potentially different) data stream.Consequently, offline compression is performed in-context, with a fixed set of model parameters.Transformers have demonstrated impressive in-context learning abilities (Brown et al., 2020;Genewein et al., 2023;Laskin et al., 2023;Wei et al., 2022), which renders them ideally suited for offline compression.However, as we will discuss in this work, Transformers are actually trained to compress well, and therefore must have good in-context learning abilities.",
    "next": "I\n\nFigure 1 | Arithmetic encoding of the sequence ' AIXI' with a probabilistic (language) model  (both in blue) resulting in the binary code '0101001' (in green).Arithmetic coding compresses data by assigning unique intervals to symbols based on the probabilities assigned by .It progressively refines these intervals to output compressed bits, which represent the original message.To decode, arithmetic coding initializes an interval based on the received compressed bits.It iteratively matches intervals with symbols using the probabilities given by  to reconstruct the original message.",
    "query": "Are there any studies on in-context reinforcement learning using algorithm distillation with causal sequence models or Transformers?"
  },
  {
    "index": 987,
    "source_corpus_id": 209439843,
    "ref_id": "b12",
    "citation_corpus_id": 85504763,
    "start": 11848,
    "end": 11868,
    "title": "ANALYSING MATHEMATICAL REASONING ABILITIES OF NEURAL MODELS",
    "abstract": "Mathematical reasoning-a core ability within human intelligence-presents some unique challenges as a domain: we do not come to understand and solve mathematical problems primarily on the back of experience and evidence, but on the basis of inferring, learning, and exploiting laws, axioms, and symbol manipulation rules. In this paper, we present a new challenge for the evaluation (and eventually the design) of neural architectures and similar system, developing a task suite of mathematics problems involving sequential questions and answers in a free-form textual input/output format. The structured nature of the mathematics domain, covering arithmetic, algebra, probability and calculus, enables the construction of training and test splits designed to clearly illuminate the capabilities and failure-modes of different architectures, as well as evaluate their ability to compose and relate knowledge and learned processes. Having described the data generation process and its potential future expansions, we conduct a comprehensive analysis of models from two broad classes of the most powerful sequence-to-sequence architectures and find notable differences in their ability to resolve mathematical problems and generalize their knowledge.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any studies that assess the mathematical reasoning capabilities of neural models through structured evaluation tasks and sequence-to-sequence architectures?"
  },
  {
    "index": 988,
    "source_corpus_id": 53831933,
    "ref_id": "b2",
    "citation_corpus_id": 3921978,
    "start": 2285,
    "end": 2305,
    "title": "Emergent Complexity via Multi-Agent Competition",
    "abstract": "Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty. This work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX.",
    "prev": "Yet, even though significant progress has been made thanks to better algorithms, training regimes, and computational infrastructure, the resulting behaviors still tend to exhibit significant idiosyncrasies (e.g.",
    "curr": "Bansal et al., 2018).",
    "next": "One advantage of working with humanoids in this context is that motion capture data is widely available and can serve to help design controllers that produce apparently humanlike movement.",
    "query": "Are there any research papers on emergent complexity in multi-agent reinforcement learning through competitive environments and self-play training methods?"
  },
  {
    "index": 990,
    "source_corpus_id": 4862861,
    "ref_id": "b1",
    "citation_corpus_id": 9963298,
    "start": 5135,
    "end": 5162,
    "title": "Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions",
    "abstract": "The context in which language is used provides a strong signal for learning to recover its meaning. In this paper, we show it can be used within a grounded CCG semantic parsing approach that learns a joint model of meaning and context for interpreting and executing natural language instructions, using various types of weak supervision. The joint nature provides crucial benefits by allowing situated cues, such as the set of visible objects, to directly influence learning. It also enables algorithms that learn while executing instructions, for example by trying to replicate human actions. Experiments on a benchmark navigational dataset demonstrate strong performance under differing forms of supervision, including correctly executing 60% more instruction sets relative to the previous state of the art.",
    "prev": "Static datasets such as visual question answering (Antol et al., 2015) provide grounding into images, but no possibility for language learning through interaction.",
    "curr": "Some works utilize a geographical environment such as a maze but still employ static datasets (Artzi & Zettlemoyer, 2013).",
    "next": "It has been argued that virtual embodiment of agents is a viable long-term strategy for artificial intelligence research and the learning of natural language semantics, particularly in the form of games which also contain human players (Kiela et al., 2016).",
    "query": "Are there any research papers on weakly supervised methods for learning semantic parsers in interactive environments that incorporate contextual cues and joint models of meaning and action?"
  },
  {
    "index": 994,
    "source_corpus_id": 238744039,
    "ref_id": "b32",
    "citation_corpus_id": 44090489,
    "start": 5646,
    "end": 5671,
    "title": "On the Impact of Various Types of Noise on Neural Machine Translation",
    "abstract": "We examine how various types of noise in the parallel training data impact the quality of neural machine translation systems. We create five types of artificial noise and analyze how they degrade performance in neural and statistical machine translation. We find that neural models are generally more harmed by noise than statistical models. For one especially egregious type of noise they learn to just copy the input sentence.",
    "prev": "Although being highly effective in low-resource translation, backtranslation also has issues, as the model is trained on potentially invalid input-output pairs.",
    "curr": "Neural machine translation models being highly sensitive to input noise (Belinkov & Bisk, 2018;Khayrallah & Koehn, 2018), this can severely deteriorate the performance.",
    "next": "Fortunately, many programming languages come with relatively mature tools and technologies for automated test data generation.",
    "query": "Are there any research papers on the impact of different types of noise in training data on neural machine translation performance, particularly comparing neural and statistical models?"
  },
  {
    "index": 996,
    "source_corpus_id": 213392702,
    "ref_id": "b25",
    "citation_corpus_id": 21946795,
    "start": 5104,
    "end": 5124,
    "title": "ENSEMBLE ADVERSARIAL TRAINING: ATTACKS AND DEFENSES",
    "abstract": "Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model's loss. We show that this form of adversarial training converges to a degenerate global minimum, wherein small curvature artifacts near the data points obfuscate a linear approximation of the loss. The model thus learns to generate weak perturbations, rather than defend against strong ones. As a result, we find that adversarial training remains vulnerable to black-box attacks, where we transfer perturbations computed on undefended models, as well as to a powerful novel single-step attack that escapes the non-smooth vicinity of the input data via a small random step. We further introduce Ensemble Adversarial Training, a technique that augments training data with perturbations transferred from other models. On ImageNet, Ensemble Adversarial Training yields models with stronger robustness to blackbox attacks. In particular, our most robust model won the first round of the NIPS 2017 competition on Defenses against Adversarial Attacks (Kurakin et al., 2017c). However, subsequent work found that more elaborate black-box attacks could significantly enhance transferability and reduce the accuracy of our models.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on ensemble adversarial training techniques for improving model robustness against black-box attacks in machine learning?"
  },
  {
    "index": 997,
    "source_corpus_id": 219401642,
    "ref_id": "b7",
    "citation_corpus_id": 202749904,
    "start": 5510,
    "end": 5513,
    "title": "HIGH FIDELITY SPEECH SYNTHESIS WITH ADVERSARIAL NETWORKS",
    "abstract": "Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention, and autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech. Our architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced. To measure the performance of GAN-TTS, we employ both subjective human evaluation (MOS -Mean Opinion Score), as well as novel quantitative metrics (Fréchet DeepSpeech Distance and Kernel DeepSpeech Distance), which we find to be well correlated with MOS. We show that GAN-TTS is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to GAN-TTS reading this abstract at https://storage.googleapis.com/ deepmind-media/research/abstract.wav.",
    "prev": "It is illustrated in Figure 1.",
    "curr": "The generator is inspired by GAN-TTS [8], a text-to-speech generative adversarial network operating on aligned linguistic features.",
    "next": "We employ the GAN-TTS generator as the decoder in our model, but instead of upsampling pre-computed linguistic features, its input comes from the aligner block.",
    "query": "Are there any research papers on adversarial networks for high-fidelity text-to-speech synthesis with feed-forward architectures?"
  },
  {
    "index": 998,
    "source_corpus_id": 259137503,
    "ref_id": "b17",
    "citation_corpus_id": 225039882,
    "start": 1627,
    "end": 1631,
    "title": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.RELATED WORKTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.",
    "prev": "Code is available at https://github.com/NVlabs/FasterViT.",
    "curr": "Introduction\n\nVision Transformers (ViTs) [18] have recently become popular in computer vision and achieved superior performance in various applications such as image classification [38,17,35], object detection [77,21] and semantic segmentation [61,10].",
    "next": "In addition to learning more uniform local and global representations across their architecture when compared to Convolutional Neural Networks (CNNs), ViTs scale properly to large-scale data and model sizes [47,45].",
    "query": "Are there any research papers on vision transformers for image recognition tasks, their performance compared to convolutional neural networks, and the impact of large-scale pre-training on model accuracy?"
  },
  {
    "index": 1000,
    "source_corpus_id": 16299141,
    "ref_id": "b2",
    "citation_corpus_id": 11212020,
    "start": 17150,
    "end": 17173,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "Other work has improved language modeling performance by modifying the RNN architecture to better handle increased recurrence depth (Zilly et al., 2016).",
    "curr": "In order to increase capacity and minimize the impact of vanishing gradients, some language and translation mod- WikiText-2  WikiText-103  Train  Valid  Test  Train  Valid  Test  Train  Valid  Test   Articles  ---600  60  60  28,475  60  60  Tokens  929,590 73,761 82, els have also added a soft attention or memory component (Bahdanau et al., 2015;Sukhbaatar et al., 2015;Cheng et al., 2016;Kumar et al., 2016;Xiong et al., 2016;Ahn et al., 2016).",
    "next": "These mechanisms allow for the retrieval and use of relevant previous hidden states.",
    "query": "Are there any research papers on encoder-decoder models with attention mechanisms for neural machine translation, particularly those that jointly learn to align and translate?"
  },
  {
    "index": 1001,
    "source_corpus_id": 252735240,
    "ref_id": "b29",
    "citation_corpus_id": 240288835,
    "start": 7226,
    "end": 7244,
    "title": "MetaICL: Learning to Learn In Context",
    "abstract": "We introduce MetaICL (Meta-training for In-Context Learning), a new meta-training framework for few-shot learning where a pretrained language model is tuned to do in-context learning on a large set of training tasks. This metatraining enables the model to more effectively learn a new task in context at test time, by simply conditioning on a few training examples with no parameter updates or task-specific templates. We experiment on a large, diverse collection of tasks consisting of 142 NLP datasets including classification, question answering, natural language inference, paraphrase detection and more, across seven different metatraining/target splits. MetaICL outperforms a range of baselines including in-context learning without meta-training and multi-task learning followed by zero-shot transfer. We find that the gains are particularly significant for target tasks that have domain shifts from the meta-training tasks, and that using a diverse set of the meta-training tasks is key to improvements. We also show that MetaICL approaches (and sometimes beats) the performance of models fully finetuned on the target task, and outperforms much bigger models with nearly 8x parameters. Finally, we show that MetaICL is complementary to human-written instructions, and the best performance can be achieved by combining both approaches. Tom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21). .2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In EMNLP.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on meta-training frameworks for improving in-context learning in few-shot NLP tasks?"
  },
  {
    "index": 1002,
    "source_corpus_id": 252595881,
    "ref_id": "b3",
    "citation_corpus_id": 46899514,
    "start": 2583,
    "end": 2610,
    "title": "Generative Code Modeling with Graphs",
    "abstract": "Generative models for source code are an interesting structured prediction problem, requiring to reason about both hard syntactic and semantic constraints as well as about natural, likely programs. We present a novel model for this problem that uses a graph to represent the intermediate state of the generated output. The generative procedure interleaves grammar-driven expansion steps with graph augmentation and neural message passing steps. An experimental evaluation shows that our new model can generate semantically meaningful expressions, outperforming a range of strong baselines.Preprint. Work in progress.",
    "prev": "Diffusion models have been used successfully in a variety of settings, outperforming all other methods on image and video (Dhariwal & Nichol, 2021;Ho et al., 2022).",
    "curr": "These successes raise hope for building powerful models for graph generation, a task with diverse applications such as molecule design (Liu et al., 2018), traffic modeling (Yu & Gu, 2019), and code completion (Brockschmidt et al., 2019).",
    "next": "However, generating graphs remains challenging due to their unordered nature and sparsity properties.",
    "query": "Are there any research papers on generative models for code that utilize graph-based approaches to address challenges in unordered structures and sparsity?"
  },
  {
    "index": 1005,
    "source_corpus_id": 235446639,
    "ref_id": "b19",
    "citation_corpus_id": 62841605,
    "start": 8329,
    "end": 8355,
    "title": "SPREADING VECTORS FOR SIMILARITY SEARCH",
    "abstract": "Discretizing multi-dimensional data distributions is a fundamental step of modern indexing methods. State-of-the-art techniques learn parameters of quantizers on training data for optimal performance, thus adapting quantizers to the data. In this work, we propose to reverse this paradigm and adapt the data to the quantizer: we train a neural net which last layer forms a fixed parameter-free quantizer, such as pre-defined points of a hyper-sphere. As a proxy objective, we design and train a neural network that favors uniformity in the spherical latent space, while preserving the neighborhood structure after the mapping. We propose a new regularizer derived from the Kozachenko-Leonenko differential entropy estimator to enforce uniformity and combine it with a locality-aware triplet loss. Experiments show that our end-to-end approach outperforms most learned quantization methods, and is competitive with the state of the art on widely adopted benchmarks. Furthermore, we show that training without the quantization step results in almost no difference in accuracy, but yields a generic catalyzer that can be applied with any subsequent quantizer. The code is available online 1 .",
    "prev": "Learning-based algorithms Over the last few years, there has been a growing interest in using machine learning techniques to improve the performance of \"classical\" algorithms.",
    "curr": "This methodology found applications in similarity search (Wang et al., 2016;Sablayrolles et al., 2019;Dong et al., 2020), graph optimization (Khalil et al., 2017;Balcan et al., 2018), data structures (Kraska et al., 2018;Mitzenmacher, 2018), online algorithms (Lykouris & Vassilvitskii, 2018;Purohit et al., 2018), compressed sensing (Mousavi et al., 2015;Baldassarre et al., 2016;Bora et al., 2017) and streaming algorithms (Hsu et al., 2019;Jiang et al., 2019).",
    "next": "The last two papers are closest to our work, as they solve various computational problems over data streams, including distinct elements estimation in Jiang et al.",
    "query": "Are there any research papers on machine learning techniques for improving classical algorithms in similarity search and streaming data processing, particularly focusing on neural network-based quantization methods?"
  },
  {
    "index": 1008,
    "source_corpus_id": 257636846,
    "ref_id": "b24",
    "citation_corpus_id": 229923720,
    "start": 31899,
    "end": 31918,
    "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
    "abstract": "Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformerbased language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution.",
    "prev": "Being able to write SDM as an MLP with minor modifications is interesting in light of SDM's connection to Transformer Attention (Bricken & Pehlevan, 2021).",
    "curr": "This link converges with work showing that Transformer MLP layers perform associative memory-like operations that approximate Top-K by showing up to 90% activation sparsity in later layers (Geva et al., 2020;Sukhbaatar et al., 2019;Nelson et al., 2022).",
    "next": "Viewing both Attention and MLPs through the lens of SDM presents their tradeoffs: Attention operates on patterns in the model's current receptive field.",
    "query": "Are there any research papers on the role of Transformer feed-forward layers as key-value memories and their relationship to attention mechanisms or associative memory operations?"
  },
  {
    "index": 1009,
    "source_corpus_id": 235829401,
    "ref_id": "b5",
    "citation_corpus_id": 168169888,
    "start": 14293,
    "end": 14311,
    "title": "Stay on the Path: Instruction Fidelity in Vision-and-Language Navigation",
    "abstract": "Advances in learning and representations have reinvigorated work that connects language to other modalities. A particularly exciting direction is Vision-and-Language Navigation (VLN), in which agents interpret natural language instructions and visual scenes to move through environments and reach goals. Despite recent progress, current research leaves unclear how much of a role language understanding plays in this task, especially because dominant evaluation metrics have focused on goal completion rather than the sequence of actions corresponding to the instructions. Here, we highlight shortcomings of current metrics for the Room-to-Room dataset(Anderson et al., 2018b)and propose a new metric, Coverage weighted by Length Score (CLS). We also show that the existing paths in the dataset are not ideal for evaluating instruction following because they are direct-to-goal shortest paths. We join existing short paths to form more challenging extended paths to create a new data set, Room-for-Room (R4R). Using R4R and CLS, we show that agents that receive rewards for instruction fidelity outperform agents that focus on goal completion.",
    "prev": "Again, CLIP-ViT-B variant leads to dramatically worse performance compared to other visual modules, that we will discuss in Section 5.",
    "curr": "Vision-and-Language Navigation\n\nVision-and-language navigation tests the agent's ability to take action according to human instructions, which recently gains popularity in embodied AI (Anderson et al., 2018b;Chen et al., 2019;Jain et al., 2019;Chen et al., 2019;Qi et al., 2020b;Krantz et al., 2020;Nguyen and Daumé III, 2019;.",
    "next": "Specifically, the agent is put at a location in the environment (Chang et al., 2017) and asked to reach a target by following the language instructions.",
    "query": "Are there any studies that explore methods for evaluating instruction fidelity in vision-and-language navigation tasks, including the development of new metrics or datasets?"
  },
  {
    "index": 1010,
    "source_corpus_id": 247518670,
    "ref_id": "b7",
    "citation_corpus_id": 2808403,
    "start": 3736,
    "end": 3761,
    "title": "Deep Learning for Physical Processes: Incorporating Prior Scientific Knowledge",
    "abstract": "We consider the use of Deep Learning methods for modeling complex phenomena like those occurring in natural physical processes. With the large amount of data gathered on these phenomena the data intensive paradigm could begin to challenge more traditional approaches elaborated over the years in fields like maths or physics. However, despite considerable successes in a variety of application domains, the machine learning field is not yet ready to handle the level of complexity required by such problems. Using an example application, namely Sea Surface Temperature Prediction, we show how general background knowledge gained from physics could be used as a guideline for designing efficient Deep Learning models. In order to motivate the approach and to assess its generality we demonstrate a formal link between the solution of a class of differential equations underlying a large family of physical phenomena and the proposed model. Experiments and comparison with series of baselines including a state of the art numerical approach is then provided. * equal contribution .",
    "prev": "First, we can encode a known PDE into a loss function that encourages the model to fulfill the equation (Raissi et al., 2019).",
    "curr": "Another way to go about this is to derive the model structure itself from known laws such as the convection-diffusion equation (de Bézenac et al., 2018).",
    "next": "We will follow the second approach.",
    "query": "Are there any studies that explore the integration of physical laws, such as differential equations, into deep learning models for modeling natural physical processes?"
  },
  {
    "index": 1011,
    "source_corpus_id": 253264964,
    "ref_id": "b28",
    "citation_corpus_id": 1428702,
    "start": 21021,
    "end": 21040,
    "title": "Learning Word Vectors for Sentiment Analysis",
    "abstract": "Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term-document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.",
    "prev": "COMPARISON WITH BASELINES ON DIFFERENT BENCHMARKS\n\nSettings.",
    "curr": "In this section, we evaluate our MPCFormer framework with different approximations and compare it with baselines on the IMDb dataset and the GLUE benchmark (Maas et al., 2011;Wang et al., 2018).",
    "next": "For all experiments in this section, we use BERT BASE as the base model.",
    "query": "Are there any research papers on combining unsupervised and supervised techniques for learning sentiment-aware word vectors, with evaluations on benchmarks like IMDb and GLUE?"
  },
  {
    "index": 1012,
    "source_corpus_id": 21731691,
    "ref_id": "b9",
    "citation_corpus_id": 2134321,
    "start": 5640,
    "end": 5643,
    "title": "Published as a conference paper at ICLR 2016 DEEP COMPRESSION: COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING",
    "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35× to 49× without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9× to 13×; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35×, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49× from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3× to 4× layerwise speedup and 3× to 7× better energy efficiency.",
    "prev": "In the deep learning literature, low-rank factorization of convolutional filters has been previously used to remove redundancy in trained CNNs [6,17].",
    "curr": "The compression of deep networks has also been studied in [1,10,11], SqueezeNet [14], etc., where the low-rank factorization of filters can be utilized.",
    "next": "MobileNets [13] used depth-wise separable convolutions to obtain significant compression.",
    "query": "Are there any research papers on techniques for compressing deep neural networks using pruning, trained quantization, and Huffman coding?"
  },
  {
    "index": 1013,
    "source_corpus_id": 254823652,
    "ref_id": "b48",
    "citation_corpus_id": 233306870,
    "start": 4363,
    "end": 4380,
    "title": "Gradient Matching for Domain Generalization",
    "abstract": "Machine learning systems typically assume that the distributions of training and test sets match closely. However, a critical requirement of such systems in the real world is their ability to generalize to unseen domains. Here, we propose an inter-domain gradient matching objective that targets domain generalization by maximizing the inner product between gradients from different domains. Since direct optimization of the gradient inner product can be computationally prohibitive -it requires computation of second-order derivatives --we derive a simpler first-order algorithm named Fish that approximates its optimization. We perform experiments on the WILDS benchmark, which captures distribution shift in the real world, as well as the DOMAINBED benchmark that focuses more on syntheticto-real transfer. Our method produces competitive results on both benchmarks, demonstrating its effectiveness across a wide range of domain generalization tasks.",
    "prev": "With increasing attention on this issue, researchers have been probing the generalisation performance of ML models by creating datasets that feature distribution shift tasks (Koh et al., 2021;Gulrajani and Lopez-Paz, 2020;Shah et al., 2020) and proposing algorithms that aim to improve generalisation performance under distribution shift (Ganin et al., 2016;Arjovsky et al., 2019;Sun and Saenko, 2016; Figure 1: Synthetic vs. realistic distribution shift: The distribution shift in synthetic datasets (left, MNIST-CIFAR and CdSprites) are usually extreme and controllable (adjusted via changing the correlation); for realistic datasets (right, WILDS-Camelyon17 and FMoW) distribution shift can be subtle, hard to identify and impossible to control.",
    "curr": "Sagawa et al., 2020;Shi et al., 2022).",
    "next": "In this work, we identify three specific problems with current approaches in distribution shift problems, in computer vision, and develop a suite of experiments to address them.",
    "query": "Are there any research papers on methods for domain generalization under distribution shift using gradient matching techniques in computer vision?"
  },
  {
    "index": 1014,
    "source_corpus_id": 221112371,
    "ref_id": "b7",
    "citation_corpus_id": 1428702,
    "start": 9552,
    "end": 9570,
    "title": "Learning Word Vectors for Sentiment Analysis",
    "abstract": "Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term-document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on combining unsupervised and supervised techniques to learn word vectors that capture both semantic and sentiment information for sentiment analysis?"
  },
  {
    "index": 1016,
    "source_corpus_id": 246822636,
    "ref_id": "b23",
    "citation_corpus_id": 8394195,
    "start": 1895,
    "end": 1914,
    "title": "LOSSY IMAGE COMPRESSION WITH COMPRESSIVE AUTOENCODERS",
    "abstract": "We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.",
    "prev": "With the development of deep learning, learned methods have led to several breakthroughs in this task.",
    "curr": "Currently, the state-of-the-art (SOTA) deep image compression models are built on the auto-encoder framework [Hinton and Salakhutdinov, 2006] with an entropy-constrained bottleneck [Theis et al., 2017, Ballé et al., 2017, Ballé et al., 2018, Mentzer et al., 2018, Minnen et al., 2018a, Lee et al., 2019, Guo et al., 2021].",
    "next": "An entropy model estimates the conditional probability distribution of latents for compression by standard entropy coding algorithms.",
    "query": "Are there any research papers on lossy image compression techniques using compressive autoencoders with entropy-constrained bottlenecks?"
  },
  {
    "index": 1017,
    "source_corpus_id": 246652106,
    "ref_id": "b16",
    "citation_corpus_id": 3144218,
    "start": 3286,
    "end": 3308,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "As shown in Figure 1 (b), a noise edge means the connection between two faces of different classes.",
    "curr": "Unlike common graph datasets such as Citeseer, Cora and Pubmed with explicit link relation as edges (Kipf & Welling, 2017), face images do not contain explicit structural information, but only deep features extracted from a trained CNN model.",
    "next": "Therefore, face images are treated as vertices, and the edges between face images are usually constructed based on the kNN (Cover & Hart, 1967) relations when building the graph: Each face serves as a probe to retrieve its k nearest neighbours by deep features (Wang et al., 2019b;Guo et al., 2020;Shen et al., 2021).",
    "query": "Are there any studies that explore semi-supervised classification using graph convolutional networks with implicit graph structures derived from deep features and kNN relations in datasets lacking explicit structural information?"
  },
  {
    "index": 1018,
    "source_corpus_id": 252596252,
    "ref_id": "b33",
    "citation_corpus_id": 227209335,
    "start": 1587,
    "end": 1607,
    "title": "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024ˆ1024 images for the first time from a score-based generative model.",
    "prev": "INTRODUCTION\n\nDiffusion models learn the implicit prior of the underlying data distribution by matching the gradient of the log density (i.e.",
    "curr": "Stein score; ∇ x log p(x)) (Song et al., 2021b).",
    "next": "The prior can be leveraged when solving inverse problems, which aim to recover x from the measurement y, related through the forward measurement operator A and the detector noise n. When we know such forward models, one can incorporate the gradient of the log likelihood (i.e.",
    "query": "Are there any research papers on score-based generative models using stochastic differential equations for inverse problems and data distribution transformation?"
  },
  {
    "index": 1019,
    "source_corpus_id": 198986015,
    "ref_id": "b33",
    "citation_corpus_id": 5273326,
    "start": 19465,
    "end": 19469,
    "title": "Published as a conference paper at ICLR 2016 FAST AND ACCURATE DEEP NETWORK LEARNING BY EXPONENTIAL LINEAR UNITS (ELUS)",
    "abstract": "We introduce the \"exponential linear unit\" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PRe-LUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network.",
    "prev": "The conditional distribution p θ (z c k | z m k ) is parameterised by a multilayer perceptron (MLP) with two hidden layers, 256 units per layer, and ELUs (Clevert et al.",
    "curr": "[34]).",
    "next": "We use the same component VAE featuring a spatial broadcast decoder as MONet to encode and decode z c k , but we replace ReLUs (Glorot et al.",
    "query": "Are there any research papers on the use of exponential linear units (ELUs) in deep neural networks for improving learning speed and classification accuracy?"
  },
  {
    "index": 1022,
    "source_corpus_id": 239049848,
    "ref_id": "b43",
    "citation_corpus_id": 3626819,
    "start": 67576,
    "end": 67596,
    "title": "Deep contextualized word representations",
    "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
    "prev": "Small weights final layer.Following Andrychowicz et al.",
    "curr": "(2020), we consider using smaller weights for the final layers of the actor and the critic.Specifically, we downscale these weights by a factor of 100 at initialization time.We refer to this strategy as scale down.Gradient clipping.Another stabilizing strategy is gradient clipping (Zhang et al., 2020).To avoid occasional exploding gradients, one simply clips the norm of gradients that are above some threshold.This strategy is popular in NLP (Gehring et al., 2017;Peters et al., 2018) and is sometimes used in RL (Raffin et al., 2019).We consider clipping the gradient when the norm, calculated independently for the actor, critic, and convolutional encoder, is larger than 1 or 10.We refer to these strategies as grad clip 1 and grad clip 10.",
    "next": "Results.",
    "query": "Are there any research papers on methods for training deep contextualized word representations using gradient clipping and weight scaling techniques?"
  },
  {
    "index": 1023,
    "source_corpus_id": 263831010,
    "ref_id": "b31",
    "citation_corpus_id": 3144218,
    "start": 24639,
    "end": 24661,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "( 2022) is included in Graphium to reduce the cost of hyper-parameter tuning.More details are given in Appendix E.7.The library supports CPU, GPU and IPU [ §E.8] hardware to accelerate training.Further library optimisations are detailed in Appendix E.9.",
    "curr": "EXPERIMENTS ON BASELINE MODELS\n\nTo demonstrate the capabilities of the Graphium library in a multi-task setting with thousands of labels, a set of standard baselines were run with simple hyperparameter sweeps using 3 popular GNNs, namely GCN (Kipf & Welling, 2017), GIN (Xu et al., 2019), and GINE (Hu et al., 2020a).",
    "next": "A basic hyper-parameter sweep was conducted for each model and multiple random seeds used for initialisation to provide a performance baseline for future experiments to be evaluated against.",
    "query": "Are there any research papers on semi-supervised learning with graph convolutional networks that evaluate baseline models and hyperparameter tuning strategies for graph-structured data?"
  },
  {
    "index": 1024,
    "source_corpus_id": 3609219,
    "ref_id": "b12",
    "citation_corpus_id": 11243593,
    "start": 7843,
    "end": 7864,
    "title": "TRACKING THE WORLD STATE WITH RECURRENT ENTITY NETWORKS",
    "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped with a dynamic long-term memory which allows it to maintain and update a representation of the state of the world as it receives new data. For language understanding tasks, it can reason on-the-fly as it reads text, not just when it is required to answer a question or respond as is the case for a Memory Network(Sukhbaatar et al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer(Graves et al., 2014;2016)it maintains a fixed size memory and can learn to perform location and content-based read and write operations. However, unlike those models it has a simple parallel architecture in which several memory locations can be updated simultaneously. The EntNet sets a new state-of-the-art on the bAbI tasks, and is the first method to solve all the tasks in the 10k training examples setting. We also demonstrate that it can solve a reasoning task which requires a large number of supporting facts, which other methods are not able to solve, and can generalize past its training horizon. It can also be practically used on large scale datasets such as Children's Book Test, where it obtains competitive performance, reading the story in a single pass.",
    "prev": "AMN does not use strong supervision but can solve tasks that require transitive logic by modeling sentence walks on the fly.",
    "curr": "EntNet constructs dynamic networks based on entities with tied weights for each entity (Henaff et al., 2017).",
    "next": "A key-value update system allows it to update relevant (learned) entities.",
    "query": "Are there any studies that explore dynamic memory networks for language understanding tasks, particularly those using entity-based key-value updates and long-term state tracking?"
  },
  {
    "index": 1026,
    "source_corpus_id": 259095535,
    "ref_id": "b14",
    "citation_corpus_id": 174799399,
    "start": 51070,
    "end": 51089,
    "title": "Learning Deep Transformer Models for Machine Translation",
    "abstract": "Transformer is the state-of-the-art model in recent machine translation evaluations. Two strands of research are promising to improve models of this kind: the first uses wide networks (a.k.a. Transformer-Big) and has been the de facto standard for the development of the Transformer system, and the other uses deeper language representation but faces the difficulty arising from learning deep networks. Here, we continue the line of research on the latter. We claim that a truly deep Transformer model can surpass the Transformer-Big counterpart by 1) proper use of layer normalization and 2) a novel way of passing the combination of previous layers to the next. On WMT'16 English-German, NIST OpenMT'12 Chinese-English and larger WMT'18 Chinese-English tasks, our deep system (30/25-layer encoder) outperforms the shallow Transformer-Big/Base baseline (6-layer encoder) by 0.4∼2.4 BLEU points. As another bonus, the deep model is 1.6X smaller in size and 3X faster in training than Transformer-Big 1 . * Corresponding author. 1  The source code is available at https://github. com/wangqiangneu/dlcl",
    "prev": "• Affine maps W len , W num : R → Rd and W node : R 2 → Rd.",
    "curr": "• A simple stack of five Transformer blocks with eight heads in the self-attention and layer normalization before the MHA and the FF (Wang et al., 2019).",
    "next": "The structure of a Transformer block is summarized in Figure 4.",
    "query": "Are there any research papers on deep Transformer models for machine translation that utilize layer normalization and novel methods for combining previous layers?"
  },
  {
    "index": 1027,
    "source_corpus_id": 253224188,
    "ref_id": "b0",
    "citation_corpus_id": 182953134,
    "start": 4058,
    "end": 4076,
    "title": "Published as a conference paper at ICLR 2020 DEEP BATCH ACTIVE LEARNING BY DIVERSE, UNCERTAIN GRADIENT LOWER BOUNDS",
    "abstract": "We design a new algorithm for batch active learning with deep neural network models. Our algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples groups of points that are disparate and high magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between uncertainty and diversity without requiring any hand-tuned hyperparameters. While other approaches sometimes succeed for particular batch sizes or architectures, BADGE consistently performs as well or better, making it a useful option for real world active learning problems.Published as a conference paper at ICLR 2020 example, might perform poorly when using a multilayer perceptron. A diversity-based approach might work well when the batch size is very large, but poorly when the batch size is small. Further, what even constitutes a \"large\" or \"small\" batch size is largely a function of the statistical properties of the data in question. These weaknesses pose a major problem for real, practical batch active learning situations, where data are unfamiliar and potentially unstructured. There is no way to know which active learning algorithm is best to use.Moreover, in a real active learning scenario, every change of hyperparameters typically causes the algorithm to label examples not chosen under other hyperparameters, provoking substantial labeling inefficiency. That is, hyperparameter sweeps in active learning can be label expensive. As a result, active learning algorithms need to \"just work\", given fixed hyperparameters, to a greater extent than is typical for supervised learning.Based on these observations, we design an approach which creates diverse batches of examples about which the current model is uncertain. We measure uncertainty as the gradient magnitude with respect to parameters in the final (output) layer, which is computed using the most likely label according to the model. To capture diversity, we collect a batch of examples where these gradients span a diverse set of directions. More specifically, we build up the batch of query points based on these hallucinated gradients using the k-MEANS++ initialization (Arthur and Vassilvitskii, 2007), which simultaneously captures both the magnitude of a candidate gradient and its distance from previously included points in the batch. We name the resulting approach Batch Active learning by Diverse Gradient Embeddings (BADGE).We show that BADGE is robust to architecture choice, batch size, and dataset, generally performing as well as or better than the best baseline across our experiments, which vary all of the aforementioned environmental conditions. We begin by introducing our notation and setting, followed by a description of the BADGE algorithm in Section 3 and experiments in Section 4. We defer our discussion of related work to Section 5.NOTATION AND SETTINGDefine [K] := {1, 2, . . . , K}. Denote by X the instance space and by Y the label space. In this work we consider multiclass classification, so Y = [K]. Denote by D the distribution from which examples are drawn, by D X the unlabeled data distribution, and by D Y|X the conditional distribution over labels given examples. We consider the pool-based active learning setup, where the learner receives an unlabeled dataset U sampled according to D X and can request labels sampled according to D Y|X for any x ∈ U . We use E D to denote expectation under the data distribution D. Given a classifier h : X → Y, which maps examples to labels, and a labeled example (x, y), we denote the 0/1 error of h on (x, y) as 01 (h(x), y) = I(h(x) = y). The performance of a classifier h is measured by its expected 0/1 error, i.e. E D [ 01 (h(x), y)] = Pr (x,y)∼D (h(x) = y). The goal of pool-based active learning is to find a classifier with a small expected 0/1 error using as few label queries as possible. Given a set S of labeled examples (x, y), where each x ∈ S is picked from U , followed by a label query, we use E S as the sample averages over S.In this paper, we consider classifiers h parameterized by underlying neural networks f of fixed architecture, with the weights in the network denoted by θ. We abbreviate the classifier with parameters θ as h θ since the architectures are fixed in any given context, and our classifiers take the form h θ (x) = argmax y∈ [K]  f (x; θ) y , where f (x; θ) ∈ R K is a probability vector of scores assigned to candidate labels, given the example x and parameters θ. We optimize the parameters by minimizing the cross-entropy loss E S [ CE (f (x; θ), y)] over the labeled examples, where CE (p, y) = K i=1 I(y = i) ln 1 /pi = ln 1 /py.",
    "prev": "With the proposed metric, we show that coresets selected by SOTA methods at high pruning rates have much worse data coverage than random pruning, suggesting a linkage between poor data coverage of SOTA methods and poor accuracy at high pruning rates.",
    "curr": "We note that data coverage has also been studied in active learning setting (Ash et al., 2019;Citovsky et al., 2021), but techniques from active learning do not trivially extend to one-shot coreset selection.",
    "next": "We discuss the similarity and differences in Section 5.",
    "query": "Are there any studies that explore batch active learning methods leveraging diverse, uncertain gradient embeddings to enhance model performance and data coverage?"
  },
  {
    "index": 1030,
    "source_corpus_id": 252595765,
    "ref_id": "b0",
    "citation_corpus_id": 30535508,
    "start": 11529,
    "end": 11550,
    "title": "LEARNING SPARSE NEURAL NETWORKS THROUGH L 0 REGULARIZATION",
    "abstract": "We propose a practical method for L 0 norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of L 0 regularization. However, since the L 0 norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected L 0 norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the hard concrete distribution for the gates, which is obtained by \"stretching\" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.",
    "prev": "It has also be applied for feature selection (Zhao et al., 2015;Li et al., 2016;Scardapane et al., 2017;Lemhadri et al., 2021).",
    "curr": "While the LASSO is the most widely-used method for relaxing the ℓ 0 sparsity constraint in feature selection, several recent works have proposed new relaxations based on stochastic gates (Srinivas et al., 2017;Louizos et al., 2018;Balın et al., 2019;Trelin & Procházka, 2020;Yamada et al., 2020).",
    "next": "This approach introduces (learnable) Bernoulli random variables for each feature during training, and minimizes the expected loss over realizations of the 0-1 variables (accepting or rejecting features).",
    "query": "Are there any research papers on L0 regularization methods for sparse neural networks using stochastic gates and differentiable approximations?"
  },
  {
    "index": 1031,
    "source_corpus_id": 58006571,
    "ref_id": "b40",
    "citation_corpus_id": 21946795,
    "start": 4326,
    "end": 4346,
    "title": "ENSEMBLE ADVERSARIAL TRAINING: ATTACKS AND DEFENSES",
    "abstract": "Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model's loss. We show that this form of adversarial training converges to a degenerate global minimum, wherein small curvature artifacts near the data points obfuscate a linear approximation of the loss. The model thus learns to generate weak perturbations, rather than defend against strong ones. As a result, we find that adversarial training remains vulnerable to black-box attacks, where we transfer perturbations computed on undefended models, as well as to a powerful novel single-step attack that escapes the non-smooth vicinity of the input data via a small random step. We further introduce Ensemble Adversarial Training, a technique that augments training data with perturbations transferred from other models. On ImageNet, Ensemble Adversarial Training yields models with stronger robustness to blackbox attacks. In particular, our most robust model won the first round of the NIPS 2017 competition on Defenses against Adversarial Attacks (Kurakin et al., 2017c). However, subsequent work found that more elaborate black-box attacks could significantly enhance transferability and reduce the accuracy of our models.",
    "prev": "For datasets that are relatively simple and have low intrinsic dimensions (MNIST, Fashion MNIST, etc), we can obtain enough training examples to make sure adversarial training covers most part of the data distribution.",
    "curr": "For high dimensional datasets (CIFAR, ImageNet), adversarial training have been shown difficult (Kurakin et al., 2016;Tramèr et al., 2018) and only limited success was obtained.",
    "next": "A recent attack proposed by  shows that adversarial training can be defeated when the input image is produced by a generative model (for example, a generative adversarial network) rather than selected directly from the test examples.",
    "query": "Are there any research papers on adversarial training techniques for high-dimensional datasets like ImageNet and their effectiveness against black-box attacks using ensemble methods?"
  },
  {
    "index": 1032,
    "source_corpus_id": 3463636,
    "ref_id": "b5",
    "citation_corpus_id": 6706414,
    "start": 2117,
    "end": 2141,
    "title": "Published as a conference paper at ICLR 2015 EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES",
    "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
    "prev": "* indicates equal contributions\n\nINTRODUCTION\n\nDeep neural networks (DNNs) have demonstrated their outstanding performance in different domains, ranging from image processing (Krizhevsky et al., 2012;He et al., 2016), text analysis (Collobert & Weston, 2008) to speech recognition .",
    "curr": "Though deep networks have exhibited high performance for these tasks, recently they have been shown to be particularly vulnerable to adversarial perturbations added to the input images (Szegedy et al., 2013;Goodfellow et al., 2015).",
    "next": "These perturbed instances are called adversarial examples, which can lead to undesirable consequences in many practical applications based on DNNs.",
    "query": "Are there any research papers on the explanation of adversarial examples in deep neural networks and methods for mitigating them through adversarial training?"
  },
  {
    "index": 1033,
    "source_corpus_id": 254198961,
    "ref_id": "b5",
    "citation_corpus_id": 53115163,
    "start": 9077,
    "end": 9096,
    "title": "EXPLORATION BY RANDOM NETWORK DISTILLATION",
    "abstract": "We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.",
    "prev": "This is motivated by applications in robotics and Embodied Intelligence where the state is not directly observable, but can be well-approximated through the combination: s = (x, q), where x denotes stacked RGB images observed by the agent's camera, and q denotes proprioceptive sensory information, e.g., the joint pose of a robot.",
    "curr": "Furthermore, shaped reward functions can be hard to script for real-world applications (Singh et al., 2019) or result in undesirable artifacts or behaviors (Amodei et al., 2016;Burda et al., 2019).",
    "next": "Thus, we desire to learn with simple sparse rewards that accurately capture task completion.",
    "query": "Are there any research papers on exploration strategies in reinforcement learning for partially observable environments using random network distillation or similar methods to handle sparse rewards?"
  },
  {
    "index": 1035,
    "source_corpus_id": 67855499,
    "ref_id": "b11",
    "citation_corpus_id": 3529936,
    "start": 2439,
    "end": 2458,
    "title": "Gradient Estimators for Implicit Models",
    "abstract": "Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the Stein gradient estimator, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference, and entropy regularised GANs that provide improved sample diversities.",
    "prev": "Traditional VI methods approximate the true posterior with oversimplified distribution families like factorized Gaussians, which can severely limit the approximation quality and induce pathologies such as over-pruning (Trippe & Turner, 2018).",
    "curr": "These limitations have motivated the recent development of implicit VI methods (Li & Turner, 2018;Shi et al., 2018b), which allow the use of flexible approximate distributions without a tractable density.",
    "next": "However, most of the implicit inference methods require to learn a \"generator network\" that maps a simple distribution to approximate the target posterior.",
    "query": "Are there any studies that explore gradient estimation techniques for implicit models in machine learning, particularly for applications like generative adversarial networks and meta-learning for approximate inference?"
  },
  {
    "index": 1037,
    "source_corpus_id": 257219472,
    "ref_id": "b10",
    "citation_corpus_id": 52967399,
    "start": 2257,
    "end": 2278,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "When applied to various LLMs, including GPT-3, our approach significantly improves the generated discourse compared to the base LLMs and other techniques in terms of both the overall language and detoxification performance.",
    "curr": "INTRODUCTION\n\nLarge-scale Transformer-based (Vaswani et al., 2017) language models (LMs) have shown tremendous progress and grown in importance across various NLP downstream tasks, often providing stateof-the-art performances over the last few years (Devlin et al., 2019;Yang et al., 2019;Raffel et al., 2020;Peters et al., 2018).",
    "next": "Despite their progress in learning linguistic knowledge, these models have been shown to capture and reproduce toxicity in the ever-larger pretraining datasets.",
    "query": "Are there any research papers on detoxification methods for large language models using transformer-based architectures to improve discourse quality and reduce generated toxicity?"
  },
  {
    "index": 1038,
    "source_corpus_id": 258180544,
    "ref_id": "b9",
    "citation_corpus_id": 49659296,
    "start": 26679,
    "end": 26707,
    "title": "OPTIMISTIC MIRROR DESCENT IN SADDLE-POINT PROBLEMS: GOING THE EXTRA (GRADIENT) MILE",
    "abstract": "A. Owing to their connection with generative adversarial networks (GANs), saddle-point problems have recently attracted considerable interest in machine learning and beyond. By necessity, most theoretical guarantees revolve around convex-concave (or even linear) problems; however, making theoretical inroads towards e cient GAN training depends crucially on moving beyond this classic framework. To make piecemeal progress along these lines, we analyze the behavior of mirror descent (MD) in a class of non-monotone problems whose solutions coincide with those of a naturally associated variational inequality -a property which we call coherence. We rst show that ordinary, \"vanilla\" MD converges under a strict version of this condition, but not otherwise; in particular, it may fail to converge even in bilinear models with a unique solution. We then show that this de ciency is mitigated by optimism: by taking an \"extra-gradient\" step, optimistic mirror descent (OMD) converges in all coherent problems. Our analysis generalizes and extends the results of Daskalakis et al. (   ) for optimistic gradient descent (OGD) in bilinear problems, and makes concrete headway for provable convergence beyond convex-concave games. We also provide stochastic analogues of these results, and we validate our analysis by numerical experiments in a wide array of GAN models (including Gaussian mixture models, and the CelebA and CIFAR-datasets).",
    "prev": "An important remark on the properties of our presented game is due.",
    "curr": "Existing literature tackles settings with (i) (weak-)monotonocity (Mertikopoulos et al., 2019;Diakonikolas et al., 2021), (ii) cocoercivity (Zhu and Marcotte, 1996), (iii) zero-duality gap (Von Neumann, 1928), (iv) unconstrained solution space (Golowich et al., 2020) .",
    "next": "Our game is carefully crafted and -although it has a distinct structure and is nonconvex-nonconcave only due to multilinearity-satisfies none of the latter properties.",
    "query": "Are there any research papers on methods using optimistic mirror descent for non-convex-nonconcave saddle-point problems in generative adversarial networks, with convergence guarantees and extra-gradient steps?"
  },
  {
    "index": 1040,
    "source_corpus_id": 51559,
    "ref_id": "b27",
    "citation_corpus_id": 1952530,
    "start": 1928,
    "end": 1947,
    "title": "Predicting Polarities of Tweets by Composing Word Embeddings with Long Short-Term Memory",
    "abstract": "In this paper, we introduce Long Short-Term Memory (LSTM) recurrent network for twitter sentiment prediction. With the help of gates and constant error carousels in the memory block structure, the model could handle interactions between words through a flexible compositional function. Experiments on a public noisy labelled data show that our model outperforms several feature-engineering approaches, with the result comparable to the current best data-driven technique. According to the evaluation on a generated negation phrase test set, the proposed architecture doubles the performance of non-neural model based on bag-of-word features. Furthermore, words with special functions (such as negation and transition) are distinguished and the dissimilarities of words with opposite sentiment are magnified. An interesting case study on negation expression processing shows a promising potential of the architecture dealing with complex sentiment phrases.",
    "prev": "Recurrent layers can also be stacked, increasing network depth, representational power and often accuracy.",
    "curr": "RNN applications in the natural language domain range from sentence classification (Wang et al., 2015) to word-and character-level language modeling (Zaremba et al., 2014).",
    "next": "RNNs are also commonly the basic building block for more complex models for tasks such as machine translation (Bahdanau et al., 2015;Luong et al., 2015; or question answering (Kumar et al., 2016;.",
    "query": "Are there any research papers on the application of recurrent neural networks, particularly long short-term memory (LSTM) models, in natural language processing tasks such as sentiment analysis, language modeling, and machine translation?"
  },
  {
    "index": 1042,
    "source_corpus_id": 245668925,
    "ref_id": "b18",
    "citation_corpus_id": 15986631,
    "start": 3620,
    "end": 3638,
    "title": "Deep Reinforcement Learning with a Natural Language Action Space",
    "abstract": "This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games. Termed a deep reinforcement relevance network (DRRN), the architecture represents action and state spaces with separate embedding vectors, which are combined with an interaction function to approximate the Q-function in reinforcement learning. We evaluate the DRRN on two popular text games, showing superior performance over other deep Qlearning architectures. Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text.",
    "prev": "On the other hand, since rewards are sparse, the agent only gets a few high-scoring trajectories to learn from, requiring vigorous exploitation in order to get back to the furthest point of the game and make progress thereon.",
    "curr": "Prior approaches to solving these games (He et al., 2016a;Guo et al., 2020) usually employ a single policy and action selection strategy, making it difficult to strike the right balance between exploration and exploitation.",
    "next": "In this paper, we propose eXploit-Then-eXplore (XTX), an algorithm for multi-stage control to explicitly decompose the exploitation and exploration phases within each episode.",
    "query": "Are there any studies that explore methods for balancing exploration and exploitation in reinforcement learning with sparse rewards, particularly in environments with natural language action spaces?"
  },
  {
    "index": 1045,
    "source_corpus_id": 258564352,
    "ref_id": "b11",
    "citation_corpus_id": 222133157,
    "start": 3787,
    "end": 3807,
    "title": "Published as a conference paper at ICLR 2021 MASTERING ATARI WITH DISCRETE WORLD MODELS",
    "abstract": "Intelligent agents need to generalize from past experience to achieve goals in complex environments. World models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. While learning world models from image inputs has recently become feasible for some tasks, modeling Atari games accurately enough to derive successful behaviors has remained an open challenge for many years. We introduce DreamerV2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model. The world model uses discrete representations and is trained separately from the policy. DreamerV2 constitutes the first agent that achieves human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. With the same computational budget and wall-clock time, Dreamer V2 reaches 200M frames and surpasses the final performance of the top single-GPU agents IQN and Rainbow. DreamerV2 is also applicable to tasks with continuous actions, where it learns an accurate world model of a complex humanoid robot and solves stand-up and walking from only pixel inputs.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on using discrete world models in reinforcement learning to achieve human-level performance in Atari games through sample-efficient learning from imagined outcomes?"
  },
  {
    "index": 1046,
    "source_corpus_id": 251066616,
    "ref_id": "b9",
    "citation_corpus_id": 8768364,
    "start": 53746,
    "end": 53765,
    "title": "Density estimation using Real NVP",
    "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.",
    "prev": "B.6 LIMITATIONS AND FUTURE WORK\n\nThe proposed work focuses mainly on a mixture density network (MDN) as the generative channel model, which allows us to exploit some of their useful properties in our formulation.",
    "curr": "Generalizing the proposed few-shot domain adaptation to other types of generative channel models such as conditional GANs, VAEs, and normalizing flows (Dinh et al., 2017) could be an interesting direction.",
    "next": "These generative models can handle more high-dimensional structured inputs.",
    "query": "Are there any research papers on applying few-shot domain adaptation techniques to generative models such as conditional GANs, VAEs, and normalizing flows for density estimation tasks?"
  },
  {
    "index": 1051,
    "source_corpus_id": 239050418,
    "ref_id": "b22",
    "citation_corpus_id": 236087592,
    "start": 2888,
    "end": 2908,
    "title": "VISUAL REPRESENTATION LEARNING DOES NOT GENERALIZE STRONGLY WITHIN THE SAME DOMAIN",
    "abstract": "An important component for generalization in machine learning is to uncover underlying latent factors of variation as well as the mechanism through which each factor acts in the world. In this paper, we test whether 17 unsupervised, weakly supervised, and fully supervised representation learning approaches correctly infer the generative factors of variation in simple datasets (dSprites, Shapes3D, MPI3D) from controlled environments, and on our contributed CelebGlow dataset. In contrast to prior robustness work that introduces novel factors of variation during test time, such as blur or other (un)structured noise, we here recompose, interpolate, or extrapolate only existing factors of variation from the training data set (e.g., small and medium-sized objects during training and large objects during testing). Models that learn the correct mechanism should be able to generalize to this benchmark. In total, we train and test 2000+ models and observe that all of them struggle to learn the underlying mechanism regardless of supervision signal and architectural bias. Moreover, the generalization capabilities of all tested models drop significantly as we move from artificial datasets towards more realistic real-world datasets. Despite their inability to identify the correct mechanism, the models are quite modular as their ability to infer other in-distribution factors remains fairly stable, providing only a single factor is out-of-distribution. These results point to an important yet understudied problem of learning mechanistic models of observations that can facilitate generalization. Tenenbaum. Simulation as an engine of physical scene understanding. . Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828, 2013. M. Besserve, R. Sun, D. Janzing, and B. Schölkopf. A theory of independent mechanisms for extrapolation in generative models.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on the challenges of generalizing visual representation learning models within the same domain, focusing on their inability to learn underlying generative factors of variation?"
  },
  {
    "index": 1053,
    "source_corpus_id": 211082795,
    "ref_id": "b21",
    "citation_corpus_id": 604334,
    "start": 1673,
    "end": 1695,
    "title": "Intriguing properties of neural networks",
    "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
    "prev": "∀x .",
    "curr": "x−x p ≤ =⇒F (x)=F (x )(1)\nThis problem carries practical significance, as such networks have been extensively shown to be vulnerable to adversarial examples (Szegedy et al., 2014;Papernot et al., 2016), wherein small-norm perturbations are chosen to cause aribtrary misclassifications.",
    "next": "Numerous solutions have been proposed to address variants of this problem.",
    "query": "Are there any research papers on adversarial examples in neural networks and methods to mitigate their vulnerability?"
  },
  {
    "index": 1054,
    "source_corpus_id": 233306870,
    "ref_id": "b6",
    "citation_corpus_id": 68222714,
    "start": 3462,
    "end": 3475,
    "title": "APPROXIMATING CNNS WITH BAG-OF-LOCAL- FEATURES MODELS WORKS SURPRISINGLY WELL ON IMAGENET",
    "abstract": "Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difficult to understand how they reach their decisions. We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain. Our model, a simple variant of the ResNet-50 architecture called BagNet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering. This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet (87.6% top-5 for 33 × 33 px features and Alexnet performance for 17 × 17 px features). The constraint on local features makes it straight-forward to analyse how exactly each part of the image influences the classification. Furthermore, the BagNets behave similar to state-of-the art deep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of feature sensitivity, error distribution and interactions between image parts. This suggests that the improvements of DNNs over previous bag-of-feature classifiers in the last few years is mostly achieved by better fine-tuning rather than by qualitatively different decision strategies.",
    "prev": "Consider, for example, a model that is built to distinguish between cows and camels using photos collected in nature under different climates.",
    "curr": "Since CNNs are known to have a bias towards texture (Geirhos et al., 2018, Brendel andBethge, 2019), if we simply try to minimize the average loss across different domains, the classifier is prone to spuriously correlate \"cow\" with grass and \"camels\" with desert, and predict the species using only the background.",
    "next": "Such a classifier can be rendered useless when the animals are placed indoors or in a zoo.",
    "query": "Are there any studies that investigate the impact of texture bias in CNNs and explore alternative models using local features to improve domain generalization in image classification?"
  },
  {
    "index": 1055,
    "source_corpus_id": 252683303,
    "ref_id": "b16",
    "citation_corpus_id": 231632658,
    "start": 3133,
    "end": 3151,
    "title": "What Makes Good In-Context Examples for GPT-3?",
    "abstract": "GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting incontext examples (relative to random sampling) that better leverage GPT-3's in-context learning capabilities. Inspired by the recent success of leveraging a retrieval module to augment neural networks, we propose to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt. Intuitively, the examples selected with such a strategy may serve as more informative inputs to unleash GPT-3's power of text generation. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline. Moreover, it is observed that the sentence encoders finetuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-totext generation (44.3% on the ToTTo dataset) and open-domain question answering (45.5% on the NQ dataset).",
    "prev": "This work studies example selection in chain-of-thoughts multi-step reasoning.",
    "curr": "Example selection is a central problem in the prompting literature (Liu et al., 2022;Rubin et al., 2022;Su et al., 2022;Lazaridou et al., 2022).",
    "next": "It asks what instances make the best prompts for solving the tasks of interest.",
    "query": "Are there any studies that investigate methods for selecting effective in-context examples in large language models, particularly using retrieval-based strategies and semantic similarity to enhance performance on natural language understanding and generation tasks?"
  },
  {
    "index": 1057,
    "source_corpus_id": 248965297,
    "ref_id": "b13",
    "citation_corpus_id": 52967399,
    "start": 2456,
    "end": 2477,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "In ACL, 2021.",
    "curr": "INTRODUCTION\n\nLarge-scale pre-trained language models (PLMs) have achieved great success in promoting state of the art on various natural language understanding and generation tasks (Devlin et al., 2019;Radford et al., 2019;Yang et al., 2019;Brown et al., 2020;.",
    "next": "PLM self-supervision training largely benefits from harvesting local context information in the pre-training corpus.",
    "query": "Are there any research papers on bidirectional pretraining of transformer-based language models for natural language understanding tasks?"
  },
  {
    "index": 1058,
    "source_corpus_id": 256868350,
    "ref_id": "b6",
    "citation_corpus_id": 195750622,
    "start": 30219,
    "end": 30241,
    "title": "Selection Via Proxy: Efficient Data Selection For Deep Learning *",
    "abstract": "Data selection methods such as active learning and core-set selection are useful tools for machine learning on large datasets, but they can be prohibitively expensive to apply in deep learning. Unlike in other areas of machine learning, the feature representations that these techniques depend on are learned in deep learning rather than given, which takes a substantial amount of training time. In this work, we show that we can significantly improve the computational efficiency of data selection in deep learning by using a much smaller proxy model to perform data selection for tasks that will eventually require a large target model (e.g., selecting data points to label for active learning). In deep learning, we can scale down models by removing hidden layers or reducing their dimension to create proxies that are an order of magnitude faster. Although these small proxy models have significantly higher error, we find that they empirically provide useful rankings for data selection that have a high correlation with those of larger models. We evaluate this \"selection via proxy\" (SVP) approach on several data selection tasks. For active learning, applying SVP to Sener and Savarese [2018]'s recent method for active learning in deep learning gives a 4× improvement in execution time while yielding the same model accuracy. For core-set selection, we show that a proxy model that trains 10× faster than a target ResNet164 model on CIFAR10 can be used to remove 50% of the training data without compromising the accuracy of the target model, making end-to-end training time improvements via core-set selection possible. * updated work from Coleman et al. [2019]    Preprint. Under review.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on efficient data selection methods for deep learning using proxy models in active learning or core-set selection tasks?"
  },
  {
    "index": 1059,
    "source_corpus_id": 255440465,
    "ref_id": "b14",
    "citation_corpus_id": 225039882,
    "start": 1724,
    "end": 1728,
    "title": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.RELATED WORKTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.",
    "prev": "Introduction\n\nThe transformer architecture [50] has become an important and highly influential model family, due to its simplicity, scalability, and its wide range of applications.",
    "curr": "While originally stemming from the domain of natural language processing (NLP), with the advent of the Vision transformer (ViT) [15], this has become a standard architecture in computer vision, setting various state-of-the-art (SoTA) performances on tasks ranging from representation learning, semantic segmentation, object detection and video understanding [4,5,18,30,31].",
    "next": "However, the original formulation of the transformer includes a quadratic computational complexity with respect to the number of input tokens.",
    "query": "Are there any research papers on the application of vision transformers (ViT) for image recognition tasks, focusing on large-scale pre-training and performance comparisons with convolutional neural networks?"
  },
  {
    "index": 1061,
    "source_corpus_id": 226221794,
    "ref_id": "b29",
    "citation_corpus_id": 221878944,
    "start": 9619,
    "end": 9636,
    "title": "How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks",
    "abstract": "We study how neural networks trained by gradient descent extrapolate, i.e., what they learn outside the support of training distribution. Previous works report mixed empirical results when extrapolating with neural networks: while multilayer perceptrons (MLPs) do not extrapolate well in simple tasks, Graph Neural Networks (GNNs), a structured network with MLP modules, have some success in more complex tasks. We provide a theoretical explanation and identify conditions under which MLPs and GNNs extrapolate well. We start by showing ReLU MLPs trained by gradient descent converge quickly to linear functions along any direction from the origin, which suggests ReLU MLPs cannot extrapolate well in most non-linear tasks. On the other hand, ReLU MLPs can provably converge to a linear target function when the training distribution is \"diverse\" enough. These observations lead to a hypothesis: GNNs can extrapolate well in dynamic programming (DP) tasks if we encode appropriate non-linearity in the architecture and input representation. We provide theoretical and empirical support for the hypothesis. Our theory explains previous extrapolation success and suggest their limitations: successful extrapolation relies on incorporating task-specific non-linearity, which often requires domain knowledge or extensive model search.",
    "prev": "Also, we consider a larger family of distribution shifts including covariate shifts.",
    "curr": "The above two categories are not exhaustivee.g., there are some recent works that characterize how some inductive biases favor extrapolation [Xu et al., 2021] and can be better for OOD generalization.",
    "next": "Sample Complexity of Invariant Risk Minimization\n\n\nInvariant Risk Minimization\n\nWe start with some background on IRM [Arjovsky et al., 2019].",
    "query": "Are there any studies exploring how neural networks extrapolate in out-of-distribution scenarios, focusing on inductive biases, architectural design, and their impact on generalization?"
  },
  {
    "index": 1062,
    "source_corpus_id": 259298560,
    "ref_id": "b29",
    "citation_corpus_id": 3488815,
    "start": 12433,
    "end": 12453,
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. * Authors ordered alphabetically.",
    "prev": "We also consider Voneresnet (biological V1 front-end (Dapello et al., 2020)) and ResNet-50 trained as a base network for action recognition (Chen et al., 2021) but include these as separate examples in the Appendix since they use a modified architecture.",
    "curr": "MODELS TRAINED FOR ROBUSTNESS\n\nWe consider two forms of models trained for minimizing a classification loss L ce in the face of input perturbations δ ∈ R h×w×c subject to constraints on the overall magnitude of perturbations in the input space, where x, y, θ are the network input, output, and classifier parameters, respectively:\nL ce (θ, x + δ, y)(1)\nIn adversarially trained networks, projected gradient descent from the output space finds maximal directions of perturbation in the input space limited to length ϵ, and training entails minimizing the effect of these perturbation directions on the network's output (Madry et al., 2018).",
    "next": "In random smoothing (Lecuyer et al., 2018;, a supervised network is trained but in the face of Gaussian noise added to the input space as the base classifier before performing a probabilistic inference.",
    "query": "Are there any research papers on deep learning models trained for robustness against adversarial attacks, specifically using adversarial training and random smoothing techniques?"
  },
  {
    "index": 1068,
    "source_corpus_id": 235614244,
    "ref_id": "b4",
    "citation_corpus_id": 11212020,
    "start": 23811,
    "end": 23834,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "ntic segmentation is a saturated task, and most contemporary published works have approximate performances, Hamburger shows considerable improvements over previous state-of-the-art attention modules.",
    "curr": "Method mIoU(%) PSPNet (Zhao et al., 2017) 82.6 DFN * (Yu et al., 2018) 82.7 EncNet  82.9 DANet * (Fu et al., 2019) 82.6 DMNet * (He et al., 2019a) 84.4 APCNet * (He et al., 2019b) 84.2 CFNet * (Zhang et al., 2019b) 84.2 SpyGR *  84.2 SANet * (Zhong et al., 2020) 83.2 OCR * (Yuan et al., 2020) 84.3 HamNet 85.9 Method mIoU(%) PSPNet (Zhao et al., 2017) 47.8 SGR * (Liang et al., 2018) 50.8 EncNet  51.7 DANet * (Fu et al., 2019) 52.6 EMANet * (Li et al., 2019a) 53.1 DMNet * (He et al., 2019a) 54.4 APCNet * (He et al., 2019b) 54.7 CFNet * (Zhang et al., 2019b) 54.0 SpyGR *  52.8 SANet * (Zhong et al., 2020) 53.0 OCR * (Yuan et al., 2020) 54.8 HamNet 55.2 \n\n\nIMAGE GENERATION\n\n\nRELATED WORK\n\nGeneral Survey for Attention The last five years have witnessed a roaring success of attention mechanisms (Bahdanau et al., 2015;Mnih et al., 2014;Xu et al., 2015;Luong et al., 2015) in deep learning.",
    "next": "Roughly speaking, the attention mechanism is a term of adaptively generating the targets' weights to be attended according to the requests.",
    "query": "Are there any research papers on attention mechanisms for semantic segmentation that improve upon previous state-of-the-art models in terms of performance metrics like mIoU?"
  },
  {
    "index": 1069,
    "source_corpus_id": 222132819,
    "ref_id": "b12",
    "citation_corpus_id": 202565422,
    "start": 40425,
    "end": 40443,
    "title": "HIERARCHICAL FORESIGHT: SELF-SUPERVISED LEARNING OF LONG-HORIZON TASKS VIA VISUAL SUBGOAL GENERATION",
    "abstract": "Video prediction models combined with planning algorithms have shown promise in enabling robots to learn to perform many vision-based tasks through only selfsupervision, reaching novel goals in cluttered scenes with unseen objects. However, due to the compounding uncertainty in long horizon video prediction and poor scalability of sampling-based planning optimizers, one significant limitation of these approaches is the ability to plan over long horizons to reach distant goals.To that end, we propose a framework for subgoal generation and planning, hierarchical visual foresight (HVF), which generates subgoal images conditioned on a goal image, and uses them for planning. The subgoal images are directly optimized to decompose the task into easy to plan segments, and as a result, we observe that the method naturally identifies semantically meaningful states as subgoals. Across three out of four simulated vision-based manipulation tasks, we find that our method achieves nearly a 200% performance improvement over planning without subgoals and model-free RL approaches. Further, our experiments illustrate that our approach extends to real, cluttered visual scenes. † Work completed at Google Brain Videos and code are available at:",
    "prev": "(c) Role action spaces and role policy structure.",
    "curr": "Figure 2 :\n2Nachum et al., 2018b;a; Levy et al., 2018; Ghosh et al., 2018; Dwiel  et al., 2019;Nair & Finn, 2019;Nasiriany et al., 2019; Dilokthanakul et al., 2019)  or by discovering reusable skills(Daniel et al., 2012; Gregor et al., 2016;Warde-Farley et al., 2018;Shankar & Gupta, 2020;Thomas et al., 2018;Sharma et al., 2020).",
    "next": "In multi-agent settings, many challenges, such as efficient communication(Ossenkopf et al., 2019) and labor division(Wang et al., 2020c) in large systems, necessitate hierarchical learning along the second dimension -over agents(Zhang et al., 2010).Ahilan & Dayan (2019) propose a hierarchy in FeUdal(Dayan & Hinton, 1993;Vezhnevets et al., 2017) style for cooperative multi-agent tasks, where managers and workers are predefined and workers are expected to achieve the goal generated by managers.",
    "query": "Are there any research papers on hierarchical self-supervised learning methods for long-horizon visual tasks using subgoal generation and planning algorithms?"
  },
  {
    "index": 1074,
    "source_corpus_id": 253080708,
    "ref_id": "b23",
    "citation_corpus_id": 3144218,
    "start": 2175,
    "end": 2199,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "GNNs broadly follow a message-passing framework, meaning that each layer of the GNN aggregates the representations of a node and its neighbors, and transforms these features into a new representation for that node.",
    "curr": "The aggregation function used by the GNN layer is taken to be locally permutationinvariant, since the ordering of the neighbors of a node is arbitrary, and its specific form is a key component of the GNN architecture; varying it gives rise to several common GNN variants (Kipf and Welling, 2017;Veličković et al., 2018;Li et al., 2015;Hamilton et al., 2017;Xu et al., 2019).",
    "next": "The output of a GNN can be used for tasks such as graph classification or node classification.",
    "query": "Are there any research papers on semi-supervised learning approaches using graph convolutional networks for classification tasks?"
  },
  {
    "index": 1075,
    "source_corpus_id": 53113561,
    "ref_id": "b17",
    "citation_corpus_id": 1957433,
    "start": 6778,
    "end": 6802,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "BASIC DESIGN OF MACHINE COMPREHENSION MODELS\n\nFor single-turn MC, many top-performing models share a similar architecture, consisting of four major components: (1) question encoding, (2) context encoding, (3) reasoning, and finally (4) answer prediction.",
    "curr": "Initially the word embeddings (e.g., Pennington et al., 2014;Peters et al., 2018) of question tokens Q and context tokens C are taken as input and fed into contextual integration layers, such as LSTMs (Hochreiter & Schmidhuber, 1997) or self attentions (Yu et al., 2018), to encode the question and context.",
    "next": "Multiple integration layers provide contextualized representations of context, and are often inter-weaved with attention, which inject question information.",
    "query": "Are there any research papers on machine comprehension models that utilize word embeddings and contextual integration layers such as LSTMs or self-attention mechanisms?"
  },
  {
    "index": 1076,
    "source_corpus_id": 147704157,
    "ref_id": "b19",
    "citation_corpus_id": 65455367,
    "start": 3260,
    "end": 3279,
    "title": "ON THE CONVERGENCE OF ADAM AND BEYOND",
    "abstract": "Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous analysis of ADAM algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with \"long-term memory\" of past gradients, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on the convergence analysis of ADAM and related stochastic optimization algorithms, and approaches to address their convergence failures through modifications like incorporating long-term memory of gradients?"
  },
  {
    "index": 1077,
    "source_corpus_id": 229923128,
    "ref_id": "b18",
    "citation_corpus_id": 8822680,
    "start": 2025,
    "end": 2048,
    "title": "Six Challenges for Neural Machine Translation",
    "abstract": "We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrasebased statistical machine translation.",
    "prev": "INTRODUCTION\n\nWhen translating a word, translation models need to spend a substantial amount of its capacity in disambiguating its sense in the source language and choose a lexeme in the target language which adequately express its meaning (Choi et al., 2017;Tamchyna, 2017).",
    "curr": "However, neural machine translation (NMT) has a severe problem on lexical choice, since it usually has mistranslation errors on low-frequency words (Koehn & Knowles, 2017;Nguyen & Chiang, 2018).",
    "next": "Table 1: All samples that contain the source word \"纽 马 基特\" in raw and distilled training corpora, which are different in target sides (RAW-TGT vs. KD-TGT).",
    "query": "Are there any research papers on challenges in neural machine translation related to lexical choice and mistranslation of low-frequency words?"
  },
  {
    "index": 1078,
    "source_corpus_id": 238856778,
    "ref_id": "b30",
    "citation_corpus_id": 1957433,
    "start": 19469,
    "end": 19494,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "(Even without having ever seen a white wolf, a typical English speaker can guess that a white wolf is more likely to resemble an arctic fox than a snorkel.)",
    "curr": "These kinds of relations are often captured by embeddings of class labels (or more detailed class descriptions) (Pennington et al., 2014).",
    "next": "When available, this kind of information about class semantics can be used to construct an improved subspace regularizer by encouraging new class representations to lie close to a convex combination of base classes weighted by their semantic similarity.",
    "query": "Are there any research papers on word representation methods that leverage semantic relationships and class embeddings to enhance subspace regularizers?"
  },
  {
    "index": 1080,
    "source_corpus_id": 263672002,
    "ref_id": "b27",
    "citation_corpus_id": 227209335,
    "start": 2217,
    "end": 2236,
    "title": "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024ˆ1024 images for the first time from a score-based generative model.",
    "prev": "an be characterized as the minimizers of simple quadratic objectives, and they are defined on a simplex that generalizes the time variable in the usual dynamical transport framework.The resulting transport on the simplex is influenced by all marginals, and we show that multi-way correspondences can be extracted.The identification of such correspondences has applications to style transfer, algorithmic fairness, and data decorruption.In addition, the multimarginal perspective enables an efficient algorithm for reducing the dynamical transport cost in the ordinary two-marginal setting.We demonstrate these capacities with several numerical examples.",
    "curr": "INTRODUCTION\n\nGenerative models built upon dynamical transport of measure, in which two probability densities are connected by a learnable transformation, underlie many recent advances in unsupervised learning (Rombach et al., 2022;Dhariwal & Nichol, 2021).Contemporary methods such as normalizing flows (Rezende & Mohamed, 2015) and diffusions (Song et al., 2021b) transform samples from one density ρ 0 into samples from another density ρ 1 through an ordinary or stochastic differential equation (ODE/SDE).In such frameworks one must learn the velocity field defining the ODE/SDE.One effective algorithm for learning the velocity is based on the construction of a stochastic interpolant, a stochastic process that interpolates between the two probability densities at the level of the individual samples.The velocity can be characterized conveniently as the solution of a tractable square loss regression problem.In conventional generative modeling, one chooses ρ 0 to be an analytically tractable reference density, such as the standard normal density, while ρ 1 is some target density of interest, accessible only through a dataset of samples.In this setting, a general stochastic interpolant x t can be written as\nx t = α 0 (t)x 0 + α 1 (t)x 1 ,(1)\nwhere x 0 ∼ ρ 0 and x 1 ∼ ρ 1 and we allow for the possibility of dependence between x 0 and x 1 .Meanwhile α",
    "next": null,
    "query": "Are there any research papers on score-based generative models using stochastic differential equations for data synthesis, particularly focusing on reverse-time SDEs, predictor-corrector frameworks, and applications in image generation?"
  },
  {
    "index": 1081,
    "source_corpus_id": 260351224,
    "ref_id": "b4",
    "citation_corpus_id": 29153681,
    "start": 11096,
    "end": 11121,
    "title": "META-LEARNING WITH DIFFERENTIABLE CLOSED-FORM SOLVERS",
    "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.Most work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent. Nonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently. In this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning. The main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data. This requires back-propagating errors through the solver steps. While normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage. We propose both closed-form and iterative solvers, based on ridge regression and logistic regression components. Our methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.",
    "prev": "Bilevel optimization.",
    "curr": "Bilevel optimization is a powerful tool to study many machine learning applications such as hyperparameter optimization (Franceschi et al., 2018;Shaban et al., 2019), meta-learning (Bertinetto et al., 2018;Franceschi et al., 2018;Rajeswaran et al., 2019;Ji et al., 2020;, neural architecture search (Liu et al., 2018;Zhang et al., 2021a), etc.",
    "next": "Existing approaches are usually approximate implicit differentiation (AID) based (Domke, 2012;Pedregosa, 2016;Gould et al., 2016;Liao et al., 2018;Lorraine et al., 2020), or iterative differentiation (ITD) based (Domke, 2012;Maclaurin et al., 2015;Franceschi et al., 2017;Finn et al., 2017;Shaban et al., 2019;Rajeswaran et al., 2019;Liu et al., 2020).",
    "query": "Are there any research papers on bilevel optimization methods for machine learning applications, such as hyperparameter tuning, meta-learning, or neural architecture search, and their implementation through approximate implicit or iterative differentiation techniques?"
  },
  {
    "index": 1082,
    "source_corpus_id": 108297416,
    "ref_id": "b0",
    "citation_corpus_id": 3482308,
    "start": 28333,
    "end": 28352,
    "title": "Understanding Deep Neural Networks with Rectified Linear Units",
    "abstract": "In this paper we investigate the family of functions representable by deep neural networks (DNN) with rectified linear units (ReLU). We give the first-ever polynomial time (in the size of data) algorithm to train to global optimality a ReLU DNN with one hidden layer, assuming the input dimension and number of nodes of the network as fixed constants.We also improve on the known lower bounds on size (from exponential to super exponential) for approximating a ReLU deep net function by a shallower ReLU net. Our gap theorems hold for smoothly parametrized families of \"hard\" functions, contrary to countable, discrete families known in the literature. An example consequence of our gap theorems is the following: for every natural number k there exists a function representable by a ReLU DNN with k 2 hidden layers and total size k 3 , such that any ReLU DNN with at most k hidden layers will require at least 1 2 k k+1 − 1 total nodes.Finally, we construct a family of R n → R piecewise linear functions for n ≥ 2 (also smoothly parameterized), whose number of affine pieces scales exponentially with the dimension n at any fixed size and depth. To the best of our knowledge, such a construction with exponential dependence on n has not been achieved by previous families of \"hard\" functions in the neural nets literature. This construction utilizes the theory of zonotopes from polyhedral theory.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on the theoretical analysis of deep neural networks with rectified linear units, including polynomial time training algorithms, approximation bounds, and constructions of hard functions with exponential complexity?"
  },
  {
    "index": 1083,
    "source_corpus_id": 256663850,
    "ref_id": "b4",
    "citation_corpus_id": 229376913,
    "start": 8174,
    "end": 8191,
    "title": "RealFormer: Transformer Likes Residual Attention",
    "abstract": "Transformer is the backbone of modern NLP models. In this paper, we propose Real-Former, a simple and generic technique to create Residual Attention Layer Transformer networks that significantly outperform the canonical Transformer and its variants (BERT, ETC, etc.) on a wide spectrum of tasks including Masked Language Modeling, GLUE, SQuAD, Neural Machine Translation, WikiHop, HotpotQA, Natural Questions, and OpenKP. We also observe empirically that RealFormer stabilizes training and leads to models with sparser attention. Source code and pre-trained checkpoints for RealFormer can be found at https",
    "prev": "To distinguish it from our RLA, we rename the former as RLA g in the following.",
    "curr": "RealFormer (He et al., 2021) and EA-Transformer (Wang et al., 2021) both added attention scores in the previous layer to the current one, connecting the layers by residual attention.",
    "next": "Bapna et al.",
    "query": "Are there any studies that explore transformer architectures incorporating residual attention mechanisms for natural language processing tasks?"
  },
  {
    "index": 1086,
    "source_corpus_id": 212414027,
    "ref_id": "b6",
    "citation_corpus_id": 53464644,
    "start": 24902,
    "end": 24925,
    "title": "LEARNING PROTEIN SEQUENCE EMBEDDINGS USING INFORMATION FROM STRUCTURE",
    "abstract": "Inferring the structural properties of a protein from its amino acid sequence is a challenging yet important problem in biology. Structures are not known for the vast majority of protein sequences, but structure is critical for understanding function. Existing approaches for detecting structural similarity between proteins from sequence are unable to recognize and exploit structural patterns when sequences have diverged too far, limiting our ability to transfer knowledge between structurally related proteins. We newly approach this problem through the lens of representation learning. We introduce a framework that maps any protein sequence to a sequence of vector embeddings -one per amino acid position -that encode structural information. We train bidirectional long short-term memory (LSTM) models on protein sequences with a two-part feedback mechanism that incorporates information from (i) global structural similarity between proteins and (ii) pairwise residue contact maps for individual proteins. To enable learning from structural similarity information, we define a novel similarity measure between arbitrarylength sequences of vector embeddings based on a soft symmetric alignment (SSA) between them. Our method is able to learn useful position-specific embeddings despite lacking direct observations of position-level correspondence between sequences. We show empirically that our multi-task framework outperforms other sequence-based methods and even a top-performing structure-based alignment method when predicting structural similarity, our goal. Finally, we demonstrate that our learned embeddings can be transferred to other protein sequence problems, improving the state-of-the-art in transmembrane domain prediction. 1 1 source code and datasets are available at Published as a conference paper at ICLR 2019 but the problem is challenging, because sequence similarity and structural similarity are only loosely related [1, 2, 3, 4], e.g. similar structural folds can be formed by diverse sequences. As a result, our ability to transfer knowledge between proteins with similar structures is limited.In this work, we address this problem by learning protein sequence embeddings using weak supervision from global structural similarity for the first time. Specifically, we aim to learn a bidirectional LSTM (biLSTM) embedding model, mapping sequences of amino acids to sequences of vector representations, such that residues occurring in similar structural contexts will be close in embedding space. This is difficult, because we have not observed position-level correspondences between sequences, only global sequence similarity. We solve this by defining a whole sequence similarity measure from sequences of vector embeddings. The measure decomposes into an alignment of the sequences and pairwise comparison of the aligned positions in embedding space. For the alignment, we propose a soft symmetric alignment (SSA) mechanism -a symmetrization of the directional alignment commonly used in attention mechanisms. Furthermore, in order to take advantage of information about local structural context within proteins, we extend this framework to include position-level supervision from contacts between residues in the individual protein structures. This multitask framework(Figure 1) allows us to newly leverage both global structural similarity between proteins and residue-residue contacts within proteins for training embedding models.",
    "prev": "In contrast to our work, these methods operate over small molecular graphs and were not applied to large macromolecules, like proteins.",
    "curr": "In parallel, recent work proposes that generative models pre-trained on protein sequences can transfer knowledge to downstream supervised tasks (Bepler & Berger, 2019;Alley et al., 2019;Yang et al., 2019;Rives et al., 2019).",
    "next": "These methods have also been explored for protein design (Wang et al., 2018).",
    "query": "Are there any research papers on methods for learning protein sequence embeddings using structural information to enable knowledge transfer or improve downstream tasks in protein analysis?"
  },
  {
    "index": 1087,
    "source_corpus_id": 259203325,
    "ref_id": "b3",
    "citation_corpus_id": 199371687,
    "start": 66589,
    "end": 66610,
    "title": "The BEA-2019 Shared Task on Grammatical Error Correction",
    "abstract": "This paper reports on the BEA-",
    "prev": "For our target X set, Jayanthi et al.",
    "curr": "(2020) provide 40k pairs of real mistakes by humans taken from the BEA grammar correction corpus (Bryant et al., 2019), which we use as our ideal target X.\n\n\nC.4.2 GIO\n\nWe first generate embeddings for the train and target sets using MPNet-Base-V2 (Song et al., 2020) on the input side of the data; see Appendix C.2 for an example, we use the same process as in our WMT experiments.",
    "next": "On an AWS p3dn.24xlarge machine with 8 NVIDIA Tesla V100 GPUs, it takes about 4 hours to generate the embeddings for the 15M data.",
    "query": "Are there any research papers on using the BEA grammar correction corpus for grammatical error correction tasks with pre-trained models for embedding generation?"
  },
  {
    "index": 1088,
    "source_corpus_id": 222140947,
    "ref_id": "b17",
    "citation_corpus_id": 4009713,
    "start": 2340,
    "end": 2362,
    "title": "Published as a conference paper at ICLR 2018 UNSUPERVISED REPRESENTATION LEARNING BY PRE- DICTING IMAGE ROTATIONS",
    "abstract": "Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training Con-vNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet.",
    "prev": "Several early methods propose pretext tasks that explore the inherent structures within a single image.",
    "curr": "For example, by identifying spatial arrangement (Doersch et al., 2015), orientation (Gidaris et al., 2018), or chromatic channels , models learn useful representations for downstream tasks.",
    "next": "Recently, another line of works Hjelm et al., 2018;Misra & van der Maaten, 2020;Chen et al., 2020a), e.g.",
    "query": "Are there any research papers on unsupervised representation learning using rotation prediction as a pretext task?"
  },
  {
    "index": 1089,
    "source_corpus_id": 235485093,
    "ref_id": "b14",
    "citation_corpus_id": 212657453,
    "start": 37146,
    "end": 37164,
    "title": "GAUGE EQUIVARIANT MESH CNNS ANISOTROPIC CONVOLUTIONS ON GEOMETRIC GRAPHS",
    "abstract": "A common approach to define convolutions on meshes is to interpret them as a graph and apply graph convolutional networks (GCNs). Such GCNs utilize isotropic kernels and are therefore insensitive to the relative orientation of vertices and thus to the geometry of the mesh as a whole. We propose Gauge Equivariant Mesh CNNs which generalize GCNs to apply anisotropic gauge equivariant kernels. Since the resulting features carry orientation information, we introduce a geometric message passing scheme defined by parallel transporting features over mesh edges. Our experiments validate the significantly improved expressivity of the proposed model over conventional GCNs and other methods. * Equal Contribution",
    "prev": "Another promising application of PDOs is an extension to manifolds.",
    "curr": "Gauge CNNs (Cohen et al., 2019c;Kicanaoglu et al., 2019;Haan et al., 2021;) are a rather general framework for convolutions on manifolds; see Weiler et al.",
    "next": "(2021) for a thorough treatment and literature review.",
    "query": "Are there any research papers on gauge equivariant mesh CNNs for anisotropic convolutions on manifolds and geometric graphs?"
  },
  {
    "index": 1091,
    "source_corpus_id": 259138847,
    "ref_id": "b8",
    "citation_corpus_id": 247958465,
    "start": 5999,
    "end": 6002,
    "title": "MoEfication: Transformer Feed-forward Layers are Mixtures of Experts",
    "abstract": "Recent work has shown that feed-forward networks (FFNs) in pre-trained Transformers are a key component, storing various linguistic and factual knowledge.However, the computational patterns of FFNs are still unclear.In this work, we study the computational patterns of FFNs and observe that most inputs only activate a tiny ratio of neurons of FFNs.This phenomenon is similar to the sparsity of the human brain, which drives research on functional partitions of the human brain.To verify whether functional partitions also emerge in FFNs, we propose to convert a model into its MoE version with the same parameters, namely MoEfication.Specifically, MoEfication consists of two phases: (1) splitting the parameters of FFNs into multiple functional partitions as experts, and (2) building expert routers to decide which experts will be used for each input.Experimental results show that MoEfication can conditionally use 10% to 30% of FFN parameters while maintaining over 95% original performance for different models on various downstream tasks.Besides, MoEfication brings two advantages: (1) it significantly reduces the FLOPS of inference, i.e., 2x speedup with 25% of FFN parameters, and (2) it provides a fine-grained perspective to study the inner mechanism of FFNs.The source code of this paper can be obtained from https://github.com/thunlp/MoEfication.",
    "prev": "Moreover, unlike classical sparse methods where such a connection is established via explicit sparse regularization [8], the sparsity observed in Transformers is emergent without any explicit design.",
    "curr": "It is worth noting that the observation that Transformers produce sparse activations is previously reported in [9].",
    "next": "Our paper significantly extends upon results in [9] to demonstrate that sparsity emerges prevalently at all layers of Transformers, for both language and vision tasks, on both training and evaluation data, and for some architectures beyond Transformers.",
    "query": "Are there any research papers on the emergence of sparsity in Transformer feed-forward layers and the application of Mixture of Experts (MoE) for improving model efficiency?"
  },
  {
    "index": 1092,
    "source_corpus_id": 236087421,
    "ref_id": "b34",
    "citation_corpus_id": 14124313,
    "start": 5101,
    "end": 5129,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "In the MLP-Mixer , the model obtains the global receptive field through matrix transposition and token-mixing projection such that the long-range dependencies are covered.",
    "curr": "However, this rarely makes full use of the local information, which is very important in CNN-like architecture (Simonyan & Zisserman, 2015;He et al., 2016) because not all pixels need long-range dependencies, and the Therefore, a more ideal way to introduce locality is to directly model the relationship between a feature point and its surrounding feature points at any position, without the need to set a fixed window (and window size) in advance.",
    "next": "To aggregate the features of different spatial positions in the same position and model their relationships, inspired by (Wu et al., 2018;Lin et al., 2019;Wang et al., 2020;Ho et al., 2019), we propose an axial shift strategy for MLP-based architecture, where we spatially shift features in both horizontal and vertical directions.",
    "query": "Are there any research papers on methods for incorporating local information into MLP-based architectures through axial shift strategies or similar approaches inspired by CNNs?"
  },
  {
    "index": 1094,
    "source_corpus_id": 256662465,
    "ref_id": "b37",
    "citation_corpus_id": 220363897,
    "start": 2927,
    "end": 2945,
    "title": "Meta-Learning Symmetries by Reparameterization",
    "abstract": "Many successful deep learning architectures are equivariant to certain transformations in order to conserve parameters and improve generalization: most famously, convolution layers are equivariant to shifts of the input. This approach only works when practitioners know a priori symmetries of the task and can manually construct an architecture with the corresponding equivariances. Our goal is a general approach for learning equivariances from data, without needing prior knowledge of a task's symmetries or custom task-specific architectures. We present a method for learning and encoding equivariances into networks by learning corresponding parameter sharing patterns from data. Our method can provably encode equivarianceinducing parameter sharing for any finite group of symmetry transformations, and we find experimentally that it can automatically learn a variety of equivariances from symmetries in data. We provide our experiment code and pre-trained models at https://github.com/AllanYangZhou/metalearning-symmetries.",
    "prev": "If appropriate equivariance constraints are instead learned, the benefits of equivariance could extend to applications where the data may have unknown or imperfect symmetries.",
    "curr": "Learning approximate equivariance has been recently approached through novel layer operations (Wang et al., 2022;Finzi et al., 2021;Zhou et al., 2020;Yeh et al., 2022;Basu et al., 2021).",
    "next": "Separately, the field of neural architecture search (NAS) aims to optimize full neural network architectures (Zoph & Le, 2017;Real et al., 2017;Elsken et al., 2017;Liu et al., 2018;Lu et al., 2019).",
    "query": "Are there any research papers on methods for learning equivariance in neural networks from data without prior knowledge of symmetries, using reparameterization or parameter sharing techniques?"
  },
  {
    "index": 1096,
    "source_corpus_id": 250048824,
    "ref_id": "b6",
    "citation_corpus_id": 240354066,
    "start": 2154,
    "end": 2157,
    "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
    "abstract": "A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of 10000 or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model, and showed that for appropriate choices of the state matrix A, this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning A with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation 60× faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors. 1",
    "prev": "* Equal contribution.",
    "curr": "Introduction\n\nThe Structured State Space model (S4) is a recent deep learning model based on continuous-time dynamical systems that has shown promise on a wide variety of sequence modeling tasks [7].",
    "next": "It is defined as a linear time-invariant (LTI) state space model (SSM), which give it multiple properties [6]: as an SSM, S4 can be simulated as a discrete-time recurrence for efficiency in online or autoregressive settings, and as a LTI model, S4 can be converted into a convolution for parallelizability and computational efficiency at training time.",
    "query": "Are there any research papers on efficient sequence modeling techniques using structured state space models for handling long-range dependencies?"
  },
  {
    "index": 1097,
    "source_corpus_id": 225103395,
    "ref_id": "b35",
    "citation_corpus_id": 2723173,
    "start": 6570,
    "end": 6590,
    "title": "FITNETS: HINTS FOR THIN DEEP NETS",
    "abstract": "While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.",
    "prev": "Further study of accuracy and error sets have been conducted in (Hacohen & Weinshall, 2020) (error sets over training), and (Hooker et al., 2019) (error after pruning).",
    "curr": "Other work has demonstrated that it is often possible for narrower or shallower neural networks to attain similar accuracy to larger networks when the smaller networks are trained to mimic the larger networks' predictions (Ba & Caruana, 2014;Romero et al., 2015).",
    "next": "We instead seek to study the impact of width and depth on network internal representations and (per-example) outputs, by applying techniques for measuring similarity of neural network hidden representations (Kornblith et al., 2019;Raghu et al., 2017a;Morcos et al., 2018).",
    "query": "Are there any research papers on methods for training thinner and deeper neural networks using intermediate representations as hints in knowledge distillation?"
  },
  {
    "index": 1100,
    "source_corpus_id": 36483539,
    "ref_id": "b37",
    "citation_corpus_id": 12713052,
    "start": 2560,
    "end": 2577,
    "title": "NEURAL ARCHITECTURE SEARCH WITH REINFORCEMENT LEARNING",
    "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214. * Work done as a member of the Google Brain Residency program (g.co/brainresidency.) Under review as a conference paper at ICLR 2017 neural network can be typically specified by a variable-length string. It is therefore possible to use a recurrent network -the controller -to generate such string. Training the network specified by the string -the \"child network\" -on the real data will result in an accuracy on a validation set. Using this accuracy as the reward signal, we can compute the policy gradient to update the controller. As a result, in the next iteration, the controller will give higher probabilities to architectures that receive high accuracies. In other words, the controller will learn to improve its search over time.Our experiments show that Neural Architecture Search can design good models from scratch, an achievement considered not possible with other methods. On image recognition with CIFAR-10, Neural Architecture Search can find a novel ConvNet model that is better than most human-invented architectures. Our CIFAR-10 model achieves a 3.65 test set error, while being 1.05x faster than the current best model. On language modeling with Penn Treebank, Neural Architecture Search can design a novel recurrent cell that is also better than previous RNN and LSTM architectures. The cell that our model found achieves a test set perplexity of 62.4 on the Penn Treebank dataset, which is 3.6 perplexity better than the previous state-of-the-art.",
    "prev": "(2017)) is a class of approaches to reduce the size of Deep Neural Networks (DNNs) and accelerate inference.",
    "curr": "Meanwhile, Structure Learning (Zoph & Le (2017), Philipp & Carbonell (2017), Cortes et al.",
    "next": "(2017)) becomes an active research area for DNN structure exploration, which can potentially replace human labor with machine automation for design space exploration.",
    "query": "Are there any research papers on neural architecture search using reinforcement learning for automated deep neural network design?"
  },
  {
    "index": 1101,
    "source_corpus_id": 256662499,
    "ref_id": "b55",
    "citation_corpus_id": 13751870,
    "start": 2617,
    "end": 2629,
    "title": "A Call for Clarity in Reporting BLEU Scores",
    "abstract": "The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to \"the\" BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for usersupplied reference processing, and provide a new tool, SACREBLEU, 1 to facilitate this. . 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. ArXiv eprints, abs/1609.08144.",
    "prev": "INTRODUCTION\n\nTraining of deep learning models utilizes randomness to improve generalization and training efficiency, thus causing an inherent nondeterminism that hampers the reliability of machine learning evaluation -the consistency of the measurement of evaluation scores across replicated training runs.Gundersen et al.",
    "curr": "(2022) list several sources of nondeterminism, e.g., implementation-level nondeterminism such as random ordering in floating-point accumulation in parallel GPU threads (Pham et al., 2021), algorithmic factors such as variations in meta-parameters and model architecture (Lucic et al., 2018;Henderson et al., 2018;D'Amour et al., 2020), or data-level factors such as variations in pre-processing and evaluation metrics (Post, 2018;Chen et al., 2022) or varying characteristics of data in different splits (Gorman & Bedrick, 2019;Søgaard et al., 2021).Zhuang et al.",
    "next": "(2022) show that implementation-level nondeterminism is partly irreducible, leading to variability in evaluation scores even for training runs on identical data, algorithmic settings and infrastructure.Furthermore, they point out strong effects of certain types of algorithm-level nondeterminism on certain subsets of the data.",
    "query": "Are there any research papers on addressing inconsistencies in BLEU score reporting due to unreported parameters like tokenization and normalization schemes, and tools for standardized evaluation in machine translation?"
  },
  {
    "index": 1102,
    "source_corpus_id": 263333942,
    "ref_id": "b10",
    "citation_corpus_id": 13046179,
    "start": 2171,
    "end": 2197,
    "title": "A BASELINE FOR DETECTING MISCLASSIFIED AND OUT-OF-DISTRIBUTION EXAMPLES IN NEURAL NETWORKS",
    "abstract": "We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.Published as a conference paper at ICLR 2017 one method which outperforms the baseline on some (but not all) tasks. This new method evaluates the quality of a neural network's input reconstruction to determine if an example is abnormal.",
    "prev": "INTRODUCTION\n\nIn deep neural networks, out-of-distribution (OOD) detection distinguishes samples which deviate from the training distribution.Standard OOD detection concerns semantic shifts (Yang et al., 2022;Zhang et al., 2023), where OOD data is defined as test samples from semantic categories unseen during training.Ideally, the neural network should be able to reject such samples as being OOD, while still maintaining strong performance on in-distribution (ID) test samples belonging to seen training categories.",
    "curr": "Methods for detecting OOD samples work by scoring network outputs such as logits or softmax values (Hendrycks & Gimpel, 2017;Hendrycks et al., 2022), post-hoc network adjustment during inference to improve OOD scoring (Sun & Li, 2022;Sun et al., 2021;Djurisic et al., 2023), or by adjusting model training (Wei et al., 2022;Ming et al., 2023;DeVries & Taylor, 2018).These approaches can be used either independently or in conjunction with one another.Typically, post-hoc adjustments together with OOD scoring is the preferred combination since it is highly effective at discerning OOD samples with minimal ID drop and can also be applied directly to already-trained models off-the-shelf.Examples include ReAct (Sun et al., 2021), DICE (Sun & Li, 2022) and more recently, ASH (Djurisic et al., 2023).",
    "next": "On the surface, each method takes different and sometimes even contradictory approaches.ReAct rectifies penultimate activations which exceed a threshold; ASH, on the other hand, prunes penultimate activations that are too low while amplifying remaining activations.While ASH currently achieves state-of-the-art performance,",
    "query": "Are there any research papers on methods for detecting misclassified and out-of-distribution examples in neural networks using softmax probabilities or input reconstruction techniques?"
  },
  {
    "index": 1103,
    "source_corpus_id": 249712234,
    "ref_id": "b33",
    "citation_corpus_id": 6212000,
    "start": 6056,
    "end": 6076,
    "title": "UNDERSTANDING DEEP LEARNING REQUIRES RE- THINKING GENERALIZATION",
    "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.",
    "prev": "Introduction\n\nA central challenge of machine learning theory is understanding the generalization of overparameterized models.",
    "curr": "While in many real-world settings deep networks achieve low test loss, their high capacity makes theoretical analysis with classical tools difficult, or sometimes impossible (Zhang et al., 2017;Nagarajan and Kolter, 2019b).",
    "next": "Most classical theoretical tools are based on uniform convergence (UC), a property that, when it holds, guarantees that the test loss will be close to the training loss, uniformly over a class of candidate models.",
    "query": "Are there any research papers on the generalization of overparameterized deep learning models and the limitations of classical theoretical tools such as uniform convergence?"
  },
  {
    "index": 1104,
    "source_corpus_id": 67749672,
    "ref_id": "b46",
    "citation_corpus_id": 14687186,
    "start": 3842,
    "end": 3863,
    "title": "Parsing with Compositional Vector Grammars",
    "abstract": "Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness. Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%. It is fast to train and implemented approximately as an efficient reranker it is about 20% faster than the current Stanford factored parser. The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.",
    "prev": "One feature shared by many humandesigned representation systems is compositionality: the capacity to represent complex concepts (from objects to procedures to beliefs) by combining simple parts (Fodor & Lepore, 2002).",
    "curr": "While many machine learning approaches make use of human-designed compositional analyses for representation and prediction (Socher et al., 2013;Dong & Lapata, 2016), it is also natural to ask whether (and how) compositionality arises in learning problems where compositional structure has not been built in from the start.",
    "next": "Consider the example in Figure 1, which shows a hypothetical character-based encoding scheme learned for a simple communication task (similar to the one studied by Lazaridou et al., 2016).",
    "query": "Are there any studies on compositional vector grammars for natural language parsing that combine probabilistic context-free grammars with recursive neural networks to learn syntactico-semantic representations?"
  },
  {
    "index": 1105,
    "source_corpus_id": 85501306,
    "ref_id": "b19",
    "citation_corpus_id": 6628106,
    "start": 20679,
    "end": 20698,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "In each training batch, 16 low-quality (LQ) patches with the size of 48 × 48 are extracted as inputs.",
    "curr": "Our model is trained by ADAM optimizer (Kingma & Ba, 2014) with β 1 = 0.9, β 2 = 0.999, and = 10 −8 .",
    "next": "The initial learning rate is set to 10 −4 and then decreases to half every 2 × 10 5 iterations of back-propagation.",
    "query": "Are there any research papers on adaptive stochastic optimization methods like Adam, particularly their implementation details, learning rate schedules, and convergence properties in machine learning?"
  },
  {
    "index": 1107,
    "source_corpus_id": 29153681,
    "ref_id": "b14",
    "citation_corpus_id": 3431470,
    "start": 8924,
    "end": 8945,
    "title": "FEW-SHOT LEARNING WITH GRAPH NEURAL NET- WORKS",
    "abstract": "We propose to study the problem of few-shot learning with the prism of inference on a partially observed graphical model, constructed from a collection of input images whose label can be either observed or not. By assimilating generic message-passing inference algorithms with their neural-network counterparts, we define a graph neural network architecture that generalizes several of the recently proposed few-shot learning models. Besides providing improved numerical performance, our framework is easily extended to variants of few-shot learning, such as semi-supervised or active learning, demonstrating the ability of graph-based models to operate well on 'relational' tasks.",
    "prev": "Despite its simplicity, this general strategy is particularly effective and it is at the core of several stateof-the-art few-shot classification algorithms (Vinyals et al., 2016;Snell et al., 2017;Sung et al., 2018).",
    "curr": "Interestingly, Garcia & Bruna (2018) interpret learning as information propagation from support (training) to query (test) images and propose a graph neural network that can generalize matching-based approaches.",
    "next": "Since this line of work relies on learning a similarity metric, one distinctive characteristic is that parameter updates only occur within the long time horizon of the outer training loop.",
    "query": "Are there any research papers on applying graph neural networks with message-passing inference for few-shot learning tasks?"
  },
  {
    "index": 1109,
    "source_corpus_id": 232307426,
    "ref_id": "b19",
    "citation_corpus_id": 52877454,
    "start": 1868,
    "end": 1893,
    "title": "DEEP GRAPH INFOMAX",
    "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs-both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.",
    "prev": "Self-supervised learning, a subset of unsupervised learning, learns representations by allowing the data to provide supervision (Devlin et al., 2018).",
    "curr": "Among its mainstream strategies, self-supervised contrastive learning has been successful in visual object recognition (He et al., 2020;Tian et al., 2019;Chen et al., 2020c), speech recognition (Oord et al., 2018;Rivière et al., 2020), language modeling (Kong et al., 2019), graph representation learning (Velickovic et al., 2019) and reinforcement learning (Kipf et al., 2019).",
    "next": "The idea of self-supervised contrastive learning is to learn latent representations such that related instances (e.g., patches from the same image; defined as positive pairs) will have representations within close distance, while unrelated instances (e.g., patches from two different images; defined as negative pairs) will have distant representations (Arora et al., 2019).",
    "query": "Are there any research papers on self-supervised contrastive learning methods for graph representation learning that maximize mutual information between patch representations and graph summaries?"
  },
  {
    "index": 1110,
    "source_corpus_id": 244908617,
    "ref_id": "b12",
    "citation_corpus_id": 56657912,
    "start": 25448,
    "end": 25478,
    "title": "BENCHMARKING NEURAL NETWORK ROBUSTNESS TO COMMON CORRUPTIONS AND PERTURBATIONS",
    "abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, IMAGENET-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called IMAGENET-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.",
    "prev": "First, we learn pixel classifiers on the clean images using the DDPM, SwAV and MAE representations on the Bedroom-28 and Horse-21 datasets.",
    "curr": "Then, 18 diverse corruption types, adopted from (Hendrycks & Dietterich, 2019), are applied to test images.",
    "next": "Each corruption has five levels of severity.",
    "query": "Are there any research papers on benchmarking neural network robustness to common corruptions and perturbations in image classification tasks?"
  },
  {
    "index": 1112,
    "source_corpus_id": 238531695,
    "ref_id": "b23",
    "citation_corpus_id": 52967399,
    "start": 2372,
    "end": 2392,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": " hand, CEL blends each embedding with multiple patches of different scales, providing the self-attention module itself with cross-scale features.On the other hand, LSDA splits the self-attention module into a short-distance one and a long-distance counterpart, which not only reduces the computational burden but also keeps both small-scale and large-scale features in the embeddings.Through the above two designs, we achieve cross-scale attention.Besides, we put forward a dynamic position bias for vision transformers to make the popular relative position bias apply to variable-sized images.Hinging on the cross-scale attention module, we construct a versatile vision architecture, dubbed CrossFormer, which accommodates variable-sized inputs.Extensive experiments show that CrossFormer outperforms the other vision transformers on image classification, object detection, instance segmentation, and semantic segmentation tasks.",
    "curr": "1\n\nINTRODUCTION\n\nIt turns out that transformer (Vaswani et al., 2017;Devlin et al., 2019;Brown et al., 2020) has achieved great success in the field of natural language processing (NLP).Benefitting from its selfattention module, transformer is born with the key ability to build long-distance dependencies.Since long-distance dependencies are also needed by a number of vision tasks (Zhang & Yang, 2021;Chu et al., 2021), a surge of research work (Dosovitskiy et al., 2021;Touvron et al., 2021;Wang et al., 2021) has been conducted to explore various transformer-based vision architectures.",
    "next": "A transformer requires a sequence of embeddings2 (e.g., word embeddings) as input.To adapt this requirement to typical vision tasks, most existing vision transformers (Dosovitskiy et al., 2021;Touvron et al., 2021;Wang et al., 2021;Liu et al., 2021b) produce embeddings by splitting an input image into equal-sized patches.For example, a 224 × 224 image can be split into 56 × 56 patches of size 4 × 4, and these patches are projected through a linear layer to yield an embedding sequence.Inside a",
    "query": "Are there any research papers on vision transformers with cross-scale attention mechanisms and dynamic position bias for variable-sized inputs?"
  },
  {
    "index": 1113,
    "source_corpus_id": 263830433,
    "ref_id": "b15",
    "citation_corpus_id": 233296808,
    "start": 4926,
    "end": 4947,
    "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
    "abstract": "In this work, we explore \"prompt tuning,\" a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \"prompt ensembling.\" We release code and model checkpoints to reproduce our experiments. 1ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,  Danilo Giampiccolo, Bernardo Magnini, and Idan  Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6-4. Venice. . 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
    "prev": "ext descriptions of the desired tasks that are often significantly easier to obtain compared to full model weights or even a generic linear classifier over the last layer.The versatility and performance of prompting Table 1: Comparison with existing state-of-the-art generalization bounds for test error on different datasets.We report both data-independent and data-dependent bounds (⋆ indicates data-dependent prior and − indicates that the bounds are not available).Note that different works use different architectures and analytic tools so direct comparison can be more nuanced.Nonetheless, our bounds on prompt engineering are significantly tighter than the existing PAC-Bayes bounds in the literature, often within a few percent of the actual test error.",
    "curr": "Dataset\n\nZhou pretrained models have led to the rise of prompt engineering, an emergent paradigm in machine learning where practitioners carefully design the task specification in text or even learn the prompts in a data-driven fashion (Lester et al., 2021).For example, to obtain a two-class image classifier, one would write two sentences that describe the classes (e.g., \"This is a dog\" and \"This is a cat\"), and the two sentences are turned into text embeddings which can be used to classify image embeddings.Despite its empirical success, little is understood of how and why prompting these pretrained models work and, in particular, why the method seems to suffer little from overfitting: manually tuning or even greedily optimizing prompts on a given training set often performs nearly as well on the corresponding test set.",
    "next": "In this paper, we demonstrate that rather simple analysis tools capture this behavior surprisingly well (under some assumptions).In particular, we show that classical PAC-Bayes bounds (McAllester, 1999), when applied to the discrete hypothesis class defined by prompts (and specifically with a prior given by a large language model), are often remarkably tight, even for large domains: for example, we achieve a generalizati",
    "query": "Are there any research papers on parameter-efficient prompt tuning methods for frozen language models and their generalization performance with scaling, particularly using PAC-Bayes analysis?"
  },
  {
    "index": 1114,
    "source_corpus_id": 254408645,
    "ref_id": "b31",
    "citation_corpus_id": 203139700,
    "start": 38747,
    "end": 38763,
    "title": "Texar: A Modularized, Versatile, and Extensible Toolkit for Text Generation",
    "abstract": "We introduce Texar, an open-source toolkit aiming to support the broad set of text generation tasks that transform any inputs into natural language, such as machine translation, summarization, dialog, content manipulation, and so forth. With the design goals of modularity, versatility, and extensibility in mind, Texar extracts common patterns underlying the diverse tasks and methodologies, creates a library of highly reusable modules and functionalities, and allows arbitrary model architectures and algorithmic paradigms. In Texar, model architecture, inference, and learning processes are properly decomposed. Modules at a high concept level can be freely assembled or plugged in/swapped out. Texar is thus particularly suitable for researchers and practitioners to do fast prototyping and experimentation. The versatile toolkit also fosters technique sharing across different text generation tasks. Texar supports both TensorFlow and PyTorch, and is released under Apache License 2.0 at https: //www.texar.io. 1 1 An expanded version of the tech report can be found at https://arxiv.org/abs/1809.00794",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any tools or resources for text generation that provide modularity, versatility, and extensibility in model design and implementation?"
  },
  {
    "index": 1115,
    "source_corpus_id": 3464416,
    "ref_id": "b12",
    "citation_corpus_id": 6628106,
    "start": 29143,
    "end": 29162,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "Dropout was applied to only fully-connected layers of the network with the probability of retaining the unit being 0.5.",
    "curr": "For all experiments, the softmax classifier is used, and each model is trained by optimizing the objective function using Adam learning rule (Kingma & Ba, 2014).",
    "next": "For each out-ofdistribution dataset, we randomly select 1,000 images for tuning the penalty parameter β, mini-batch size and learning rate.",
    "query": "Are there any research papers on Adam optimization methods for stochastic optimization in machine learning?"
  },
  {
    "index": 1116,
    "source_corpus_id": 252693131,
    "ref_id": "b22",
    "citation_corpus_id": 209439843,
    "start": 2844,
    "end": 2866,
    "title": "MEASURING COMPOSITIONAL GENERALIZATION: A COMPREHENSIVE METHOD ON REALISTIC DATA",
    "abstract": "State-of-the-art machine learning methods exhibit limited compositional generalization. At the same time, there is a lack of realistic benchmarks that comprehensively measure this ability, which makes it challenging to find and evaluate improvements. We introduce a novel method to systematically construct such benchmarks by maximizing compound divergence while guaranteeing a small atom divergence between train and test sets, and we quantitatively compare this method to other approaches for creating compositional generalization benchmarks. We present a large and realistic natural language question answering dataset that is constructed according to this method, and we use it to analyze the compositional generalization ability of three machine learning architectures. We find that they fail to generalize compositionally and that there is a surprisingly strong negative correlation between compound divergence and accuracy. We also demonstrate how our method can be used to create new compositionality benchmarks on top of the existing SCAN dataset, which confirms these findings. Radev. Improving text-to-SQL evaluation methodology. In ACL, 2018. URL http://aclweb.org/anthology/P18-1033.Jerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2):3-71, 1988. URL https://pdfs.semanticscholar.org/d806/ 76034bfabfea59f35698af0f715a555fcf50.pdf.",
    "prev": "This type of compositionality is central to the human ability to generalize from limited data to novel combinations (Lake et al., 2017).",
    "curr": "Recently, several datasets have been proposed to test systematic generalization of machine learning models-SCAN (Lake & Baroni, 2018), PCFG (Hupkes et al., 2020), CFQ (Keysers et al., 2020), and HINT (Li et al., 2021), to name a few.",
    "next": "While conventional neural networks fail dramatically on these datasets, certain inductive biases have been explored to improve systematic generalization.",
    "query": "Are there any research papers on methods for measuring compositional generalization in machine learning using realistic benchmarks and datasets?"
  },
  {
    "index": 1120,
    "source_corpus_id": 256390109,
    "ref_id": "b26",
    "citation_corpus_id": 247244493,
    "start": 2830,
    "end": 2848,
    "title": "BETTER SUPERVISORY SIGNALS BY OBSERVING LEARNING PATHS",
    "abstract": "Better-supervised models might have better performance. In this paper, we first clarify what makes for good supervision for a classification problem, and then explain two existing label refining methods, label smoothing and knowledge distillation, in terms of our proposed criterion. To further answer why and how better supervision emerges, we observe the learning path, i.e., the trajectory of the model's predictions during training, for each training sample. We find that the model can spontaneously refine \"bad\" labels through a \"zig-zag\" learning path, which occurs on both toy and real datasets. Observing the learning path not only provides a new perspective for understanding knowledge distillation, overfitting, and learning dynamics, but also reveals that the supervisory signal of a teacher network can be very unstable near the best points in training on real tasks. Inspired by this, we propose a new knowledge distillation scheme, Filter-KD, which improves downstream classification performance in various settings.",
    "prev": "Simultaneously, several works have aimed to rigorously formalize why KD can improve the student model performance.",
    "curr": "Some prominent observations from this line of work are that (self-)distillation induces certain favorable optimization biases in the training objective [Phuong andLampert, 2019, Ji andZhu, 2020], lowers variance of the objective [Menon et al., 2021, Dao et al., 2021, Ren et al., 2022, increases regularization towards learning \"simpler\" functions [Mobahi et al., 2020], transfers information from different data views [Allen-Zhu and Li, 2020], and scales per-example gradients based on the teacher's confidence [Furlanello et al., 2018, Tang et al., 2020.",
    "next": "Despite this remarkable progress, there are still many open problems and unexplained phenomena around knowledge distillation; to name a few: -Why do soft labels (sometimes) help?",
    "query": "Are there any studies that investigate the mechanisms behind knowledge distillation and the role of supervisory signals in improving model performance through learning path analysis?"
  },
  {
    "index": 1121,
    "source_corpus_id": 221818900,
    "ref_id": "b24",
    "citation_corpus_id": 49882757,
    "start": 1897,
    "end": 1915,
    "title": "ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech",
    "abstract": "In this work, we propose an alternative solution for parallel wave generation by WaveNet. In contrast to parallel WaveNet (Oord et al., 2018), we distill a Gaussian inverse autoregressive flow from the autoregressive WaveNet by minimizing a novel regularized KL divergence between their highly-peaked output distributions. Our method computes the KL divergence in closed-form, which simplifies the training algorithm and provides very efficient distillation. In addition, we propose the first text-to-wave neural architecture for speech synthesis, which is fully convolutional and enables fast end-to-end training from scratch. It significantly outperforms the previous pipeline that connects a text-to-spectrogram model to a separately trained WaveNet (Ping et al., 2018). We also successfully distill a parallel waveform synthesizer conditioned on the hidden representation in this end-to-end model. 2 * These authors contributed equally to this work. Correspondence to <weiping.thu@gmail.com>. Our method is named after the musical instrument clarinet, whose sound resembles human voice.2 Audio samples are in https://clarinet-demo.github.io/ arXiv:1807.07281v2 [cs.CL] 30 Jul 2018",
    "prev": "In previous work, likelihood-based models, including autoregressive models (van den Oord et al., 2016;Kalchbrenner et al., 2018; and flow-based models Prenger et al., 2019;, have predominated in audio synthesis because of the simple training objective and superior ability of modeling the fine details of waveform in real data.",
    "curr": "There are other waveform models, which often require auxiliary losses for training, such as flow-based models trained by distillation Ping et al., 2019), variational auto-encoder (VAE) based model , and generative adversarial network (GAN) based models Yamamoto et al., 2020).",
    "next": "Most of previous waveform models focus on audio synthesis with informative local conditioner (e.g., mel spectrogram or aligned linguistic features), with only a few exceptions in unconditional generation .",
    "query": "Are there any research papers on parallel wave generation in end-to-end text-to-speech using distillation techniques or alternative models to autoregressive approaches?"
  },
  {
    "index": 1124,
    "source_corpus_id": 247996510,
    "ref_id": "b4",
    "citation_corpus_id": 235368289,
    "start": 4765,
    "end": 4786,
    "title": "Measuring and Improving BERT's Mathematical Abilities by Predicting the Order of Reasoning",
    "abstract": "Imagine you are in a supermarket. You have two bananas in your basket and want to buy four apples. How many fruits do you have in total? This seemingly straightforward question can be challenging for data-driven language models, even if trained at scale. However, we would expect such generic language models to possess some mathematical abilities in addition to typical linguistic competence. Towards this goal, we investigate if a commonly used language model, BERT, possesses such mathematical abilities and, if so, to what degree. For that, we fine-tune BERT on a popular dataset for word math problems, AQuA-RAT, and conduct several tests to understand learned representations better. Since we teach models trained on natural language to do formal mathematics, we hypothesize that such models would benefit from training on semi-formal steps that explain how math results are derived. To better accommodate such training, we also propose new pretext tasks for learning mathematical rules. We call them (Neighbor) Reasoning Order Prediction (ROP or NROP). With this new model, we achieve significantly better outcomes than data-driven baselines and even on-par with more tailored models. We also show how to reduce positional bias in such models.",
    "prev": "(2020) showed that SATNet (Wang et al., 2019) could not solve visual Sudoku without using intermediate labels to identify individual Sudoku digit images.",
    "curr": "Similar limitations were observed in language related compounded tasks, including commonsense reasoning (Liu et al., 2022;Wei et al., 2022;Zelikman et al., 2022), math word problems (Piękos et al., 2021;Wei et al., 2022), and programs execution (Nye et al., 2022).",
    "next": "The go-to architectures in this domain are powerful language models, which are trained as sequence-to-sequence models over text.",
    "query": "Are there any research papers on improving language models' mathematical reasoning abilities through sequence-to-sequence training and pretext tasks focused on reasoning order prediction?"
  },
  {
    "index": 1128,
    "source_corpus_id": 236087352,
    "ref_id": "b33",
    "citation_corpus_id": 227054483,
    "start": 2817,
    "end": 2836,
    "title": "PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING",
    "abstract": "Reinforcement learning provides a general framework for flexible decision making and control, but requires extensive data collection for each new task that an agent needs to learn. In other machine learning fields, such as natural language processing or computer vision, pre-training on large, previously collected datasets to bootstrap learning for new tasks has emerged as a powerful paradigm to reduce data requirements when learning a new task. In this paper, we ask the following question: how can we enable similarly useful pre-training for RL agents? We propose a method for pre-training behavioral priors that can capture complex input-output relationships observed in successful trials from a wide range of previously seen tasks, and we show how this learned prior can be used for rapidly learning new tasks without impeding the RL agent's ability to try out novel behaviors. We demonstrate the effectiveness of our approach in challenging robotic manipulation domains involving image observations and sparse reward functions, where our method outperforms prior works by a substantial margin. Additional materials can be found on our project website: https://sites.google.com/view/parrot-rl.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on data-driven pre-training methods for reinforcement learning that leverage behavioral priors from diverse tasks to improve learning efficiency and reduce data requirements?"
  },
  {
    "index": 1129,
    "source_corpus_id": 257757426,
    "ref_id": "b20",
    "citation_corpus_id": 238634325,
    "start": 7554,
    "end": 7578,
    "title": "OFFLINE REINFORCEMENT LEARNING WITH IMPLICIT Q-LEARNING",
    "abstract": "Offline reinforcement learning requires reconciling two conflicting aims: learning a policy that improves over the behavior policy that collected the dataset, while at the same time minimizing the deviation from the behavior policy so as to avoid errors due to distributional shift. This trade-off is critical, because most current offline reinforcement learning methods need to query the value of unseen actions during training to improve the policy, and therefore need to either constrain these actions to be in-distribution, or else regularize their values. We propose a new offline RL method that never needs to evaluate actions outside of the dataset, but still enables the learned policy to improve substantially over the best behavior in the data through generalization. The main insight in our work is that, instead of evaluating unseen actions from the latest policy, we can approximate the policy improvement step implicitly by treating the state value function as a random variable, with randomness determined by the action (while still integrating over the dynamics to avoid excessive optimism), and then taking a state conditional upper expectile of this random variable to estimate the value of the best actions in that state. This leverages the generalization capacity of the function approximator to estimate the value of the best available action at a given state without ever directly querying a Q-function with this unseen action. Our algorithm alternates between fitting this upper expectile value function and backing it up into a Q-function, without any explicit policy. Then, we extract the policy via advantage-weighted behavioral cloning, which also avoids querying out-of-sample actions. We dub our method implicit Q-learning (IQL). IQL is easy to implement, computationally efficient, and only requires fitting an additional critic with an asymmetric L2 loss. 1 IQL demonstrates the state-of-the-art performance on D4RL, a standard benchmark for offline reinforcement learning. We also demonstrate that IQL achieves strong performance fine-tuning using online interaction after offline initialization.",
    "prev": "This setting is particularly attractive for applications where there is previous logged experience available but online data collection is expensive (e.g., robotics, healthcare).",
    "curr": "Recently, the field of offline RL has made significant progress, and many offline RL algorithms have been proposed to learn improved policies from diverse and sub-optimal offline demonstrations Fujimoto & Gu, 2021;Kostrikov et al., 2022c;.",
    "next": "Offline RL research typically assumes that the offline dataset is reward-annotated.",
    "query": "Are there any research papers on offline reinforcement learning methods that avoid evaluating out-of-distribution actions and improve policies through implicit Q-learning techniques?"
  },
  {
    "index": 1130,
    "source_corpus_id": 239769065,
    "ref_id": "b30",
    "citation_corpus_id": 3687922,
    "start": 2541,
    "end": 2544,
    "title": "SEMI-PARAMETRIC TOPOLOGICAL MEMORY FOR NAVIGATION",
    "abstract": "We introduce a new memory architecture for navigation in previously unseen environments, inspired by landmark-based navigation in animals. The proposed semiparametric topological memory (SPTM) consists of a (non-parametric) graph with nodes corresponding to locations in the environment and a (parametric) deep network capable of retrieving nodes from the graph based on observations. The graph stores no metric information, only connectivity of locations corresponding to the nodes. We use SPTM as a planning module in a navigation system. Given only 5 minutes of footage of a previously unseen maze, an SPTM-based navigation agent can build a topological map of the environment and use it to confidently navigate towards goals. The average success rate of the SPTM agent in goal-directed navigation across test environments is higher than the best-performing baseline by a factor of three.",
    "prev": "Some prior methods approach this problem by performing search or optimization over subgoals at test time.",
    "curr": "However, these test-time planning methods either rely on graph search [7,31], which scales poorly with dimensionality [14], or continuous optimization over subgoals [26], which is expensive and can result in model exploitation.",
    "next": "In this paper, we take a different tack and instead use search at training time to automatically generate a curriculum.",
    "query": "Are there any research papers on topological memory architectures for navigation in unseen environments that use training-time search to generate curricula?"
  },
  {
    "index": 1131,
    "source_corpus_id": 248810913,
    "ref_id": "b30",
    "citation_corpus_id": 108300988,
    "start": 2602,
    "end": 2624,
    "title": "WHAT DO YOU LEARN FROM CONTEXT? PROBING FOR SENTENCE STRUCTURE IN CONTEXTUALIZED WORD REPRESENTATIONS",
    "abstract": "Contextualized representation models such as ELMo(Peters et al., 2018a)andBERT (Devlin et al., 2018)have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.",
    "prev": "A large number of studies have been devoted towards interpreting DNNs.",
    "curr": "A major line of research work has focused on DNNs in interpreting deep Natural Language Processing (NLP) models and their ability to learn various pre-defined linguistic concepts (Tenney et al., 2019b;Liu et al., 2019a).",
    "next": "More specifically, they rely on pre-defined linguistic concepts such as: parts-of-speech tags and semantic tags, and probe whether the specific linguistic knowledge is learned in various parts of the network.",
    "query": "Are there any research papers on methods for probing contextualized word representations to analyze syntactic and semantic structure in deep NLP models?"
  },
  {
    "index": 1132,
    "source_corpus_id": 52881014,
    "ref_id": "b25",
    "citation_corpus_id": 10082291,
    "start": 14948,
    "end": 14975,
    "title": "Fast Adaptation in Generative Models with Generative Matching Networks",
    "abstract": "Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples. We develop a new generative model called Generative Matching Network which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks. By conditioning on the additional input dataset, our model can instantly learn new concepts that were not available in the training data but conform to a similar generative process. The proposed framework does not explicitly restrict diversity of the conditioning data and also does not require an extensive inference procedure for training or adaptation. Our experiments on the Omniglot dataset demonstrate that Generative Matching Networks significantly improve predictive performance on the fly as more additional data is available and outperform existing state of the art conditional generative models.",
    "prev": "This method has shown promise in robotics (Finn et al., 2017b;Yu et al., 2018).",
    "curr": "In generative modeling, few-shot learning has been addressed from several perspectives, including matching networks (Bartunov and Vetrov, 2017) and variable inference for memory addressing (Bornschein et al., 2017).",
    "next": "Rezende et al.",
    "query": "Are there any research papers on generative models with few-shot learning techniques for fast adaptation using Generative Matching Networks?"
  },
  {
    "index": 1133,
    "source_corpus_id": 1091965,
    "ref_id": "b22",
    "citation_corpus_id": 14124313,
    "start": 2130,
    "end": 2158,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "(2012) developed AlexNet, an ImageNet-winning CNN with more than 1.1 × 10 9 multiplies.",
    "curr": "In 2014, ImageNetwinning and runner up CNNs increased the number of multiplies to 1.4 × 10 9 (Szegedy et al., 2015) and 1.6 × 10 10 (Simonyan & Zisserman, 2015) respectively.",
    "next": "Despite the powerful representational ability of large scale CNNs, their computational workload prohibits deployment on mobile devices.",
    "query": "Are there any research papers on deep convolutional neural networks for image recognition and their computational efficiency in mobile deployment?"
  },
  {
    "index": 1134,
    "source_corpus_id": 218901036,
    "ref_id": "b9",
    "citation_corpus_id": 6706414,
    "start": 2472,
    "end": 2476,
    "title": "Published as a conference paper at ICLR 2015 EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES",
    "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
    "prev": "Motivation and Contributions\n\nThe outputs of deep networks are known to be very sensitive to small perturbations to the input.",
    "curr": "This sensitivity can be exploited to create adversarial examples that undermine robustness by causing trained networks to produce defective results from input changes that are imperceptible to a human [10].",
    "next": "The adversarial scenarios studied in this paper are primarily untargeted white-box attacks on image classification networks.",
    "query": "Are there any research papers on adversarial examples in image classification and techniques for improving neural network robustness through adversarial training?"
  },
  {
    "index": 1135,
    "source_corpus_id": 237593074,
    "ref_id": "b10",
    "citation_corpus_id": 225039882,
    "start": 7314,
    "end": 7339,
    "title": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.RELATED WORKTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.",
    "prev": "Architecture We use an encoder-decoder architecture.",
    "curr": "The encoder can be a general image encoder that perceives pixels and encodes them into hidden representations, such as a ConvNet (LeCun et al., 1989;Krizhevsky et al., 2012;He et al., 2016), Transformer (Vaswani et al., 2017;Dosovitskiy et al., 2020), or their combination (Carion et al., 2020).",
    "next": "For generation we use a Transformer decoder, widely used in modern language modeling (Radford et al., 2018;Raffel et al., 2019).",
    "query": "Are there any research papers on using transformer-based encoder-decoder architectures for image recognition tasks, particularly those involving pre-training on large-scale datasets and the application of image patches?"
  },
  {
    "index": 1138,
    "source_corpus_id": 235358707,
    "ref_id": "b14",
    "citation_corpus_id": 4009713,
    "start": 1860,
    "end": 1881,
    "title": "Published as a conference paper at ICLR 2018 UNSUPERVISED REPRESENTATION LEARNING BY PRE- DICTING IMAGE ROTATIONS",
    "abstract": "Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training Con-vNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet.",
    "prev": "The source code is available at https://github.com/tsaishien-chen/IFND.",
    "curr": "INTRODUCTION\n\nSelf-supervised learning of visual representation (Doersch et al., 2015;Pathak et al., 2016;Noroozi & Favaro, 2016;Oord et al., 2018;Gidaris et al., 2018;Chen et al., 2020a) aims to learn a semanticaware embedding space based on the image data without the supervision of human-labeled annotations.",
    "next": "Recently, significant advances have been made by contrastive learning approaches (Hjelm et al., 2019;Tian et al., 2019;Chen et al., 2020b;He et al., 2020;Chen et al., 2020d) to reduce the performance gap with the supervised counterparts.",
    "query": "Are there any research papers on unsupervised representation learning using image rotation prediction as a self-supervised task?"
  },
  {
    "index": 1139,
    "source_corpus_id": 248377648,
    "ref_id": "b11",
    "citation_corpus_id": 28202810,
    "start": 33316,
    "end": 33340,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "In our experiments we use expert policies to collect offline datasets for the pre-training tasks of standing, forward walking, and backward walking respectively.",
    "curr": "To collect these datasets, we pre-train polices with SAC (Haarnoja et al., 2018a) in the state space and collect rollouts of the visual observation.",
    "next": "We test our representation on the downstream task of \"running\".",
    "query": "Are there any research papers on using soft actor-critic algorithms for pre-training in motor tasks with offline datasets and evaluating performance on downstream locomotion tasks?"
  },
  {
    "index": 1140,
    "source_corpus_id": 210932183,
    "ref_id": "b31",
    "citation_corpus_id": 12713052,
    "start": 1608,
    "end": 1625,
    "title": "NEURAL ARCHITECTURE SEARCH WITH REINFORCEMENT LEARNING",
    "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214. * Work done as a member of the Google Brain Residency program (g.co/brainresidency.) Under review as a conference paper at ICLR 2017 neural network can be typically specified by a variable-length string. It is therefore possible to use a recurrent network -the controller -to generate such string. Training the network specified by the string -the \"child network\" -on the real data will result in an accuracy on a validation set. Using this accuracy as the reward signal, we can compute the policy gradient to update the controller. As a result, in the next iteration, the controller will give higher probabilities to architectures that receive high accuracies. In other words, the controller will learn to improve its search over time.Our experiments show that Neural Architecture Search can design good models from scratch, an achievement considered not possible with other methods. On image recognition with CIFAR-10, Neural Architecture Search can find a novel ConvNet model that is better than most human-invented architectures. Our CIFAR-10 model achieves a 3.65 test set error, while being 1.05x faster than the current best model. On language modeling with Penn Treebank, Neural Architecture Search can design a novel recurrent cell that is also better than previous RNN and LSTM architectures. The cell that our model found achieves a test set perplexity of 62.4 on the Penn Treebank dataset, which is 3.6 perplexity better than the previous state-of-the-art.",
    "prev": "To showcase the framework, we compare several state-of-the-art one-shot NAS methods, examine how sensitive they are to their hyperparameters and how they can be improved by tuning their hyperparameters, and compare their performance to that of blackbox optimizers for NAS-Bench-101.",
    "curr": "INTRODUCTION\n\nWhile neural architecture search (NAS) has attracted a lot of attention due to the effectiveness in automatically designing state-of-the-art neural networks (Zoph & Le, 2017;Real et al., 2017;, the focus has recently shifted to making the search process more efficient (Pham et al., 2018;Elsken et al., 2019;Xie et al., 2019;Cai et al., 2019;Casale et al., 2019).",
    "next": "The most crucial concept which led to a reduction in search costs to the order of a single function evaluation is certainly the weight-sharing paradigm: Training only a single large architecture (the one-shot model) subsuming all the possible architectures in the search space (Brock et al., 2018;Pham et al., 2018).",
    "query": "Are there any research papers on neural architecture search using reinforcement learning with recurrent networks for generating neural network models in image and language tasks?"
  },
  {
    "index": 1141,
    "source_corpus_id": 49667227,
    "ref_id": "b66",
    "citation_corpus_id": 22014305,
    "start": 41491,
    "end": 41494,
    "title": "ROUTING NETWORKS: ADAPTIVE SELECTION OF NON-LINEAR FUNCTIONS FOR MULTI-TASK LEARN- ING",
    "abstract": "Multi-task learning (MTL) with neural networks leverages commonalities in tasks to improve performance, but often suffers from task interference which reduces the benefits of transfer. To address this issue we introduce the routing network paradigm, a novel neural network and training algorithm. A routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks. A function block may be any neural network -for example a fully-connected or a convolutional layer. Given an input the router makes a routing decision, choosing a function block to apply and passing the output back to the router recursively, terminating when a fixed recursion depth is reached. In this way the routing network dynamically composes different function blocks for each input. We employ a collaborative multi-agent reinforcement learning (MARL) approach to jointly train the router and function blocks. We evaluate our model against cross-stitch networks and shared-layer baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a significant improvement in accuracy, with sharper convergence. In addition, routing networks have nearly constant per-task training cost while cross-stitch networks scale linearly with the number of tasks. On CIFAR-100 (20 tasks) we obtain cross-stitch performance levels with an 85% reduction in training time.",
    "prev": "Finding such structure is important for generalization because it shields various parts of the representation or program from changing when one component changes.",
    "curr": "For example, [24,42,67] have begun to examine how to route through an fixed architecture to encapsulate computation, but the hard limitation that prevents fixed architectures from scaling to problems with complexity beyond that which they were trained is their limited number of computation steps.",
    "next": "A parallel line of work runs through the hierarchical reinforcement learning (HRL) literature [9], with recent work [8,27,50,58,84] attempting to learn both lower-level policies as well as a higher-level policy that calls them.",
    "query": "Are there any research papers on adaptive routing mechanisms for multi-task learning using reinforcement learning approaches?"
  },
  {
    "index": 1142,
    "source_corpus_id": 219792972,
    "ref_id": "b24",
    "citation_corpus_id": 3568073,
    "start": 7236,
    "end": 7240,
    "title": "PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION",
    "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 1024 2 . We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.",
    "prev": "MSGAN (aka Miss-GAN) (23) aims to solve the missing mode problem of conditional GANs through a regularization term that maximizes the distance between the generated images with respect to the distance between their corresponding input latent codes.",
    "curr": "Progressive GANs (24) are growing both the generator and discriminator progressively, and approach resembling the layer-wise training of autoencoders.",
    "next": "Method\n\n\nPreliminaries\n\nWe define an N -way, K (tr) -shot supervised classification task, T , as a set D (tr) T composed of i ∈ {1, .",
    "query": "Are there any research papers on progressive training methodologies for GANs to enhance image quality, stability, and variation in generated outputs?"
  },
  {
    "index": 1146,
    "source_corpus_id": 258988005,
    "ref_id": "b11",
    "citation_corpus_id": 237605600,
    "start": 7790,
    "end": 7806,
    "title": "Published as a conference paper at ICLR 2022 SQRT(D) DIMENSION DEPENDENCE OF LANGEVIN MONTE CARLO",
    "abstract": "This article considers the popular MCMC method of unadjusted Langevin Monte Carlo (LMC) and provides a non-asymptotic analysis of its sampling error in 2-Wasserstein distance. The proof is based on a refinement of mean-square analysis in Li et al.(2019), and this refined framework automates the analysis of a large class of sampling algorithms based on discretizations of contractive SDEs. Using this framework, we establish an O √ d / mixing time bound for LMC, without warm start, under the common log-smooth and log-strongly-convex conditions, plus a growth condition on the 3rd-order derivative of the potential of target measures. This bound improves the best previously known O d / result and is optimal (in terms of order) in both dimension d and accuracy tolerance for target measures satisfying the aforementioned assumptions. Our theoretical analysis is further validated by numerical experiments.",
    "prev": "However, we have been particularly motivated by the research on Langevin MCMC which is a class of gradientbased sampling algorithms obtained by discretizing the Langevin diffusion (Parisi, 1981).",
    "curr": "There is a growing body of work on the analysis of Langevin MCMC algorithms of various complexity (overdampled, Metropolis-adjusted, underdamped, higher-order) for sampling from log-concave distributions (Dalalyan, 2017;Durmus & Moulines, 2017;Cheng et al., 2018;Dwivedi et al., 2018;Shen & Lee, 2019;Cao et al., 2020;Mou et al., 2021;Li et al., 2022).",
    "next": "In our experiments, we compare several recent Langevin MCMC methods, which may be of independent interest.",
    "query": "Are there any research papers on the non-asymptotic analysis of Langevin Monte Carlo methods and their dimension dependence in sampling from log-concave distributions?"
  },
  {
    "index": 1151,
    "source_corpus_id": 227054483,
    "ref_id": "b6",
    "citation_corpus_id": 52967399,
    "start": 2962,
    "end": 2983,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "In other fields of machine learning, the use of large prior datasets to bootstrap acquisition of new capabilities has been studied extensively to good effect.",
    "curr": "For example, language models trained on large, diverse datasets offer representations that drastically improve the efficiency of learning downstream tasks (Devlin et al., 2019).",
    "next": "What would be the analogue of this kind of pre-training in robotics and RL?",
    "query": "Are there any research papers on pre-training methods in robotics and reinforcement learning analogous to those used in language models like BERT?"
  },
  {
    "index": 1152,
    "source_corpus_id": 238531427,
    "ref_id": "b0",
    "citation_corpus_id": 7167114,
    "start": 10161,
    "end": 10180,
    "title": "DEEP VARIATIONAL INFORMATION BOTTLENECK",
    "abstract": "We present a variational approximation to the information bottleneck ofTishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method \"Deep Variational Information Bottleneck\", or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on variational information bottleneck methods applied in deep learning for improving model robustness and generalization?"
  },
  {
    "index": 1153,
    "source_corpus_id": 235390683,
    "ref_id": "b27",
    "citation_corpus_id": 3292002,
    "start": 2577,
    "end": 2601,
    "title": "GRAPH ATTENTION NETWORKS",
    "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-theart results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a proteinprotein interaction dataset (wherein test graphs remain unseen during training).",
    "prev": "Due to their prevalence and rich descriptive capacity, pattern mining and discovery on graph data is a prominent research area with powerful implications.",
    "curr": "As the generalization of deep neural networks on graph data, graph neural networks (GNNs) have proved to be powerful in learning representations for graphs and associated entities (nodes, edges, subgraphs), and they have been employed in various applications such as node classification (Kipf & Welling, 2016a;Veličković et al., 2018), node clustering (Pan et al., 2018), recommender systems (Ying et al., 2018) and drug discovery (Duvenaud et al., 2015).",
    "next": "In recent years, the explosive interest in self-supervised learning (SSL) has suggested its great potential in empowering stronger neural networks in an unsupervised manner Kolesnikov et al., 2019;Doersch et al., 2015).",
    "query": "Are there any research papers on graph attention networks and their use of self-attention mechanisms for learning node and graph representations in tasks like node classification, clustering, and drug discovery?"
  },
  {
    "index": 1154,
    "source_corpus_id": 244773230,
    "ref_id": "b17",
    "citation_corpus_id": 204206524,
    "start": 35508,
    "end": 35534,
    "title": "Published as a conference paper at ICLR 2020 DYNAMICAL DISTANCE LEARNING FOR SEMI-SUPERVISED AND UNSUPERVISED SKILL DISCOVERY",
    "abstract": "Reinforcement learning requires manual specification of a reward function to learn a task. While in principle this reward function only needs to specify the task goal, in practice reinforcement learning can be very time-consuming or even infeasible unless the reward function is shaped so as to provide a smooth gradient towards a successful outcome. This shaping is difficult to specify by hand, particularly when the task is learned from raw observations, such as images. In this paper, we study how we can automatically learn dynamical distances: a measure of the expected number of time steps to reach a given goal state from any other state. These dynamical distances can be used to provide well-shaped reward functions for reaching new goals, making it possible to learn complex tasks efficiently. We show that dynamical distances can be used in a semi-supervised regime, where unsupervised interaction with the environment is used to learn the dynamical distances, while a small amount of preference supervision is used to determine the task goal, without any manually engineered reward function or goal examples. We evaluate our method both on a real-world robot and in simulation. We show that our method can learn to turn a valve with a real-world 9-DoF hand, using raw image observations and just ten preference labels, without any other supervision. Videos of the learned skills can be found on the project website: https://sites.google.com/view/dynamical-distance-learning.",
    "prev": "In the future, we would like to extend this work to multi-task settings, employ vision and move towards the context of batch RL, e.g.",
    "curr": "through employing distance learning techniques (Hartikainen et al., 2020) to build on more informative goal spaces.",
    "next": "We measure the performance of the learnt representations to encode progress by running a KNN classification.",
    "query": "Are there any research papers on dynamical distance learning techniques for semi-supervised and unsupervised skill discovery in reinforcement learning, using minimal supervision and image-based observations?"
  },
  {
    "index": 1158,
    "source_corpus_id": 54445324,
    "ref_id": "b3",
    "citation_corpus_id": 3503217,
    "start": 25596,
    "end": 25621,
    "title": "Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments",
    "abstract": "Ability to continuously learn and adapt from limited experience in nonstationary environments is an important milestone on the path towards general intelligence. In this paper, we cast the problem of continuous adaptation into the learning-to-learn framework. We develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios. Additionally, we design a new multi-agent competitive environment, RoboSumo, and define iterated adaptation games for testing various aspects of continuous adaptation strategies. We demonstrate that meta-learning enables significantly more efficient adaptation than reactive baselines in the few-shot regime. Our experiments with a population of agents that learn and compete suggest that meta-learners are the fittest.While virtually any changes in an environment could induce some kind of nonstationarity (e.g., changes in the physics or characteristics of the agent), environments with multiple agents are particularly challenging due to complexity of the emergent behavior and are of practical interest with applications ranging from multiplayer games [16] to coordinating self-driving fleets[17]. Multi-agent environments are nonstationary from the perspective of any individual agent since all actors are learning and changing concurrently[7,18]. In this paper, we consider the problem of continuous adaptation to a learning opponent in a competitive multi-agent setting.To this end, we design RoboSumo-a 3D environment with simulated physics that allows pairs of agents to compete against each other. To test continuous adaptation, we introduce iterated adaptation games-a new setting where a trained agent competes against the same opponent for multiple rounds of a repeated game, while both are allowed to update their policies and change their behaviors between the rounds. In such iterated games, from the agent's perspective, the environment changes from round to round, and the agent ought to adapt in order to win the game. Additionally, the competitive component of the environment makes it not only nonstationary but also adversarial, which provides a natural training curriculum and encourages learning robust strategies[7,19,20].We evaluate our meta-learning agents along with a number of baselines on a (single-agent) locomotion task with handcrafted nonstationarity and on iterated adaptation games in RoboSumo. Our results demonstrate that meta-learned adaptation strategies clearly dominate other adaptation methods in the few-shot regime in both single-and multi-agent settings. Finally, we carry out a large-scale experiment where we train a diverse population of agents with different anatomies, policy architectures, and adaptation methods, and make them interact by competing against each other in iterated games. We evaluate the agents based on their TrueSkills[21]in these games, as well as evolve the population as whole for a few generations-the agents that lose disappear, while the winners get duplicated. Our results suggest that the agents with meta-learned adaptation strategies end up being the fittest. Videos that demonstrate adaptation behaviors in different tasks are available at https://goo.gl/tboqaN.",
    "prev": "More relevant to our framework are approaches that parameterize the training process through a recurrent neural network that takes the gradient as input and produces a new set of parameters (Ravi & Larochelle, 2016;Santoro et al., 2016;Andrychowicz et al., 2016;Hochreiter et al., 2001).",
    "curr": "The approach most closely related to us learns an initialization such that the model can adapt to a new task through one or a few gradient updates (Finn et al., 2017;Nichol et al., 2018;Al-Shedivat et al., 2017;Lee & Choi, 2018).",
    "next": "In contrast to our work, these methods focus exclusively on few-shot learning, where the gradient path is trivial as only a single or a handful of training steps are allowed, limiting them to settings where the current task is closely related to previous ones.",
    "query": "Are there any research papers on meta-learning techniques for continuous adaptation in nonstationary and competitive environments, particularly in multi-agent settings?"
  },
  {
    "index": 1160,
    "source_corpus_id": 3290366,
    "ref_id": "b1",
    "citation_corpus_id": 3495200,
    "start": 3105,
    "end": 3108,
    "title": "LEARNING TO REPRESENT PROGRAMS WITH GRAPHS",
    "abstract": "Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code's known syntax. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures. In this work, we present how to construct graphs from source code and how to scale Gated Graph Neural Networks training to such large graphs. We evaluate our method on two tasks: VARNAMING, in which a network attempts to predict the name of a variable given its usage, and VARMISUSE, in which the network learns to reason about selecting the correct variable that should be used at a given program location. Our comparison to methods that use less structured program representations shows the advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve the VARMISUSE task in many cases. Additionally, our testing showed that VARMISUSE identifies a number of bugs in mature open-source projects.",
    "prev": "Examples include prediction of properties of chemical molecules [9], answering questions about knowledge graphs [25], natural language processing with parsestructured inputs (trees or richer structures like Abstract Meaning Representations) [4], predicting properties of data structures or source code in programming * Work done partially while author was at Microsoft Research.",
    "curr": "languages [2,22], and making predictions from scene graphs [39].",
    "next": "Sequence data can be seen as a special case of a simple chain-structured graph.",
    "query": "Are there any research papers on using graph-based representations for analyzing and reasoning about source code structures, such as in variable naming or detecting variable misuse?"
  },
  {
    "index": 1161,
    "source_corpus_id": 254926905,
    "ref_id": "b2",
    "citation_corpus_id": 17682909,
    "start": 2924,
    "end": 2926,
    "title": "Spectral Networks and Deep Locally Connected Networks on Graphs",
    "abstract": "Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for lowdimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.",
    "prev": "Can we quantitatively model the effect of applying a finite number of graph convolutions and theoretically predict the \"sweet spot\" for the choice of depth?In this paper, we propose a non-asymptotic analysis framework to study the effects of graph convolutions and oversmoothing using the Contextual Stochastic Block Model (CSBM)[18].",
    "curr": "The CSBM mimics the community structure\n\nIntroduction\n\nGraph Neural Networks (GNNs) are a powerful framework for learning with graph-structured data and have shown great promise in diverse domains such as molecular chemistry, physics, and social network analysis [1,2,3,4,5,6,7].",
    "next": "Most GNN models are built by stacking graph convolutions or message-passing layers [8], where the representation of each node is computed by recursively aggregating and transforming the representations of its neighboring nodes.",
    "query": "Are there any research papers on generalizing convolutional neural networks to graph-structured data using spectral methods and deep locally connected networks?"
  },
  {
    "index": 1162,
    "source_corpus_id": 235613482,
    "ref_id": "b21",
    "citation_corpus_id": 214220671,
    "start": 10830,
    "end": 10848,
    "title": "ABSTRACT DIAGRAMMATIC REASONING WITH MULTIPLEX GRAPH NETWORKS",
    "abstract": "reasoning, particularly in the visual domain, is a complex human ability, but it remains a challenging problem for artificial neural learning systems. In this work we propose MXGNet, a multilayer graph neural network for multi-panel diagrammatic reasoning tasks. MXGNet combines three powerful concepts, namely, object-level representation, graph neural networks and multiplex graphs, for solving visual reasoning tasks. MXGNet first extracts object-level representations for each element in all panels of the diagrams, and then forms a multi-layer multiplex graph capturing multiple relations between objects across different diagram panels. MXGNet summarises the multiple graphs extracted from the diagrams of the task, and uses this summarisation to pick the most probable answer from the given candidates. We have tested MXGNet on two types of diagrammatic reasoning tasks, namely Diagram Syllogisms and Raven Progressive Matrices (RPM). For an Euler Diagram Syllogism task MXGNet achieves state-of-the-art accuracy of 99.8%. For PGM and RAVEN, two comprehensive datasets for RPM reasoning, MXGNet outperforms the state-of-the-art models by a considerable margin.",
    "prev": "(2019) proposed a Logic Embedding Network (LEN) with distracting features, which also uses the auxiliary annotations to boost the performance and designed a teacher model to control the learning trajectories.",
    "curr": "MXGNet (Wang et al., 2020) is a multi-layer graph neural network for multi-panel diagrammatic reasoning tasks.",
    "next": "For better performance, MXGNet also uses auxiliary annotations for model training.",
    "query": "Are there any studies that explore the use of multiplex graph networks for multi-panel diagrammatic reasoning tasks, incorporating auxiliary annotations to improve model performance?"
  },
  {
    "index": 1164,
    "source_corpus_id": 247922261,
    "ref_id": "b18",
    "citation_corpus_id": 8348149,
    "start": 2513,
    "end": 2534,
    "title": "Learning Effective and Interpretable Semantic Models using Non-Negative Sparse Embedding",
    "abstract": "In this paper, we introduce an application of matrix factorization to produce corpus-derived, distributional models of semantics that demonstrate cognitive plausibility. We find that word representations learned by Non-Negative Sparse Embedding (NNSE), a variant of matrix factorization, are sparse, effective, and highly interpretable. To the best of our knowledge, this is the first approach which yields semantic representation of words satisfying these three desirable properties. Though extensive experimental evaluations on multiple real-world tasks and datasets, we demonstrate the superiority of semantic models learned by NNSE over other state-of-the-art baselines.",
    "prev": "For this we draw inspiration from overcomplete representations: representations of an input that are nonunique combinations of a number of basis vectors greater than the input's dimensionality (Lewicki & Sejnowski, 2000).",
    "curr": "Mostly studied in the context of the sparse coding literature (Gregor & LeCun, 2010;Goodfellow et al., 2012;Olshausen, 2013), sparse overcomplete representations have been shown to increase stability in the presence of noise (Donoho et al., 2006), have applications in neuroscience (Olshausen & Field, 1996;Lee et al., 2007), and lead to more interpretable representations (Murphy et al., 2012;Fyshe et al., 2015;Faruqui et al., 2015).",
    "next": "However, the choice of basis vectors is generally assumed to be learned using traditional methods such as ICA (Teh et al., 2003) or fitting linear models (Lewicki & Sejnowski, 2000), limiting the expressive power of the encoding function.",
    "query": "Are there any research papers on non-negative sparse embedding methods for creating interpretable and effective semantic models?"
  },
  {
    "index": 1165,
    "source_corpus_id": 222133257,
    "ref_id": "b4",
    "citation_corpus_id": 5590763,
    "start": 6162,
    "end": 6165,
    "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "prev": "By unrolling this algorithm, the authors demonstrate superior recovery over ALISTA for a specific setting of M, N and s.\n\nIn a related approach [20] identify undershooting, meaning that reconstructed components are smaller than target components, as a shortcoming of LISTA and propose Gated-LISTA to address these issues.",
    "curr": "The authors introduce gain and overshoot gates to LISTA, which can amplify the reconstruction after each iteration before and after thresholding, yielding an architecture resembling GRU cells [5].",
    "next": "The authors demonstrate better sparse reconstruction than previous LISTA-variants and also show that adding their proposed gates to ALISTA, named AGLISTA, it is possible to improve its performance in the same setting of M, N and s as ALISTA-AT.",
    "query": "Are there any studies on improving sparse reconstruction algorithms using gated mechanisms similar to GRU cells in variants of LISTA?"
  },
  {
    "index": 1166,
    "source_corpus_id": 231698498,
    "ref_id": "b58",
    "citation_corpus_id": 202888885,
    "start": 2681,
    "end": 2700,
    "title": "Published as a conference paper at ICLR 2020 DRAWING EARLY-BIRD TICKETS: TOWARDS MORE EF- FICIENT TRAINING OF DEEP NETWORKS",
    "abstract": "Frankle & Carbin, 2019)shows that there exist winning tickets (small but critical subnetworks) for dense, randomly initialized networks, that can be trained alone to achieve a comparable accuracy to the latter in a similar number of iterations. However, the identification of these winning tickets still requires the costly train-prune-retrain process, limiting their practical benefits. In this paper, we discover for the first time that the winning tickets can be identified at a very early training stage, which we term as Early-Bird (EB) tickets, via lowcost training schemes (e.g., early stopping and low-precision training) at large learning rates. Our finding on the existence of EB tickets is consistent with recently reported observations that the key connectivity patterns of neural networks emerge early. Furthermore, we propose a mask distance metric that can be used to identify EB tickets with a low computational overhead, without needing to know the true winning tickets that emerge after the full training. Finally, we leverage the existence of EB tickets and the proposed mask distance to develop efficient training methods, which are achieved by first identifying EB tickets via low-cost schemes, and then continuing to train merely the EB tickets towards the target accuracy. Experiments based on various deep networks and datasets validate: 1) the existence of EB tickets and the effectiveness of mask distance in efficiently identifying them; and 2) that the proposed efficient training via EB tickets can achieve up to 5.8× ∼ 10.7× energy savings while maintaining comparable or even better accuracy as compared to the most competitive state-ofthe-art training methods, demonstrating a promising and easily adopted method for tackling the often cost-prohibitive deep network training.",
    "prev": "INTRODUCTION\n\nThe record-breaking performance of modern deep neural networks (DNNs) comes at a prohibitive training cost due to the required massive training data and parameters, limiting the development of the highly demanded DNN-powered intelligent solutions for numerous applications Wu et al., 2018).",
    "curr": "As an illustration, training ResNet-50 involves 10 18 FLOPs (floating-point operations) and can take 14 days on one state-of-the-art (SOTA) GPU (You et al., 2020b).",
    "next": "Meanwhile, the large DNN training costs have raised increasing financial and environmental concerns.",
    "query": "Are there any research papers on efficient training methods for deep neural networks that leverage early identification of subnetworks (winning tickets) through low-cost training schemes to achieve energy savings and maintain accuracy?"
  },
  {
    "index": 1168,
    "source_corpus_id": 259298217,
    "ref_id": "b42",
    "citation_corpus_id": 246652474,
    "start": 34913,
    "end": 34929,
    "title": "GRAPH-RELATIONAL DOMAIN ADAPTATION",
    "abstract": "Existing domain adaptation methods tend to treat every domain equally and align them all perfectly. Such uniform alignment ignores topological structures among different domains; therefore it may be beneficial for nearby domains, but not necessarily for distant domains. In this work, we relax such uniform alignment by using a domain graph to encode domain adjacency, e.g., a graph of states in the US with each state as a domain and each edge indicating adjacency, thereby allowing domains to align flexibly based on the graph structure. We generalize the existing adversarial learning framework with a novel graph discriminator using encodingconditioned graph embeddings. Theoretical analysis shows that at equilibrium, our method recovers classic domain adaptation when the graph is a clique, and achieves non-trivial alignment for other types of graphs. Empirical results show that our approach successfully generalizes uniform alignment, naturally incorporates domain information represented by graphs, and improves upon existing domain adaptation methods on both synthetic and real-world datasets 1 . * Work conducted during internship at AWS AI Labs. 1 Code will soon be available at https://github.com/Wang-ML-Lab/GRDA arXiv:2202.03628v2 [cs.LG] 21 Apr 2023Published as a conference paper at ICLR 2022One naïve DA method for such graph-relational domains is to perform DA for each pair of neighboring domains separately. Unfortunately, due to the strict alignment between each domain pair, this method will still lead to uniform alignment so long as the graph is connected. To generalize DA to the graphrelational domains, we argue that an ideal method should (1) only enforce uniform alignment when the domain graph is a clique (i.e., every two domains are adjacent), and (2) more importantly, relax uniform alignment to adapt more flexibly across domains according to any non-clique domain graph, thereby naturally incorporating information on the domain adjacency. In this paper, we generalize adversarial DA methods and replace the traditional binary (or multi-class) discriminator with a novel graph discriminator: instead of distinguishing among different domains, our graph discriminator takes as input the encodings of data to reconstruct the domain graph. We show that our method enjoys the following theoretical guarantees: it recovers classic DA when the the domain graph is a clique, and realizes intuitive alignments for other types of graphs such as chains and stars (seeFig. 4). We summarize our contributions as follows:• We propose to use a graph to characterize domain relations and develop graph-relational domain adaptation (GRDA) as the first general adversarial DA method to adapt across domains living on a graph. • We provide theoretical analysis showing that at equilibrium, our method can retain the capability of uniform alignment when the domain graph is a clique, and achieve non-trivial alignment for other types of graphs. • Empirical results on both synthetic and real-world datasets demonstrate the superiority of our method over the state-of-the-art DA methods. Eric Granger. Unsupervised multi-target domain adaptation through knowledge distillation. In . Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation. In Bridging theory and algorithm for domain adaptation. arXiv preprint arXiv:1904.05801, 2019.",
    "prev": "CLASSIFICATION\n\nDatasets.",
    "curr": "DG-15 (Xu et al., 2022) is a synthetic binary classification dataset with 15 groups.",
    "next": "Each group contains 100 data points.",
    "query": "Are there any research papers on graph-relational domain adaptation methods using graph structures and discriminators for improving domain alignment in classification tasks?"
  },
  {
    "index": 1170,
    "source_corpus_id": 231719359,
    "ref_id": "b29",
    "citation_corpus_id": 52900371,
    "start": 3378,
    "end": 3391,
    "title": "FLUCTUATION-DISSIPATION RELATIONS FOR STOCHASTIC GRADIENT DESCENT",
    "abstract": "The notion of the stationary equilibrium ensemble has played a central role in statistical mechanics. In machine learning as well, training serves as generalized equilibration that drives the probability distribution of model parameters toward stationarity. Here, we derive stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. These relations hold exactly for any stationary state and can in particular be used to adaptively set training schedule. We can further use the relations to efficiently extract information pertaining to a loss-function landscape such as the magnitudes of its Hessian and anharmonicity. Our claims are empirically verified.",
    "prev": " this SDE analogy assumes that each minibatch is randomly sampled from the full dataset, which implies that some examples will be sampled multiple times in one epoch.",
    "curr": "Furthermore, the most common SDE analogy holds only for vanishing learning rates (Yaida, 2019) and therefore misses the generalization benefits of finite learning rates which we identify in this work.",
    "next": "An important exception is Li et al.",
    "query": "Are there any research papers on fluctuation-dissipation relations in stochastic gradient descent and their application to adaptive training schedules or loss landscape analysis?"
  },
  {
    "index": 1172,
    "source_corpus_id": 257496038,
    "ref_id": "b11",
    "citation_corpus_id": 208547755,
    "start": 3566,
    "end": 3586,
    "title": "Published as a conference paper at ICLR 2020 DREAM TO CONTROL: LEARNING BEHAVIORS BY LATENT IMAGINATION",
    "abstract": "Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.",
    "prev": "Predictions of the reward r t , discount factor γ t , and next latent state z t+1 are computed based on h t using MLPs.",
    "curr": "o t− z t− ô t− a t− h t− r t− · · · · · ·\n1 arXiv:2303.07109v1 [cs.LG] 13 Mar 2023\n\nPublished as a conference paper at ICLR 2023 A promising model-based concept is learning in imagination (Ha & Schmidhuber, 2018;Kaiser et al., 2020;Hafner et al., 2020;Hafner et al., 2021): instead of learning behaviors from the collected experience directly, a generative model of the environment dynamics is learned in a (self-)supervised manner.",
    "next": "Such a so-called world model can create new trajectories by iteratively predicting the next state and reward.",
    "query": "Are there any research papers on reinforcement learning methods that use world models for learning behaviors through latent imagination in high-dimensional sensory environments?"
  },
  {
    "index": 1174,
    "source_corpus_id": 3473020,
    "ref_id": "b28",
    "citation_corpus_id": 13298214,
    "start": 7727,
    "end": 7749,
    "title": "LEARNING TO NAVIGATE IN COMPLEX ENVIRONMENTS",
    "abstract": "Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs. In particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks. This approach can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour 1 , its ability to localise, and its network activity dynamics, showing that the agent implicitly learns key navigation abilities.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on reinforcement learning approaches for navigation in complex environments that incorporate auxiliary tasks with multimodal sensory inputs?"
  },
  {
    "index": 1176,
    "source_corpus_id": 52917627,
    "ref_id": "b6",
    "citation_corpus_id": 5763832,
    "start": 6756,
    "end": 6777,
    "title": "A Differentiable Physics Engine for Deep Learning in Robotics",
    "abstract": "An important field in robotics is the optimization of controllers. Currently, robots are often treated as a black box in this optimization process, which is the reason why derivative-free optimization methods such as evolutionary algorithms or reinforcement learning are omnipresent. When gradient-based methods are used, models are kept small or rely on finite difference approximations for the Jacobian. This method quickly grows expensive with increasing numbers of parameters, such as found in deep learning. We propose the implementation of a modern physics engine, which can differentiate control parameters. This engine is implemented for both CPU and GPU. Firstly, this paper shows how such an engine speeds up the optimization process, even for small problems. Furthermore, it explains why this is an alternative approach to deep Q-learning, for using deep learning in robotics. Finally, we argue that this is a big step for deep learning in robotics, as it opens up new possibilities to optimize robots, both in hardware and software.",
    "prev": "RELATED WORK\n\nDifferentiable physics simulators.",
    "curr": "Researchers have developed many differentiable physics simulators (Ehrhardt et al., 2017;Degrave et al., 2016).",
    "next": "In particular, Battaglia et al.",
    "query": "Are there any research papers on differentiable physics engines for deep learning in robotics and their application in optimizing controllers with gradient-based methods?"
  },
  {
    "index": 1178,
    "source_corpus_id": 239616181,
    "ref_id": "b31",
    "citation_corpus_id": 210064473,
    "start": 5294,
    "end": 5312,
    "title": "SPACE: UNSUPERVISED OBJECT-ORIENTED SCENE REPRESENTATION VIA SPATIAL ATTENTION AND DECOMPOSITION",
    "abstract": "The ability to decompose complex multi-object scenes into meaningful abstractions like objects is fundamental to achieve higher-level cognition. Previous approaches for unsupervised object-oriented scene representation learning are either based on spatial-attention or scene-mixture approaches and limited in scalability which is a main obstacle towards modeling real-world scenes. In this paper, we propose a generative latent variable model, called SPACE, that provides a unified probabilistic modeling framework that combines the best of spatial-attention and scene-mixture approaches. SPACE can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. Previous models are good at either of these, but not both. SPACE also resolves the scalability problems of previous methods by incorporating parallel spatial-attention and thus is applicable to scenes with a large number of objects without performance degradations. We show through experiments on Atari and 3D-Rooms that SPACE achieves the above properties consistently in comparison to SPAIR, IODINE, and GENESIS. Results of our experiments can be found on our project website: https://sites.google.com/view/space-project-page * Visiting Student at Rutgers University. Authors named inside {} equally contributed. Correspondance to",
    "prev": "(4) While achieving better performance, the proposed model is much simpler than previous approaches.",
    "curr": "PRELIMINARIES\n\n\nOBJECT-CENTRIC REPRESENTATION LEARNING WITH PIXEL-MIXTURE DECODER\n\nA common framework for learning object-centric representations is via a form of auto-encoders (Locatello et al., 2020;Lin et al., 2020b).",
    "next": "In this framework, an encoder takes an input image to return a set of object representation vectors or slot vectors {s 1 , .",
    "query": "Are there any research papers on unsupervised object-oriented scene representation using spatial attention and decomposition techniques?"
  },
  {
    "index": 1179,
    "source_corpus_id": 252668432,
    "ref_id": "b11",
    "citation_corpus_id": 229297705,
    "start": 5683,
    "end": 5699,
    "title": "TOWARDS RESOLVING THE IMPLICIT BIAS OF GRADIENT DESCENT FOR MATRIX FACTORIZATION: GREEDY LOW-RANK LEARNING",
    "abstract": "Matrix factorization is a simple and natural test-bed to investigate the implicit regularization of gradient descent. Gunasekar et al. (2018)  conjectured that Gradient Flow with infinitesimal initialization converges to the solution that minimizes the nuclear norm, but a series of recent papers argued that the language of norm minimization is not sufficient to give a full characterization for the implicit regularization. In this work, we provide theoretical and empirical evidence that for depth-2 matrix factorization, gradient flow with infinitesimal initialization is mathematically equivalent to a simple heuristic rank minimization algorithm, Greedy Low-Rank Learning, under some reasonable assumptions. This generalizes the rank minimization view from previous works to a much broader setting and enables us to construct counter-examples to refute the conjecture from (Gunasekar et al.,  2018). We also extend the results to the case where depth ≥ 3, and we show that the benefit of being deeper is that the above convergence has a much weaker dependence over initialization magnitude so that this rank minimization is more likely to take effect for initialization with practical scale. * Alphabet ordering.",
    "prev": "In contrast to the so-called kernel regime where the implicit bias is described by the Neural Tangent Kernel (Jacot et al., 2018), there are several active regimes (also called rich or feature-learning regimes), whose implicit bias often feature a form sparsity that is absent from the kernel regime.",
    "curr": "Such active regimes have been observed for example in DNNs with small initialization (Chizat & Bach, 2018;Rotskoff & Vanden-Eijnden, 2018;Li et al., 2020;Jacot et al., 2022a), with L 2regularization (Savarese et al., 2019;Ongie et al., 2020;Jacot et al., 2022b) or when trained on exponentially decaying losses (Gunasekar et al., 2018a;b;Soudry et al., 2018;Du et al., 2018;Ji & Telgarsky, 2018;Chizat & Bach, 2020;Ji & Telgarsky, 2020).",
    "next": "In the latter two cases, the implicit bias is described by the representation cost:\nR(f ) = min W:f W =f W 2\nwhere f is a function that can be represented by the network and the minimization is over all parameters W that result in a network function f W equal to f , the parameters W form a vector and W is the L 2 -norm.",
    "query": "Are there any research papers on the implicit bias of gradient descent in matrix factorization, particularly focusing on low-rank learning and sparsity in different training regimes?"
  },
  {
    "index": 1180,
    "source_corpus_id": 53332150,
    "ref_id": "b20",
    "citation_corpus_id": 6628106,
    "start": 2376,
    "end": 2395,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "In comparison, the optimization routines used to train deep nets have arguable changed only little.",
    "curr": "Comparably simple first-order methods like SGD (Robbins & Monro, 1951), its momentum variants (MOMENTUM) (Polyak, 1964;Nesterov, 1983) and ADAM (Kingma & Ba, 2015) remain standards (Goodfellow et al., 2016;Karpathy, 2017).",
    "next": "The low practical relevance of more advanced optimization methods is not for lack of research, though.",
    "query": "Are there any studies that investigate the continued dominance of first-order optimization methods such as SGD, momentum variants, and Adam in deep learning, despite the existence of more advanced optimization techniques?"
  },
  {
    "index": 1182,
    "source_corpus_id": 248177810,
    "ref_id": "b41",
    "citation_corpus_id": 52895589,
    "start": 7982,
    "end": 7999,
    "title": "HOW POWERFUL ARE GRAPH NEURAL NETWORKS?",
    "abstract": "Graph Neural Networks (GNNs) for representation learning of graphs broadly follow a neighborhood aggregation framework, where the representation vector of a node is computed by recursively aggregating and transforming feature vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs in capturing different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",
    "prev": null,
    "curr": null,
    "next": null,
    "query": "Are there any research papers on the theoretical analysis of graph neural networks' expressive power and their limitations in capturing complex graph structures?"
  },
  {
    "index": 1186,
    "source_corpus_id": 232013402,
    "ref_id": "b21",
    "citation_corpus_id": 3144218,
    "start": 2403,
    "end": 2425,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "INTRODUCTION\n\nGraph neural networks (GNNs) (Zhou et al., 2018;Wu et al., 2019), which work with graph structured data, have recently attracted considerable attention, as they can learn expressive representations for various graph-related tasks such as node classification, link prediction, and graph classification.",
    "curr": "While the majority of the existing works on GNNs focus on the message passing strategies for neighborhood aggregation (Kipf & Welling, 2017;Hamilton et al., 2017), which aims to encode the nodes in a graph accurately, graph pooling Ying et al., 2018) that maps the set of nodes into a compact representation is crucial in capturing a meaningful structure of an entire graph.",
    "next": "As a simplest approach for graph pooling, we can average or sum all node features in the given graph (Atwood & Towsley, 2016;Xu et al., 2019) (Figure 1 (B)).",
    "query": "Are there any research papers on semi-supervised classification using graph convolutional networks and graph pooling techniques?"
  },
  {
    "index": 1187,
    "source_corpus_id": 264288929,
    "ref_id": "b2",
    "citation_corpus_id": 53115163,
    "start": 5411,
    "end": 5430,
    "title": "EXPLORATION BY RANDOM NETWORK DISTILLATION",
    "abstract": "We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.",
    "prev": "23b]'s K 6 /7 regret through a computationally efficient algorithm.Still, there remain significant gaps between the current upper bounds and the √ K lower bound.In this work, we push the frontiers both on the information theoretical limits and the achievable bounds under computational constraints: 1) we present the first (computationally inefficient) algorithm that provably obtains O( √ K) regret, showing that this is the minimax K dependence (Section 3); 2) we obtain O(K 3 /4 ) regret with a polynomial-time algorithm (Section 4).Below, we briefly describe the elements in our approaches.",
    "curr": "Inefficient\n\n√ K algorithm.We convert the linear MDP problem to a linear bandit problem by mapping each policy to a single dH-dimensional feature vector, where d is the ambient dimension of the linear MDP and H K 3 /4 in this paper.Beyond theoretical advancement, exploration in policy optimization has also showcased its potential in addressing real-world challenges, as evidenced by empirical studies [Burda et al., 2018, Pan et al., 2019].",
    "next": "Preliminaries\n\nNo-Regret Learning in MDPs.An (episodic) MDP is specified by a tuple M = (S, A, P ) where S is the state space (possibly infinite), A is the action space (assumed to be finite with size A = |A|), P : S × A → ∆(S) is the transition kernal.The state space is assumed to be layered, i.e., S = S 1 ∪ S 2 ∪ • • • ∪ S H where S h ∩ S h ′ = ∅ for any 1 ≤ h < h ′ ≤ H, and transition is only possible from one layer to the next, that is, P (s ′ | s, a) = 0 only when s ∈ S h and s ′ ∈ S h+1 .Without loss of generality, we assume S 1 = {s 1 }.",
    "query": "Are there any research papers on exploration strategies in reinforcement learning that utilize random network distillation or related techniques for improving sample efficiency in complex environments?"
  },
  {
    "index": 1189,
    "source_corpus_id": 264436566,
    "ref_id": "b51",
    "citation_corpus_id": 222140788,
    "start": 27408,
    "end": 27412,
    "title": "Published as a conference paper at ICLR 2021 DENOISING DIFFUSION IMPLICIT MODELS",
    "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples 10× to 50× faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error.",
    "prev": "Dataset.We collect meshes in 4 categories (T-shirt, Top, Skirts, Trousers) from the Cloth3D dataset [4] and regroup them into two new categories: upper garments (including T-shirt and Top) and lower garments (including Skirts and Trousers).For both MeshDiffusion (using the DMTet representation) G-MeshDiffusion (using the G-SHELL representation), we run inverse rendering on meshes with known environment lightmaps and known materials using RGB, binary mask, and depth supervision.We generally follow the same settings of [35] for G-MeshDiffusion.For GET3D, we follow the same training setting as [11] and render multiview RGB images for training.",
    "curr": "Evaluation metrics.For each model, we sample a set of meshes, with the size of the test sets, using 100 steps of DDIM [52] and apply standard Laplacian smoothing to these meshes.Similar to [11,35], we evaluate point cloud metrics between the point clouds sampled from generated meshes and those from ground truth meshes.To compensate the lack of perceptual measure in the point cloud metrics, we also evaluate the generated results with multiview FID (MV-FID) [35,63], which is computed by an average of FID (Fréchet Inception Distance) scores of 20 views (rendered with fixed light sources and a diffuse-only mesh material).During rendering, we do not re-orient the face normals towards the camera so that the difference between watertight and open surfaces can be taken into account.",
    "next": "Qualitative and quantitative results.The quantitative results are given in Table 4.We observe that G-MeshDiffusion generally achieves better performance than the watertight mesh generation methods (MeshDiffusion and GET3D), but more importantly, G-MeshDiffusion can better capture the single-sided nature of non-watertight meshes as it",
    "query": "Are there any research papers on denoising diffusion implicit models for mesh generation with evaluation metrics such as point cloud metrics and multiview FID?"
  },
  {
    "index": 1191,
    "source_corpus_id": 231986303,
    "ref_id": "b23",
    "citation_corpus_id": 3509777,
    "start": 5768,
    "end": 5771,
    "title": "The power of deeper networks for expressing natural functions",
    "abstract": "It is well-known that neural networks are universal approximators, but that deeper networks tend to be much more efficient than shallow ones. We shed light on this by proving that the total number of neurons m required to approximate natural classes of multivariate polynomials of n variables grows only linearly with n for deep neural networks, but grows exponentially when merely a single hidden layer is allowed. We also provide evidence that when the number of hidden layers is increased from 1 to k, the neuron requirement grows exponentially not with n but with n 1/k , suggesting that the minimum number of layers required for computational tractability grows only logarithmically with n.",
    "prev": "For example, it has been shown that it is possible to set the weights of a deep ReLU network such that the number of linear regions computed by the network grows exponentially in the depth [7,9,20,26,27].",
    "curr": "Other works consider the degree of polynomials approximable by networks of different depths [19,24] and the topological invariants of networks [5].",
    "next": "While such work has sometimes been used to explain the utility of different neural network architectures (especially deeper ones), a second strand of prior work has shown that a significant gap can exist between the functions expressible by a given architecture and those which may be learned in practice.",
    "query": "Are there any research papers on the theoretical analysis of deep neural networks' expressive power, particularly focusing on their efficiency in approximating natural functions and the gap between theoretical capabilities and practical learning?"
  },
  {
    "index": 1192,
    "source_corpus_id": 259937490,
    "ref_id": "b9",
    "citation_corpus_id": 244130146,
    "start": 3443,
    "end": 3447,
    "title": "Published as a conference paper at ICLR 2022 SOLVING INVERSE PROBLEMS IN MEDICAL IMAGING WITH SCORE-BASED GENERATIVE MODELS",
    "abstract": "Reconstructing medical images from partial measurements is an important inverse problem in Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). Existing solutions based on machine learning typically train a model to directly map measurements to medical images, leveraging a training dataset of paired images and measurements. These measurements are typically synthesized from images using a fixed physical model of the measurement process, which hinders the generalization capability of models to unknown measurement processes. To address this issue, we propose a fully unsupervised technique for inverse problem solving, leveraging the recently introduced score-based generative models. Specifically, we first train a score-based generative model on medical images to capture their prior distribution. Given measurements and a physical model of the measurement process at test time, we introduce a sampling method to reconstruct an image consistent with both the prior and the observed measurements. Our method does not assume a fixed measurement process during training, and can thus be flexibly adapted to different measurement processes at test time. Empirically, we observe comparable or better performance to supervised learning techniques in several medical imaging tasks in CT and MRI, while demonstrating significantly better generalization to unknown measurement processes.",
    "prev": "notes some forward measurement operator (can be linear or nonlinear) and η ∈ R m is additive noise.Usually, we are interested in the case when m < n, which follows many real-world scenarios.When m < n, the problem is ill-posed and some kind of regularizer (or prior) is necessary to obtain a meaningful solution.",
    "curr": "In the literature, the traditional approach of using hand-crafted priors (e.g.sparsity) is slowly being replaced by rich, learned priors such as deep generative models.Recently, there has been a lot of interests in using diffusion models as structural priors due to their state-of-the-art performance in image generation [7][8][9].Compared to GANs, diffusion models are generally easier and more stable to train as they do not rely on an adversarial training scheme, making them a generative prior that is more readily accessible [7].The most common approach for using diffusion models as priors is to resort to posterior sampling, which has been extensively explored in the literature [10][11][12][13][14][15][16][17].However, despite their remarkable success, these techniques exhibit several limitations.The primary challenge is that the majority of existing works that employ diffusion models as priors train these models directly in the pixel space, which requires substantial computational resources and a large volume of training data.In real-world applications such as computed tomography (CT) and magnetic resonance imaging (MRI) reconstruction, where images are inherently either 3D or even 4D [18], training diffusion models directly in the pixel space is often infeasible.",
    "next": "CT Reconstruction\n\nLatent diffusion models (LDMs), which embed data in order to operate in a lower-dimensional space, present a potential solution to this challenge, along with considerable improvements in computational efficiency [19,20].They also can provide a great amount of flexibility, as they can enable one to transfer and generalize these models to different domains by fine-tuning on small amo",
    "query": "Are there any research papers on using score-based generative models or diffusion models for solving inverse problems in medical imaging, such as CT and MRI reconstruction, with a focus on unsupervised learning and learned priors?"
  },
  {
    "index": 1193,
    "source_corpus_id": 252873172,
    "ref_id": "b8",
    "citation_corpus_id": 238744187,
    "start": 2727,
    "end": 2744,
    "title": "OPEN-VOCABULARY OBJECT DETECTION VIA VISION AND LANGUAGE KNOWLEDGE DISTILLATION",
    "abstract": "We aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs. The fundamental challenge is the availability of training data. It is costly to further scale up the number of classes contained in existing object detection datasets. To overcome this challenge, we propose ViLD, a training method via Vision and Language knowledge Distillation. Our method distills the knowledge from a pretrained open-vocabulary image classification model (teacher) into a two-stage detector (student). Specifically, we use the teacher model to encode category texts and image regions of object proposals. Then we train a student detector, whose region embeddings of detected boxes are aligned with the text and image embeddings inferred by the teacher. We benchmark on LVIS by holding out all rare categories as novel categories that are not seen during training. ViLD obtains 16.1 mask AP r with a ResNet-50 backbone, even outperforming the supervised counterpart by 3.8. When trained with a stronger teacher model ALIGN, ViLD achieves 26.3 AP r . The model can directly transfer to other datasets without finetuning, achieving 72.2 AP 50 on PASCAL VOC, 36.6 AP on COCO and 11.8 AP on Objects365. On COCO, ViLD outperforms the previous state-of-theart (Zareian et al., 2021) by 4.8 on novel AP and 11.4 on overall AP. Code and demo are open-sourced at https://github.com/tensorflow/tpu/ tree/master/models/official/detection/projects/vild.",
    "prev": "Unlike image classification, object detection is more challenging and has a larger gap than the pre-training tasks.",
    "curr": "Several methods, such as RegionCLIP (Zhong et al., 2022), ViLD (Gu et al., 2021), and Detic , have been suggested to transfer knowledge from pre-trained CLIP  model to some modules of object detectors.",
    "next": "However, real images and annotations are still required for some key modules of the object detectors, such as RPNs or RoI heads.",
    "query": "Are there any research papers on open-vocabulary object detection methods using vision and language knowledge distillation, particularly those that transfer knowledge from pre-trained models to object detectors?"
  },
  {
    "index": 1195,
    "source_corpus_id": 258557287,
    "ref_id": "b22",
    "citation_corpus_id": 227209335,
    "start": 11292,
    "end": 11311,
    "title": "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024ˆ1024 images for the first time from a score-based generative model.",
    "prev": "BACKGROUND\n\nIn this section, we first review diffusion models in Section 3.1 and we discuss how they are used for solving inverse problems in Section 3.2.",
    "curr": "DENOISING DIFFUSION MODELS\n\nDiffusion models (Sohl-Dickstein et al., 2015;Ho et al., 2020;Song et al., 2021b) consist of two processes: a forward process that gradually adds noise to input images and a reverse process that learns to generate images by iterative denoising.",
    "next": "Formally the forward process can be expressed by the variance preserving stochastic differential equation (VP-SDE) (Song et al., 2021b) dx \" 1 2 βptqxdt`aβptqdw for t P r0, T s where βptq :\" β min`p β max´βmin q t T rescales the time variable, and dw is the standard Wiener process.",
    "query": "Are there any research papers on score-based generative models using stochastic differential equations for image synthesis and solving inverse problems with diffusion processes?"
  },
  {
    "index": 1196,
    "source_corpus_id": 3463660,
    "ref_id": "b6",
    "citation_corpus_id": 5590763,
    "start": 23522,
    "end": 23540,
    "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "prev": "We then employ a meta controller called interpreter in an iterative manner.",
    "curr": "For the ith interpretation step, the interpreter computes the word attention as:\nτ˚$ ' ' ' ' ' & ' ' ' ' ' % Word attention: o i l 9 exp \" S cos pp i´1 , w l q ‰ Attended context: w i \" ÿ l o i l w l Attended word: s i \" ÿ l o i l w l Interpreter state: p i \" GRUpp i´1 , w i q(6)\nwhere S cos is cosine similarity and GRU is the gated recurrent unit (Cho et al., 2014).",
    "next": "Here we use τ˚to represent an approximation of τ via soft word attention.",
    "query": "Are there any research papers on RNN Encoder-Decoder models for statistical machine translation that utilize attention mechanisms for phrase representation learning?"
  },
  {
    "index": 1197,
    "source_corpus_id": 211296452,
    "ref_id": "b20",
    "citation_corpus_id": 6018348,
    "start": 2313,
    "end": 2331,
    "title": "Distant Supervision for Relation Extraction with an Incomplete Knowledge Base",
    "abstract": "Distant supervision, heuristically labeling a corpus using a knowledge base, has emerged as a popular choice for training relation extractors. In this paper, we show that a significant number of \"negative\" examples generated by the labeling process are false negatives because the knowledge base is incomplete. Therefore the heuristic for generating negative examples has a serious flaw. Building on a state-of-the-art distantly-supervised extraction algorithm, we proposed an algorithm that learns from only positive and unlabeled labels at the pair-of-entity level. Experimental results demonstrate its advantage over existing algorithms.",
    "prev": "\", one can identify the entity Grateful Dead and the path of relations LeadSinger, BirthDate to efficiently extract the answer-provided that this information is present in the KB.",
    "curr": "Unfortunately, KBs are often incomplete (Min et al., 2013).",
    "next": "While relation extraction methods can be used to populate KBs, this process is inherently error-prone, expensive and slow.",
    "query": "Are there any research papers on relation extraction methods using distant supervision with incomplete knowledge bases?"
  }
]