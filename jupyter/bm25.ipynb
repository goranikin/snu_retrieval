{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8c03ead-aaf9-4c31-999f-bf806d30091f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
      "Collecting rank_bm25==0.2.2\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting transformers==4.30.2\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\n",
      "Collecting numpy==1.23.5\n",
      "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting torch==1.13.1\n",
      "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Collecting sentence-transformers==2.2.2\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting datasets==2.20.0\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (0.30.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (2.32.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.2)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (4.12.2)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1)\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1)\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1)\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1)\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.18.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.5.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.14.0)\n",
      "Collecting sentencepiece (from sentence-transformers==2.2.2)\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (20.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets==2.20.0)\n",
      "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (3.11.18)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (72.1.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.44.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.20.0) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.20.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.20.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.20.0) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.20.0) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.5.0)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision (from sentence-transformers==2.2.2)\n",
      "  Downloading torchvision-0.22.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "  Downloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "  Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "  Downloading torchvision-0.20.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "  Downloading torchvision-0.19.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "  Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "  Downloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "INFO: pip is still looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading torchvision-0.17.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading torchvision-0.17.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading torchvision-0.16.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\n",
      "  Downloading torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\n",
      "  Downloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers==2.2.2) (10.4.0)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sentence-transformers\n",
      "\u001b[33m  DEPRECATION: Building 'sentence-transformers' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'sentence-transformers'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=c6e49559b658ef0f3560d4f08f334760944e664da263907033473911bb929516\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: tokenizers, sentencepiece, pyarrow-hotfix, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, numpy, nltk, rank_bm25, nvidia-cudnn-cu11, transformers, torch, torchvision, sentence-transformers, datasets\n",
      "\u001b[2K  Attempting uninstall: tokenizers\n",
      "\u001b[2K    Found existing installation: tokenizers 0.21.1\n",
      "\u001b[2K    Uninstalling tokenizers-0.21.1:\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.21.1\n",
      "\u001b[2K  Attempting uninstall: numpym╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/15\u001b[0m [nvidia-cublas-cu11]u11]1]\n",
      "\u001b[2K    Found existing installation: numpy 1.24.4━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/15\u001b[0m [nvidia-cublas-cu11]\n",
      "\u001b[2K    Uninstalling numpy-1.24.4:m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/15\u001b[0m [nvidia-cublas-cu11]\n",
      "\u001b[2K      Successfully uninstalled numpy-1.24.4━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/15\u001b[0m [numpy]las-cu11]\n",
      "\u001b[2K  Attempting uninstall: transformers\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/15\u001b[0m [nvidia-cudnn-cu11]\n",
      "\u001b[2K    Found existing installation: transformers 4.48.3━━━━━━━━━━\u001b[0m \u001b[32m 9/15\u001b[0m [nvidia-cudnn-cu11]\n",
      "\u001b[2K    Uninstalling transformers-4.48.3:m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m10/15\u001b[0m [transformers]]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.48.3━━━━━━━━━━━━\u001b[0m \u001b[32m10/15\u001b[0m [transformers]\n",
      "\u001b[2K  Attempting uninstall: torch━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m10/15\u001b[0m [transformers]\n",
      "\u001b[2K    Found existing installation: torch 2.3.1+cu121━━━━━━━━━━━━\u001b[0m \u001b[32m10/15\u001b[0m [transformers]\n",
      "\u001b[2K    Uninstalling torch-2.3.1+cu121:━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m11/15\u001b[0m [torch]rs]\n",
      "\u001b[2K      Successfully uninstalled torch-2.3.1+cu121[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m11/15\u001b[0m [torch]\n",
      "\u001b[2K  Attempting uninstall: torchvision━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m11/15\u001b[0m [torch]\n",
      "\u001b[2K    Found existing installation: torchvision 0.18.1+cu121━━━━━\u001b[0m \u001b[32m11/15\u001b[0m [torch]\n",
      "\u001b[2K    Uninstalling torchvision-0.18.1+cu121:╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m11/15\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled torchvision-0.18.1+cu121[90m━━━━━━━\u001b[0m \u001b[32m12/15\u001b[0m [torchvision]\n",
      "\u001b[2K  Attempting uninstall: datasets━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m13/15\u001b[0m [sentence-transformers]\n",
      "\u001b[2K    Found existing installation: datasets 3.5.1╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m13/15\u001b[0m [sentence-transformers]\n",
      "\u001b[2K    Uninstalling datasets-3.5.1:━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m13/15\u001b[0m [sentence-transformers]\n",
      "\u001b[2K      Successfully uninstalled datasets-3.5.11m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m13/15\u001b[0m [sentence-transformers]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15\u001b[0m [datasets]/15\u001b[0m [datasets]sformers]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "adapters 1.1.1 requires transformers~=4.48.3, but you have transformers 4.30.2 which is incompatible.\n",
      "torchaudio 2.3.1+cu121 requires torch==2.3.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-2.20.0 nltk-3.9.1 numpy-1.23.5 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 pyarrow-hotfix-0.7 rank_bm25-0.2.2 sentence-transformers-2.2.2 sentencepiece-0.2.0 tokenizers-0.13.3 torch-1.13.1 torchvision-0.14.1 transformers-4.30.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tqdm rank_bm25==0.2.2 nltk transformers==4.30.2 numpy==1.23.5 torch==1.13.1 sentence-transformers==2.2.2 datasets==2.20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2cefea-80d4-4a62-ba00-d17057939567",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1819ccf5-0e84-47b0-9428-c582bc565e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfc4cbf9c1342129e736f14b480292d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "query_data = load_dataset(\"princeton-nlp/LitSearch\", \"query\", split=\"full\")\n",
    "corpus_clean_data = load_dataset(\"princeton-nlp/LitSearch\", \"corpus_clean\", split=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "348e81dd-50b3-48e8-9cb0-0c36aa64b6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Any\n",
    "from datasets import Dataset\n",
    "\n",
    "def get_clean_corpusid(item: dict) -> int:\n",
    "    return item['corpusid']\n",
    "\n",
    "def get_clean_title(item: dict) -> str:\n",
    "    return item['title']\n",
    "\n",
    "def get_clean_abstract(item: dict) -> str:\n",
    "    return item['abstract']\n",
    "\n",
    "def get_clean_title_abstract(item: dict) -> str:\n",
    "    title = get_clean_title(item)\n",
    "    abstract = get_clean_abstract(item)\n",
    "    return f\"Title: {title}\\nAbstract: {abstract}\"\n",
    "\n",
    "def get_clean_full_paper(item: dict) -> str:\n",
    "    return item['full_paper']\n",
    "\n",
    "def get_clean_paragraph_indices(item: dict) -> List[Tuple[int, int]]:\n",
    "    text = get_clean_full_paper(item)\n",
    "    paragraph_indices = []\n",
    "    paragraph_start = 0\n",
    "    paragraph_end = 0\n",
    "    while paragraph_start < len(text):\n",
    "        paragraph_end = text.find(\"\\n\\n\", paragraph_start)\n",
    "        if paragraph_end == -1:\n",
    "            paragraph_end = len(text)\n",
    "        paragraph_indices.append((paragraph_start, paragraph_end))\n",
    "        paragraph_start = paragraph_end + 2\n",
    "    return paragraph_indices\n",
    "\n",
    "def get_clean_text(item: dict, start_idx: int, end_idx: int) -> str:\n",
    "    text = get_clean_full_paper(item)\n",
    "    assert start_idx >= 0 and end_idx >= 0\n",
    "    assert start_idx <= end_idx\n",
    "    assert end_idx <= len(text)\n",
    "    return text[start_idx:end_idx]\n",
    "\n",
    "def get_clean_paragraphs(item: dict, min_words: int = 10) -> List[str]:\n",
    "    paragraph_indices = get_clean_paragraph_indices(item)\n",
    "    paragraphs = [get_clean_text(item, paragraph_start, paragraph_end) for paragraph_start, paragraph_end in paragraph_indices]\n",
    "    paragraphs = [paragraph for paragraph in paragraphs if len(paragraph.split()) >= min_words]\n",
    "    return paragraphs\n",
    "\n",
    "def get_clean_citations(item: dict) -> List[int]:\n",
    "    return item['citations']\n",
    "\n",
    "def get_clean_dict(data: Dataset) -> dict:\n",
    "    return {get_clean_corpusid(item): item for item in data}\n",
    "\n",
    "def create_kv_pairs(data: List[dict], key: str) -> dict:\n",
    "    if key == \"title_abstract\":\n",
    "        kv_pairs = {get_clean_title_abstract(record): get_clean_corpusid(record) for record in data}\n",
    "    elif key == \"full_paper\":\n",
    "        kv_pairs = {get_clean_full_paper(record): get_clean_corpusid(record) for record in data}\n",
    "    elif key == \"paragraphs\":\n",
    "        kv_pairs = {}\n",
    "        for record in data:\n",
    "            corpusid = get_clean_corpusid(record)\n",
    "            paragraphs = get_clean_paragraphs(record)\n",
    "            for paragraph_idx, paragraph in enumerate(paragraphs):\n",
    "                kv_pairs[paragraph] = (corpusid, paragraph_idx)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid key\")\n",
    "    return kv_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6d571a5-a4ff-4160-881d-f93b2d42f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_pairs = create_kv_pairs(corpus_clean_data, \"title_abstract\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf243aea-465f-4498-af4a-43c539697257",
   "metadata": {},
   "source": [
    "BM25 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9d4be03-e3ab-45ad-9fbd-83caf7d543e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from enum import Enum\n",
    "from typing import List, Tuple, Any\n",
    "\n",
    "class TextType(Enum):\n",
    "    KEY = 1\n",
    "    QUERY = 2\n",
    "\n",
    "class KVStore:\n",
    "    def __init__(self, index_name: str, index_type: str) -> None:\n",
    "        self.index_name = index_name\n",
    "        self.index_type = index_type\n",
    "\n",
    "        self.keys = []\n",
    "        self.encoded_keys = []\n",
    "        self.values = []\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.keys)\n",
    "\n",
    "    def _encode(self, text: str, type: TextType) -> Any:\n",
    "        return self._encode_batch([text], type, show_progress_bar=False)[0]\n",
    "    \n",
    "    def _encode_batch(self, texts: List[str], type: TextType, show_progress_bar: bool = True) -> List[Any]: \n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _query(self, encoded_query: Any, n: int) -> List[int]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        self.keys = []\n",
    "        self.encoded_keys = []\n",
    "        self.values = []\n",
    "\n",
    "    def create_index(self, key_value_pairs: List[Tuple[str, Any]]) -> None:\n",
    "        if len(self.keys) > 0:\n",
    "            raise ValueError(\"Index is not empty. Please create a new index or clear the existing one.\")\n",
    "        \n",
    "        for key, value in tqdm(key_value_pairs.items(), desc=f\"Creating {self.index_name} index\"):\n",
    "            self.keys.append(key)\n",
    "            self.values.append(value)\n",
    "        self.encoded_keys = self._encode_batch(self.keys, TextType.KEY)\n",
    "\n",
    "    def query(self, query_text: str, n: int, return_keys: bool = False) -> List[Any]:\n",
    "        encoded_query = self._encode(query_text, TextType.QUERY)\n",
    "        indices = self._query(encoded_query, n)\n",
    "        if return_keys:\n",
    "            results = [(self.keys[i], self.values[i]) for i in indices]\n",
    "        else:\n",
    "            results = [self.values[i] for i in indices]\n",
    "        return results\n",
    "\n",
    "    def save(self, dir_name: str) -> None:\n",
    "        save_dict = {}\n",
    "        for key, value in self.__dict__.items():\n",
    "            if key[0] != \"_\":\n",
    "                save_dict[key] = value\n",
    "\n",
    "        print(f\"Saving index to {os.path.join(dir_name, f'{self.index_name}.{self.index_type}')}\")\n",
    "        os.makedirs(dir_name, exist_ok=True)\n",
    "        with open(os.path.join(dir_name, f\"{self.index_name}.{self.index_type}\"), 'wb') as file:\n",
    "            pickle.dump(save_dict, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "\n",
    "    def load(self, file_path: str) -> None:\n",
    "        if len(self.keys) > 0:\n",
    "            raise ValueError(\"Index is not empty. Please create a new index or clear the existing one before loading from disk.\")\n",
    "        \n",
    "        print(f\"Loading index from {file_path}...\")\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pickle_data = pickle.load(file)\n",
    "        \n",
    "        for key, value in pickle_data.items():\n",
    "            setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d042efc-149e-4f8f-90a0-c64194b0b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "class BM25(KVStore):\n",
    "    def __init__(self, index_name: str):\n",
    "        super().__init__(index_name, 'bm25')\n",
    "\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('stopwords')\n",
    "        nltk.download('punkt_tab')\n",
    "\n",
    "        self._tokenizer = nltk.word_tokenize\n",
    "        self._stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        self._stemmer = nltk.stem.PorterStemmer().stem\n",
    "        self.index = None   # BM25 index\n",
    "\n",
    "    def _encode_batch(self, texts: List[str], type: TextType, show_progress_bar: bool = True) -> List[str]:\n",
    "        # lowercase, tokenize, remove stopwords, and stem\n",
    "        tokens_list = []\n",
    "        for text in tqdm(texts, disable=not show_progress_bar):\n",
    "            tokens = self._tokenizer(text.lower())\n",
    "            tokens = [token for token in tokens if token not in self._stop_words]\n",
    "            tokens = [self._stemmer(token) for token in tokens]\n",
    "            tokens_list.append(tokens)\n",
    "        return tokens_list\n",
    "\n",
    "    def _query(self, encoded_query: List[str], n: int) -> List[int]:\n",
    "        top_indices = np.argsort(self.index.get_scores(encoded_query))[::-1][:n].tolist()\n",
    "        return top_indices\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        super().clear()\n",
    "        self.index = None\n",
    "\n",
    "    def create_index(self, key_value_pairs: List[Tuple[str, Any]]) -> None:\n",
    "        super().create_index(key_value_pairs)\n",
    "        self.index = BM25Okapi(self.encoded_keys)\n",
    "\n",
    "    def load(self, dir_name: str) -> None:\n",
    "        super().load(dir_name)\n",
    "        self._tokenizer = nltk.word_tokenize\n",
    "        self._stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        self._stemmer = nltk.stem.PorterStemmer().stem\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c31c16bb-b19e-4ed6-a3ba-0efe8be205cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "bm25_model = BM25(\"Title_Abstract_BM25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "994eb743-db86-45fd-9320-021d9f7ada0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Title_Abstract_BM25 index: 100%|██████████| 57657/57657 [00:00<00:00, 2877707.00it/s]\n",
      "100%|██████████| 57657/57657 [01:51<00:00, 517.50it/s]\n"
     ]
    }
   ],
   "source": [
    "bm25_model.create_index(kv_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba12c9a-1205-44e5-9fc6-ed6e3f3e1090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 163/597 [00:35<01:27,  4.95it/s]"
     ]
    }
   ],
   "source": [
    "query_set = [query for query in query_data]\n",
    "for query in tqdm(query_set):\n",
    "    query_text = query[\"query\"]\n",
    "    top_k = bm25_model.query(query_text, 20)\n",
    "    query[\"retrieved\"] = top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dba19f09-5060-4fd0-8377-17fc40c9dc06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_set': 'inline_acl',\n",
       " 'query': 'Are there any research papers on methods to compress large-scale language models using task-agnostic knowledge distillation techniques?',\n",
       " 'specificity': 0,\n",
       " 'quality': 2,\n",
       " 'corpusids': [202719327],\n",
       " 'retrieved': [248227350,\n",
       "  221995575,\n",
       "  257038997,\n",
       "  235294276,\n",
       "  258212842,\n",
       "  3643430,\n",
       "  256461337,\n",
       "  201670719,\n",
       "  219176798,\n",
       "  247741658,\n",
       "  259137871,\n",
       "  247596810,\n",
       "  250390982,\n",
       "  258865530,\n",
       "  234482764,\n",
       "  239016109,\n",
       "  259370686,\n",
       "  258960671,\n",
       "  237563200,\n",
       "  222290473]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de375a-d44e-4677-8344-db6619c89170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "query_set_df = pd.DataFrame(query_set)\n",
    "broad_query_set_df = query_set_df[query_set_df['specificity'] == 0]\n",
    "specific_query_set_df = query_set_df[query_set_df['specificity'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7efc1e-336d-4cfc-9342-6c152ddc3900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(corpusids: list, retrieved: list, k: int):\n",
    "    top_k = retrieved[:k]\n",
    "    intersection = set(corpusids) & set(top_k)\n",
    "    return len(intersection) / len(corpusids) if corpusids else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e22c43-3aa3-4094-a83d-26e0519fc7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broad\n",
    "\n",
    "all_recall_at20 = []\n",
    "\n",
    "for _, query in broad_query_set_df.iterrows():\n",
    "    r20 = calculate_recall(query['corpusids'], query['retrieved'], 20)\n",
    "    all_recall_at20.append(r20)\n",
    "\n",
    "mean_recall_at20 = np.mean(all_recall_at20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4014b490-4f21-4b9a-ab9b-f4918734266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_recall_at20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da310b9-94b1-44de-be5f-6e66e141ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific\n",
    "\n",
    "all_recall_at5 = []\n",
    "\n",
    "for _, query in specific_query_set_df.iterrows():\n",
    "    r5 = calculate_recall(query['corpusids'], query['retrieved'], 5)\n",
    "    all_recall_at5.append(r5)\n",
    "\n",
    "mean_recall_at5 = np.mean(all_recall_at5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6424c6f6-85a7-4b9a-9729-40e243992343",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_recall_at5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
