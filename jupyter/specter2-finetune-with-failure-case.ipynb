{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4726a137-5d46-48a1-84f2-620623774a55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting adapters\n",
      "  Using cached adapters-1.1.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Using cached propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached adapters-1.1.1-py3-none-any.whl (289 kB)\n",
      "Using cached transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
      "Using cached datasets-3.5.1-py3-none-any.whl (491 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Using cached aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Using cached multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
      "Using cached yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "Using cached propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "Using cached pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "Using cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, safetensors, regex, pyarrow, propcache, multidict, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, datasets, adapters\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/19\u001b[0m [adapters]/19\u001b[0m [adapters]ers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed adapters-1.1.1 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.5.1 dill-0.3.8 frozenlist-1.6.0 huggingface-hub-0.30.2 multidict-6.4.3 multiprocess-0.70.16 propcache-0.3.1 pyarrow-20.0.0 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.48.3 xxhash-3.5.0 yarl-1.20.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers adapters datasets faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2508ebd-9bdc-4515-96dd-05da09940774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9067603d-6471-40f7-bafe-9a26806e2280",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87c6deb3-d9d5-4edc-9153-967eedf8d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "corpus_clean_data = load_dataset(\"princeton-nlp/LitSearch\", \"corpus_clean\", split=\"full\")\n",
    "query_data = load_dataset(\"princeton-nlp/LitSearch\", \"query\", split=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d2d480a-711d-4e47-ac71-a240fafaf785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a2372da-2eba-4e68-ba5e-aca0d3a0b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_0_train = load_from_json('./datasets/spec0_train.json')\n",
    "spec_0_val = load_from_json('./datasets/spec0_val.json')\n",
    "spec_0_test = load_from_json('./datasets/spec0_test.json')\n",
    "spec_1_train = load_from_json('./datasets/spec1_train.json')\n",
    "spec_1_val = load_from_json('./datasets/spec1_val.json')\n",
    "spec_1_test = load_from_json('./datasets/spec1_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df80078f-f2c2-439c-b20f-b2db6ea5565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = spec_0_train + spec_1_train\n",
    "val_data = spec_0_val + spec_1_val\n",
    "test_data = spec_0_test + spec_1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48353557-ff29-4224-87da-f5360af7ff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from adapters import AutoAdapterModel\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a0917ba-e6d9-4f63-8f48-e507b9c9f188",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitSearchDataset(Dataset):\n",
    "    def __init__(self, data: list[dict[str, str]], tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        question = item['question']\n",
    "\n",
    "        # 포지티브 샘플 (1개)\n",
    "        pos_ctx = item['positive_ctxs'][0]\n",
    "        pos_text = pos_ctx['title'] + self.tokenizer.sep_token + pos_ctx['text']\n",
    "\n",
    "        # 네거티브 샘플 (3개 중 랜덤 선택)\n",
    "        neg_ctx = random.choice(item['negative_ctxs'])\n",
    "        neg_text = neg_ctx['title'] + self.tokenizer.sep_token + neg_ctx['text']\n",
    "\n",
    "        # 토크나이징\n",
    "        query_tokens = self.tokenizer(\n",
    "            question,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        pos_tokens = self.tokenizer(\n",
    "            pos_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        neg_tokens = self.tokenizer(\n",
    "            neg_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # 배치 차원 제거\n",
    "        query_tokens = {k: v.squeeze(0) for k, v in query_tokens.items()}\n",
    "        pos_tokens = {k: v.squeeze(0) for k, v in pos_tokens.items()}\n",
    "        neg_tokens = {k: v.squeeze(0) for k, v in neg_tokens.items()}\n",
    "\n",
    "        return {\n",
    "            'query': query_tokens,\n",
    "            'positive': pos_tokens,\n",
    "            'negative': neg_tokens,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9eff1a-1e0d-4269-9c84-dfbf1ff52c2c",
   "metadata": {},
   "source": [
    "Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0305aa4b-dcb1-4fd8-a19b-660b19aef495",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletMarginLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletMarginLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, query_emb, pos_emb, neg_emb):\n",
    "        # L2 거리 계산\n",
    "        pos_dist = torch.norm(query_emb - pos_emb, p=2, dim=1)\n",
    "        neg_dist = torch.norm(query_emb - neg_emb, p=2, dim=1)\n",
    "\n",
    "        # max(0, pos_dist - neg_dist + margin) 형태의 손실\n",
    "        loss = torch.clamp(pos_dist - neg_dist + self.margin, min=0.0)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed01fdfe-bb49-4a95-be46-6a0e5187f59f",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beebe811-3fcd-4ded-8b28-0a3eeb9bb459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import torch\n",
    "\n",
    "class TextType(Enum):\n",
    "    KEY = 1\n",
    "    QUERY = 2\n",
    "\n",
    "class Retrieval:\n",
    "    def __init__(self, index_name: str, index_type: str) -> None:\n",
    "        self.index_name = index_name\n",
    "        self.index_type = index_type\n",
    "\n",
    "        self.keys = []\n",
    "        self.values = []\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.keys)\n",
    "\n",
    "    def _get_embeddings(self, textList: list[str], type: TextType, show_progress_bar: bool = False) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _query(self, query_embedding: torch.Tensor, top_k: int = 10) -> List[int]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def query(self, query_text: str, n: int, return_keys: bool = False) -> List[Any]:\n",
    "        embedding_query = self._get_embeddings([query_text], TextType.QUERY)\n",
    "        indices = self._query(embedding_query, n)\n",
    "        if return_keys:\n",
    "            results = [(self.keys[i], self.values[i]) for i in indices]\n",
    "        else:\n",
    "            results = [self.values[i] for i in indices]\n",
    "        return results\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.keys = []\n",
    "        self.encoded_keys = []\n",
    "        self.values = []\n",
    "\n",
    "    def create_index(self, key_value_pairs: Dict[str, int]) -> None:\n",
    "        if len(self.keys) > 0:\n",
    "            raise ValueError(\"Index is not empty. Please create a new index or clear the existing one.\")\n",
    "\n",
    "        for key, value in key_value_pairs.items():\n",
    "            self.keys.append(key)\n",
    "            self.values.append(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e64bc6d-08f9-45b4-acc7-38398b9bc2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from adapters import AutoAdapterModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import faiss\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class SPECTER2QueryAdapterFinetuner(Retrieval):\n",
    "    def __init__(self, base_model_name=\"allenai/specter2_base\", device=None):\n",
    "\n",
    "        self.keys = []\n",
    "        self.values = []\n",
    "        self.index = None\n",
    "        self.faiss_index = None\n",
    "\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "        self.model = AutoAdapterModel.from_pretrained(\"allenai/specter2_base\")\n",
    "\n",
    "        self.model.load_adapter(\"allenai/specter2\", source=\"hf\", load_as=\"proximity\")\n",
    "        self.model.load_adapter(\"allenai/specter2_adhoc_query\", source=\"hf\", load_as=\"adhoc_query\")\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if \"adapters.adhoc_query\" in name:\n",
    "                param.requires_grad = True\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def encode_text(self, input_ids, attention_mask, adapter_type=\"proximity\"):\n",
    "        \"\"\"\n",
    "        adapter_type: query -> \"adhoc_query\", text -> \"proximity\"\n",
    "        \"\"\"\n",
    "        self.model.set_active_adapters(adapter_type)\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids.to(self.device),\n",
    "            attention_mask=attention_mask.to(self.device)\n",
    "        )\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]  # CLS 토큰 임베딩 사용\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def encode_query(self, query_text):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            tokens = self.tokenizer(\n",
    "                query_text,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            return self.encode_text(\n",
    "                tokens['input_ids'],\n",
    "                tokens['attention_mask'],\n",
    "                adapter_type=\"adhoc_query\"\n",
    "            )\n",
    "\n",
    "    def encode_paper(self, title, abstract):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            text = title + self.tokenizer.sep_token + abstract\n",
    "            tokens = self.tokenizer(\n",
    "                text,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            return self.encode_text(\n",
    "                tokens['input_ids'],\n",
    "                tokens['attention_mask'],\n",
    "                adapter_type=\"proximity\"\n",
    "            )\n",
    "\n",
    "    def finetune(self, train_data, val_data=None, output_dir=\"./specter2_adhoc_query_finetuned\",\n",
    "                 lr=2e-5, batch_size=8, epochs=3, margin=1.0, eval_steps=100,\n",
    "                 weight_decay=0.01, warmup_ratio=0.1):\n",
    "\n",
    "        train_dataset = LitSearchDataset(train_data, self.tokenizer)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            [p for p in self.model.parameters() if p.requires_grad],\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "\n",
    "        total_steps = len(train_loader) * epochs\n",
    "        warmup_steps = int(total_steps * warmup_ratio)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=lr,\n",
    "            total_steps=total_steps,\n",
    "            pct_start=warmup_ratio,\n",
    "            anneal_strategy='linear'\n",
    "        )\n",
    "\n",
    "        triplet_loss = TripletMarginLoss(margin=margin)\n",
    "\n",
    "        self.model.train()\n",
    "        global_step = 0\n",
    "        best_val_loss = float('inf')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "            for batch in progress_bar:\n",
    "                # 쿼리, 포지티브, 네거티브 임베딩\n",
    "                query_emb = self.encode_text(\n",
    "                    batch['query']['input_ids'],\n",
    "                    batch['query']['attention_mask'],\n",
    "                    adapter_type=\"adhoc_query\"\n",
    "                )\n",
    "\n",
    "                pos_emb = self.encode_text(\n",
    "                    batch['positive']['input_ids'],\n",
    "                    batch['positive']['attention_mask'],\n",
    "                    adapter_type=\"proximity\"\n",
    "                )\n",
    "\n",
    "                neg_emb = self.encode_text(\n",
    "                    batch['negative']['input_ids'],\n",
    "                    batch['negative']['attention_mask'],\n",
    "                    adapter_type=\"proximity\"\n",
    "                )\n",
    "\n",
    "                loss = triplet_loss(query_emb, pos_emb, neg_emb)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_([p for p in self.model.parameters() if p.requires_grad], 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "                global_step += 1\n",
    "                if val_data is not None and global_step % eval_steps == 0:\n",
    "                    val_loss = self.evaluate(val_data, batch_size)\n",
    "                    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        self.save_model(output_dir)\n",
    "                        print(f\"Model saved to {output_dir} (val_loss: {val_loss:.4f})\")\n",
    "\n",
    "                    self.model.train()\n",
    "\n",
    "            avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Avg Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "        if val_data is None or epochs % eval_steps != 0:\n",
    "            self.save_model(output_dir)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def evaluate(self, val_data, batch_size=8):\n",
    "        val_dataset = LitSearchDataset(val_data, self.tokenizer)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "        self.model.eval()\n",
    "        triplet_loss = TripletMarginLoss(margin=1.0)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                query_emb = self.encode_text(\n",
    "                    batch['query']['input_ids'],\n",
    "                    batch['query']['attention_mask'],\n",
    "                    adapter_type=\"adhoc_query\"\n",
    "                )\n",
    "\n",
    "                pos_emb = self.encode_text(\n",
    "                    batch['positive']['input_ids'],\n",
    "                    batch['positive']['attention_mask'],\n",
    "                    adapter_type=\"proximity\"\n",
    "                )\n",
    "\n",
    "                neg_emb = self.encode_text(\n",
    "                    batch['negative']['input_ids'],\n",
    "                    batch['negative']['attention_mask'],\n",
    "                    adapter_type=\"proximity\"\n",
    "                )\n",
    "\n",
    "                loss = triplet_loss(query_emb, pos_emb, neg_emb)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(val_loader)\n",
    "\n",
    "    def save_model(self, output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        self.model.save_adapter(output_dir, \"adhoc_query\")\n",
    "\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "        print(f\"어댑터가 {output_dir}에 저장되었습니다.\")\n",
    "\n",
    "    def _get_embeddings(self, textList: list[str], type: TextType, show_progress_bar: bool = True) -> torch.Tensor:\n",
    "        if type == TextType.KEY:\n",
    "            self.model.set_active_adapters(\"proximity\")\n",
    "        else:\n",
    "            self.model.set_active_adapters(\"adhoc_query\")\n",
    "\n",
    "        batch_size = 16\n",
    "        embeddings = []\n",
    "\n",
    "        should_show_progress = show_progress_bar and (type == TextType.KEY)\n",
    "\n",
    "        iterator = range(0, len(textList), batch_size)\n",
    "        if should_show_progress:\n",
    "            iterator = tqdm(iterator, desc=\"Processing document embeddings\")\n",
    "\n",
    "        for i in iterator:\n",
    "            batch_texts = textList[i:i+batch_size]\n",
    "            encoded = self.tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=512\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**encoded)\n",
    "\n",
    "            batch_embeddings = outputs.last_hidden_state[:,0,:].cpu()\n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "            if i % (batch_size * 10) == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        embeddings = torch.cat(embeddings, dim=0)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def clear(self):\n",
    "        super().clear()\n",
    "        self.index = None\n",
    "        self.faiss_index = None\n",
    "\n",
    "    def create_index(self, key_value_pairs: dict[str, int]) -> None:\n",
    "        super().create_index(key_value_pairs)\n",
    "        self.index = self._get_embeddings(self.keys, TextType.KEY)\n",
    "\n",
    "        # FAISS 인덱스 생성\n",
    "        vector_dim = self.index.shape[1]\n",
    "        index_flat = faiss.IndexFlatIP(vector_dim)\n",
    "        index_vectors = self.index.numpy()\n",
    "        faiss.normalize_L2(index_vectors)\n",
    "\n",
    "        # 인덱스에 벡터 추가\n",
    "        index_flat.add(index_vectors)\n",
    "        self.faiss_index = index_flat\n",
    "\n",
    "    def _query(self, query_embedding: torch.Tensor, top_k: int = 10) -> list[int]:\n",
    "        if self.faiss_index is None:\n",
    "            raise ValueError(\"FAISS index has not been created yet. Call create_index first.\")\n",
    "\n",
    "        query_vector = query_embedding.numpy()\n",
    "        faiss.normalize_L2(query_vector)\n",
    "        distances, indices = self.faiss_index.search(query_vector, top_k)\n",
    "\n",
    "        return indices[0].tolist()\n",
    "\n",
    "    def query(self, query_text: str, n: int, return_keys: bool = False) -> list:\n",
    "        query_embedding = self._get_embeddings([query_text], TextType.QUERY)\n",
    "        indices = self._query(query_embedding, n)\n",
    "\n",
    "        if return_keys:\n",
    "            results = [(self.keys[i], self.values[i]) for i in indices]\n",
    "        else:\n",
    "            results = [self.values[i] for i in indices]\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b868a1-2341-4d1e-b897-16406c004d0a",
   "metadata": {},
   "source": [
    "Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be38cf3d-60d5-4833-bf3c-2d3c16e97ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(corpus_id_list: list, retrieved_id_list: list, k: int):\n",
    "    top_k = retrieved_id_list[:k]\n",
    "    intersection = set(corpus_id_list) & set(top_k)\n",
    "    return len(intersection) / len(corpus_id_list) if corpus_id_list else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fee5753f-68e0-4917-aab3-ea02aabe2642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(finetuner, test_data, query_data, k_values=[1, 5, 10, 20]):\n",
    "    # 제 IDE 로컬 환경 pyright이 자꾸 타입 터쳐서 넣은 타입 캐스팅 코드입니다.\n",
    "    query_data = cast(Dataset, query_data)\n",
    "\n",
    "    query_df = pd.DataFrame({\n",
    "        'query': query_data['query'],\n",
    "        'corpusids': query_data['corpusids']\n",
    "    })\n",
    "\n",
    "    test_questions = [item['question'] for item in test_data]\n",
    "    filtered_query_df = query_df[query_df['query'].isin(test_questions)]\n",
    "\n",
    "    results = {}\n",
    "    result = []\n",
    "\n",
    "    for k in k_values:\n",
    "        total_recall = 0\n",
    "        count = 0\n",
    "        for i, item in enumerate(test_data):\n",
    "            query = item['question']\n",
    "            top_k_results = finetuner.query(query, k)\n",
    "\n",
    "            query_row = filtered_query_df[filtered_query_df['query'] == query]\n",
    "            if not query_row.empty:\n",
    "                true_corpus_ids = query_row.iloc[0]['corpusids']\n",
    "                if isinstance(true_corpus_ids, list):\n",
    "                    true_corpus_ids_flat = true_corpus_ids\n",
    "                else:\n",
    "                    true_corpus_ids_flat = [true_corpus_ids]\n",
    "\n",
    "                intersection = set(true_corpus_ids_flat) & set(top_k_results)\n",
    "                recall = len(intersection) / len(true_corpus_ids_flat) if true_corpus_ids_flat else 0\n",
    "\n",
    "                total_recall += recall\n",
    "                count += 1\n",
    "\n",
    "        if count > 0:\n",
    "            avg_recall = total_recall / count\n",
    "            results[f'Recall@{k}'] = avg_recall\n",
    "            result.append(avg_recall)\n",
    "        else:\n",
    "            results[f'Recall@{k}'] = 0\n",
    "            result.append(0)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fefaa43a-945f-4eb3-bdb5-cce27d741cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "def get_clean_corpusid(item: dict) -> int:\n",
    "    return item['corpusid']\n",
    "\n",
    "def get_clean_title(item: dict) -> str:\n",
    "    return item['title']\n",
    "\n",
    "def get_clean_abstract(item: dict) -> str:\n",
    "    return item['abstract']\n",
    "\n",
    "def get_clean_title_abstract(item: dict, tokenizer) -> str:\n",
    "    title = get_clean_title(item)\n",
    "    abstract = get_clean_abstract(item)\n",
    "    return title + tokenizer.sep_token + abstract\n",
    "\n",
    "def create_kv_pairs(data: Dataset, key: str, tokenizer) -> dict:\n",
    "    return {get_clean_title_abstract(record, tokenizer): get_clean_corpusid(record) for record in data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14885ca0-783b-43b6-a3ae-693816b56a8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 416 samples (Spec 0: 108, Spec 1: 308)\n",
      "Validation: 61 samples (Spec 0: 16, Spec 1: 45)\n",
      "Testing: 120 samples (Spec 0: 31, Spec 1: 89)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5f199474a443f5b8b35d0e797fba82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832883e2dc8940de826a036fc0cc19c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing document embeddings: 100%|██████████| 3604/3604 [25:16<00:00,  2.38it/s]\n",
      "Epoch 1/5:  96%|█████████▌| 50/52 [00:58<00:05,  2.79s/it, loss=0]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0189\n",
      "어댑터가 ./specter2_adhoc_query_finetuned에 저장되었습니다.\n",
      "Model saved to ./specter2_adhoc_query_finetuned (val_loss: 0.0189)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 52/52 [01:00<00:00,  1.17s/it, loss=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Avg Loss: 0.0650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  92%|█████████▏| 48/52 [00:56<00:11,  2.81s/it, loss=0.146] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0117\n",
      "어댑터가 ./specter2_adhoc_query_finetuned에 저장되었습니다.\n",
      "Model saved to ./specter2_adhoc_query_finetuned (val_loss: 0.0117)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 52/52 [01:00<00:00,  1.17s/it, loss=0.00184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Avg Loss: 0.0229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  88%|████████▊ | 46/52 [00:54<00:16,  2.80s/it, loss=0]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0104\n",
      "어댑터가 ./specter2_adhoc_query_finetuned에 저장되었습니다.\n",
      "Model saved to ./specter2_adhoc_query_finetuned (val_loss: 0.0104)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 52/52 [01:00<00:00,  1.17s/it, loss=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Avg Loss: 0.0220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  85%|████████▍ | 44/52 [00:52<00:22,  2.78s/it, loss=0]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 52/52 [01:00<00:00,  1.16s/it, loss=0]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Avg Loss: 0.0134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  81%|████████  | 42/52 [00:49<00:27,  2.78s/it, loss=0]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 52/52 [01:00<00:00,  1.16s/it, loss=0]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Avg Loss: 0.0163\n",
      "어댑터가 ./specter2_adhoc_query_finetuned에 저장되었습니다.\n",
      "Fine-tuning complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import cast\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(val_data)\n",
    "random.shuffle(test_data)\n",
    "\n",
    "print(f\"Training: {len(train_data)} samples (Spec 0: {len(spec_0_train)}, Spec 1: {len(spec_1_train)})\")\n",
    "print(f\"Validation: {len(val_data)} samples (Spec 0: {len(spec_0_val)}, Spec 1: {len(spec_1_val)})\")\n",
    "print(f\"Testing: {len(test_data)} samples (Spec 0: {len(spec_0_test)}, Spec 1: {len(spec_1_test)})\")\n",
    "\n",
    "# 학습 과정\n",
    "finetuner = SPECTER2QueryAdapterFinetuner()\n",
    "\n",
    "# 제 IDE 로컬 환경 pyright이 자꾸 타입 터쳐서 넣은 타입 캐스팅 코드입니다.\n",
    "corpus_clean_data = cast(Dataset, corpus_clean_data)\n",
    "kv_pairs = create_kv_pairs(corpus_clean_data, \"title_abstract\", finetuner.tokenizer)\n",
    "finetuner.clear()\n",
    "finetuner.create_index(kv_pairs)\n",
    "\n",
    "finetuner.finetune(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    output_dir=\"./specter2_adhoc_query_finetuned\",\n",
    "    lr=2e-4,\n",
    "    batch_size=8,\n",
    "    epochs=5,\n",
    "    margin=1.0,\n",
    "    eval_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85bf0e58-3acd-4483-8677-65ae50cb6bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on test set...\n",
      "Overall test performance: {'Recall@1': 0.19166666666666668, 'Recall@5': 0.3905555555555556, 'Recall@10': 0.5113888888888889, 'Recall@20': 0.6255555555555555}\n",
      "Specificity 0 performance: {'Recall@1': 0.11290322580645161, 'Recall@5': 0.31827956989247314, 'Recall@10': 0.5602150537634408, 'Recall@20': 0.6376344086021505}\n",
      "Specificity 1 performance: {'Recall@1': 0.19101123595505617, 'Recall@5': 0.3539325842696629, 'Recall@10': 0.5224719101123596, 'Recall@20': 0.6179775280898876}\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model on test set...\")\n",
    "\n",
    "query_data = load_dataset(\"princeton-nlp/LitSearch\", \"query\", split=\"full\")\n",
    "corpus_clean_data = load_dataset(\"princeton-nlp/LitSearch\", \"corpus_clean\", split=\"full\")\n",
    "\n",
    "# 전체 테스트 세트 평가\n",
    "overall_performance = evaluate_model(finetuner, test_data, query_data)\n",
    "print(f\"Overall test performance: {overall_performance}\")\n",
    "\n",
    "# Specificity 0 쿼리에 대한 평가\n",
    "spec_0_performance = evaluate_model(finetuner, spec_0_test, query_data)\n",
    "print(f\"Specificity 0 performance: {spec_0_performance}\")\n",
    "\n",
    "# Specificity 1 쿼리에 대한 평가\n",
    "spec_1_performance = evaluate_model(finetuner, spec_1_test, query_data)\n",
    "print(f\"Specificity 1 performance: {spec_1_performance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3136ed52-1929-4a85-b0f4-87395c7829e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_analysis(finetuner, test_data, query_data, k_values=[1, 5, 10, 20]):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        tuple: (결과 성능 지표, 실패 케이스 분석 데이터)\n",
    "    \"\"\"\n",
    "    query_data = cast(Dataset, query_data)\n",
    "\n",
    "    query_df = pd.DataFrame({\n",
    "        'query': query_data['query'],\n",
    "        'corpusids': query_data['corpusids']\n",
    "    })\n",
    "\n",
    "    test_questions = [item['question'] for item in test_data]\n",
    "    filtered_query_df = query_df[query_df['query'].isin(test_questions)]\n",
    "\n",
    "    results = {}\n",
    "    fail_cases = []\n",
    "    \n",
    "    # 가장 큰 k 값에 대해 전체 분석 정보 수집\n",
    "    max_k = max(k_values)\n",
    "    \n",
    "    test_progress = tqdm(enumerate(test_data), total=len(test_data), desc=f\"Evaluating & Analyzing\")\n",
    "\n",
    "    for i, item in test_progress:\n",
    "        query = item['question']\n",
    "        \n",
    "        # 원문 쿼리의 specificity 값을 저장\n",
    "        specificity = item.get('specificity', 'unknown')\n",
    "        \n",
    "        # 모델 결과 가져오기 (반환값을 확장하여 전체 결과와, 각 결과별 텍스트 정보 포함)\n",
    "        top_results = finetuner.query(query, max_k, return_keys=True)\n",
    "        top_results_ids = [result[1] for result in top_results]  # (key, value) 쌍에서 value만 추출\n",
    "        top_results_texts = [result[0] for result in top_results]  # (key, value) 쌍에서 key만 추출\n",
    "        \n",
    "        query_row = filtered_query_df[filtered_query_df['query'] == query]\n",
    "        \n",
    "        # 해당 쿼리를 찾지 못했다면 다음으로 넘어감\n",
    "        if query_row.empty:\n",
    "            continue\n",
    "            \n",
    "        true_corpus_ids = query_row.iloc[0]['corpusids']\n",
    "        \n",
    "        # true_corpus_ids가 리스트가 아니면 리스트로 변환\n",
    "        if isinstance(true_corpus_ids, list):\n",
    "            true_corpus_ids_flat = true_corpus_ids\n",
    "        else:\n",
    "            true_corpus_ids_flat = [true_corpus_ids]\n",
    "        \n",
    "        # 검색 결과에서 실제 corpus ID를 찾지 못한 경우 (Recall@max_k가 0인 경우)\n",
    "        intersection = set(true_corpus_ids_flat) & set(top_results_ids)\n",
    "        if not intersection:\n",
    "            # 실패 케이스 수집\n",
    "            fail_case = {\n",
    "                'query': query,\n",
    "                'specificity': specificity,\n",
    "                'true_corpus_ids': true_corpus_ids_flat,\n",
    "                'retrieved_ids': top_results_ids,\n",
    "                'retrieved_texts': top_results_texts,\n",
    "                'ranks': {corpus_id: 'not found' for corpus_id in true_corpus_ids_flat}\n",
    "            }\n",
    "            fail_cases.append(fail_case)\n",
    "        else:\n",
    "            # 일부 정답을 찾았지만 일부는 놓친 경우\n",
    "            missing_ids = set(true_corpus_ids_flat) - intersection\n",
    "            if missing_ids:\n",
    "                # 찾은 corpus ID의 순위와 놓친 ID 정보를 포함한 분석 데이터 생성\n",
    "                ranks = {}\n",
    "                for corpus_id in true_corpus_ids_flat:\n",
    "                    if corpus_id in top_results_ids:\n",
    "                        ranks[corpus_id] = top_results_ids.index(corpus_id) + 1  # 1-based 순위\n",
    "                    else:\n",
    "                        ranks[corpus_id] = 'not found'\n",
    "                \n",
    "                fail_case = {\n",
    "                    'query': query,\n",
    "                    'specificity': specificity,\n",
    "                    'true_corpus_ids': true_corpus_ids_flat,\n",
    "                    'retrieved_ids': top_results_ids,\n",
    "                    'retrieved_texts': top_results_texts,\n",
    "                    'ranks': ranks,\n",
    "                    'partially_found': True\n",
    "                }\n",
    "                fail_cases.append(fail_case)\n",
    "        \n",
    "        # k 값별로 Recall 계산\n",
    "        for k in k_values:\n",
    "            key = f'Recall@{k}'\n",
    "            if key not in results:\n",
    "                results[key] = {'total': 0, 'count': 0}\n",
    "            \n",
    "            top_k_ids = top_results_ids[:k]\n",
    "            recall = len(set(true_corpus_ids_flat) & set(top_k_ids)) / len(true_corpus_ids_flat) if true_corpus_ids_flat else 0\n",
    "            \n",
    "            results[key]['total'] += recall\n",
    "            results[key]['count'] += 1\n",
    "\n",
    "    # 평균 recall 계산\n",
    "    final_results = {}\n",
    "    for k in k_values:\n",
    "        key = f'Recall@{k}'\n",
    "        if results[key]['count'] > 0:\n",
    "            final_results[key] = results[key]['total'] / results[key]['count']\n",
    "        else:\n",
    "            final_results[key] = 0\n",
    "\n",
    "    # 실패 케이스를 specificity 별로 분류\n",
    "    spec_0_fails = [case for case in fail_cases if case['specificity'] == 0]\n",
    "    spec_1_fails = [case for case in fail_cases if case['specificity'] == 1]\n",
    "    \n",
    "    fail_summary = {\n",
    "        'total_fails': len(fail_cases),\n",
    "        'specificity_0_fails': len(spec_0_fails),\n",
    "        'specificity_1_fails': len(spec_1_fails),\n",
    "        'fail_ratio': len(fail_cases) / len(test_data) if test_data else 0,\n",
    "        'spec0_fail_ratio': len(spec_0_fails) / len([item for item in test_data if item.get('specificity') == 0]) if test_data else 0,\n",
    "        'spec1_fail_ratio': len(spec_1_fails) / len([item for item in test_data if item.get('specificity') == 1]) if test_data else 0,\n",
    "    }\n",
    "    \n",
    "    analysis_data = {\n",
    "        'fail_cases': fail_cases,\n",
    "        'fail_summary': fail_summary\n",
    "    }\n",
    "\n",
    "    return final_results, analysis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08fb9f67-51fd-4c47-9f55-6e5840fe5db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_failures(finetuner, query_data):\n",
    "    # 데이터 불러오기\n",
    "    test_spec_0 = load_from_json('./datasets/spec0_test.json')\n",
    "    test_spec_1 = load_from_json('./datasets/spec1_test.json')\n",
    "    test_data = test_spec_0 + test_spec_1\n",
    "    \n",
    "    # 평가 및 정성분석 실행\n",
    "    results, analysis_data = evaluate_model_with_analysis(finetuner, test_data, query_data)\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(f\"Overall performance: {results}\")\n",
    "    print(f\"Failure analysis summary: {analysis_data['fail_summary']}\")\n",
    "    \n",
    "    # 상세 실패 케이스 분석을 위한 정보 출력\n",
    "    print(\"\\n===== 실패 케이스 샘플 (최대 5개) =====\")\n",
    "    for i, case in enumerate(analysis_data['fail_cases'][:5]):\n",
    "        print(f\"\\nCase {i+1}:\")\n",
    "        print(f\"Query: {case['query']}\")\n",
    "        print(f\"Specificity: {case['specificity']}\")\n",
    "        print(f\"True corpus IDs: {case['true_corpus_ids']}\")\n",
    "        print(f\"True corpus ID 순위: {case['ranks']}\")\n",
    "        print(f\"Retrieved top 3 documents:\")\n",
    "        for j, (doc_id, doc_text) in enumerate(zip(case['retrieved_ids'][:3], case['retrieved_texts'][:3])):\n",
    "            # 텍스트가 너무 길면 잘라서 표시\n",
    "            shortened_text = doc_text[:200] + \"...\" if len(doc_text) > 200 else doc_text\n",
    "            print(f\"  {j+1}. ID: {doc_id}, Text: {shortened_text}\")\n",
    "    \n",
    "    # 실패 케이스 데이터 저장\n",
    "    with open(\"failure_analysis.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(analysis_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"\\n상세 분석 결과가 failure_analysis.json에 저장되었습니다.\")\n",
    "    \n",
    "    return analysis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a06e8e9e-6371-4e64-a43e-d7ac8058a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_patterns(analysis_data):\n",
    "    fail_cases = analysis_data['fail_cases']\n",
    "    \n",
    "    # 1. 쿼리 길이에 따른 분석\n",
    "    query_lengths = [len(case['query'].split()) for case in fail_cases]\n",
    "    avg_query_length = sum(query_lengths) / len(query_lengths) if query_lengths else 0\n",
    "    \n",
    "    # 2. Specificity별 분석\n",
    "    spec_0_fails = [case for case in fail_cases if case['specificity'] == 0]\n",
    "    spec_1_fails = [case for case in fail_cases if case['specificity'] == 1]\n",
    "    \n",
    "    # 3. 검색된 문서들과 정답 문서 간의 유사성 분석\n",
    "    # (이 부분은 문서 내용을 분석해야 하므로 추가 데이터가 필요할 수 있음)\n",
    "    \n",
    "    patterns = {\n",
    "        'query_length_analysis': {\n",
    "            'min_length': min(query_lengths) if query_lengths else 0,\n",
    "            'max_length': max(query_lengths) if query_lengths else 0,\n",
    "            'avg_length': avg_query_length,\n",
    "            'length_distribution': {\n",
    "                'short (< 5 words)': len([l for l in query_lengths if l < 5]),\n",
    "                'medium (5-10 words)': len([l for l in query_lengths if 5 <= l <= 10]),\n",
    "                'long (> 10 words)': len([l for l in query_lengths if l > 10])\n",
    "            }\n",
    "        },\n",
    "        'specificity_analysis': {\n",
    "            'spec_0_count': len(spec_0_fails),\n",
    "            'spec_1_count': len(spec_1_fails),\n",
    "            'spec_0_avg_query_length': sum(len(case['query'].split()) for case in spec_0_fails) / len(spec_0_fails) if spec_0_fails else 0,\n",
    "            'spec_1_avg_query_length': sum(len(case['query'].split()) for case in spec_1_fails) / len(spec_1_fails) if spec_1_fails else 0,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\n===== 실패 케이스 패턴 분석 =====\")\n",
    "    print(f\"평균 쿼리 길이: {patterns['query_length_analysis']['avg_length']:.2f} 단어\")\n",
    "    print(f\"쿼리 길이 분포: {patterns['query_length_analysis']['length_distribution']}\")\n",
    "    print(f\"Specificity 0 실패 케이스: {patterns['specificity_analysis']['spec_0_count']} (평균 쿼리 길이: {patterns['specificity_analysis']['spec_0_avg_query_length']:.2f})\")\n",
    "    print(f\"Specificity 1 실패 케이스: {patterns['specificity_analysis']['spec_1_count']} (평균 쿼리 길이: {patterns['specificity_analysis']['spec_1_avg_query_length']:.2f})\")\n",
    "    \n",
    "    # 정답 문서와 상위 검색 결과 간의 주제/내용 일치도 분석을 위한 제안\n",
    "    print(\"\\n실패 케이스의 정답 문서와 검색된 문서 간의 주제/내용 일치도 분석을 위해서는\")\n",
    "    print(\"문서 전체 내용에 대한 접근과 추가적인 텍스트 분석이 필요합니다.\")\n",
    "    \n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb925f6b-c780-4790-bbe2-d93859ba8091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709094761d67439585ff082f1e08de28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c90a6447bb2746299a35eee96d449555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating & Analyzing:   0%|          | 0/120 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "FAISS index has not been created yet. Call create_index first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m analysis_data \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinetuner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 패턴 분석 실행\u001b[39;00m\n\u001b[1;32m      4\u001b[0m patterns \u001b[38;5;241m=\u001b[39m analyze_patterns(analysis_data)\n",
      "Cell \u001b[0;32mIn[31], line 11\u001b[0m, in \u001b[0;36manalyze_failures\u001b[0;34m(finetuner, query_data)\u001b[0m\n\u001b[1;32m      8\u001b[0m finetuner \u001b[38;5;241m=\u001b[39m SPECTER2QueryAdapterFinetuner()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 평가 및 정성분석 실행\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m results, analysis_data \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model_with_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinetuner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 결과 출력\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverall performance: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[30], line 31\u001b[0m, in \u001b[0;36mevaluate_model_with_analysis\u001b[0;34m(finetuner, test_data, query_data, k_values)\u001b[0m\n\u001b[1;32m     28\u001b[0m specificity \u001b[38;5;241m=\u001b[39m item\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecificity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 모델 결과 가져오기 (반환값을 확장하여 전체 결과와, 각 결과별 텍스트 정보 포함)\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m top_results \u001b[38;5;241m=\u001b[39m \u001b[43mfinetuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m top_results_ids \u001b[38;5;241m=\u001b[39m [result[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m top_results]  \u001b[38;5;66;03m# (key, value) 쌍에서 value만 추출\u001b[39;00m\n\u001b[1;32m     33\u001b[0m top_results_texts \u001b[38;5;241m=\u001b[39m [result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m top_results]  \u001b[38;5;66;03m# (key, value) 쌍에서 key만 추출\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 283\u001b[0m, in \u001b[0;36mSPECTER2QueryAdapterFinetuner.query\u001b[0;34m(self, query_text, n, return_keys)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_text: \u001b[38;5;28mstr\u001b[39m, n: \u001b[38;5;28mint\u001b[39m, return_keys: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    282\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_embeddings([query_text], TextType\u001b[38;5;241m.\u001b[39mQUERY)\n\u001b[0;32m--> 283\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_keys:\n\u001b[1;32m    286\u001b[0m         results \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys[i], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[18], line 273\u001b[0m, in \u001b[0;36mSPECTER2QueryAdapterFinetuner._query\u001b[0;34m(self, query_embedding, top_k)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_embedding: torch\u001b[38;5;241m.\u001b[39mTensor, top_k: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfaiss_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFAISS index has not been created yet. Call create_index first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    275\u001b[0m     query_vector \u001b[38;5;241m=\u001b[39m query_embedding\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    276\u001b[0m     faiss\u001b[38;5;241m.\u001b[39mnormalize_L2(query_vector)\n",
      "\u001b[0;31mValueError\u001b[0m: FAISS index has not been created yet. Call create_index first."
     ]
    }
   ],
   "source": [
    "analysis_data = analyze_failures(finetuner, query_data)\n",
    "\n",
    "# 패턴 분석 실행\n",
    "patterns = analyze_patterns(analysis_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
