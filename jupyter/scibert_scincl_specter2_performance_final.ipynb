{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f9dbf0b-fe9d-4fc6-81ed-42d1d5168b7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (25.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8fb5f6b-65d3-465b-9f91-e4362491266f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting faiss-gpu\n",
      "  Using cached faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting adapters\n",
      "  Using cached adapters-1.1.1-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.14.0)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.31.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.2.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Downloading huggingface_hub-0.31.1-py3-none-any.whl (484 kB)\n",
      "Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Using cached adapters-1.1.1-py3-none-any.whl (289 kB)\n",
      "Using cached transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
      "Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Using cached multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
      "Using cached yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "Using cached propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: faiss-gpu, xxhash, safetensors, regex, pyarrow, propcache, multidict, hf-xet, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, sentence-transformers, datasets, adapters\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/22\u001b[0m [adapters]/22\u001b[0m [adapters]transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed adapters-1.1.1 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.6.0 dill-0.3.8 faiss-gpu-1.7.2 frozenlist-1.6.0 hf-xet-1.1.0 huggingface-hub-0.31.1 multidict-6.4.3 multiprocess-0.70.16 propcache-0.3.1 pyarrow-20.0.0 regex-2024.11.6 safetensors-0.5.3 sentence-transformers-4.1.0 tokenizers-0.21.1 transformers-4.48.3 xxhash-3.5.0 yarl-1.20.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tqdm sentence-transformers transformers faiss-gpu datasets adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bce9c4-014d-45d0-9902-02e6da94b614",
   "metadata": {},
   "source": [
    "Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e07fe8c9-29e7-4270-bef3-cd000f088925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "query_data = load_dataset(\"princeton-nlp/LitSearch\", \"query\", split=\"full\")\n",
    "corpus_clean_data = load_dataset(\"princeton-nlp/LitSearch\", \"corpus_clean\", split=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "431d2602-84fd-4304-90d1-a3c9a5dab0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Any\n",
    "from datasets import Dataset\n",
    "\n",
    "def get_clean_corpusid(item: dict) -> int:\n",
    "    return item['corpusid']\n",
    "\n",
    "def get_clean_title(item: dict) -> str:\n",
    "    return item['title']\n",
    "\n",
    "def get_clean_abstract(item: dict) -> str:\n",
    "    return item['abstract']\n",
    "\n",
    "def get_clean_title_abstract(item: dict) -> str:\n",
    "    title = get_clean_title(item)\n",
    "    abstract = get_clean_abstract(item)\n",
    "    return title + '[SEP]' + abstract\n",
    "\n",
    "def get_clean_full_paper(item: dict) -> str:\n",
    "    return item['full_paper']\n",
    "\n",
    "def get_clean_paragraph_indices(item: dict) -> List[Tuple[int, int]]:\n",
    "    text = get_clean_full_paper(item)\n",
    "    paragraph_indices = []\n",
    "    paragraph_start = 0\n",
    "    paragraph_end = 0\n",
    "    while paragraph_start < len(text):\n",
    "        paragraph_end = text.find(\"\\n\\n\", paragraph_start)\n",
    "        if paragraph_end == -1:\n",
    "            paragraph_end = len(text)\n",
    "        paragraph_indices.append((paragraph_start, paragraph_end))\n",
    "        paragraph_start = paragraph_end + 2\n",
    "    return paragraph_indices\n",
    "\n",
    "def get_clean_text(item: dict, start_idx: int, end_idx: int) -> str:\n",
    "    text = get_clean_full_paper(item)\n",
    "    assert start_idx >= 0 and end_idx >= 0\n",
    "    assert start_idx <= end_idx\n",
    "    assert end_idx <= len(text)\n",
    "    return text[start_idx:end_idx]\n",
    "\n",
    "def get_clean_paragraphs(item: dict, min_words: int = 10) -> List[str]:\n",
    "    paragraph_indices = get_clean_paragraph_indices(item)\n",
    "    paragraphs = [get_clean_text(item, paragraph_start, paragraph_end) for paragraph_start, paragraph_end in paragraph_indices]\n",
    "    paragraphs = [paragraph for paragraph in paragraphs if len(paragraph.split()) >= min_words]\n",
    "    return paragraphs\n",
    "\n",
    "def get_clean_citations(item: dict) -> List[int]:\n",
    "    return item['citations']\n",
    "\n",
    "def get_clean_dict(data: Dataset) -> dict:\n",
    "    return {get_clean_corpusid(item): item for item in data}\n",
    "\n",
    "def create_kv_pairs(data: List[dict], key: str) -> dict:\n",
    "    if key == \"title_abstract\":\n",
    "        kv_pairs = {get_clean_title_abstract(record): get_clean_corpusid(record) for record in data}\n",
    "    elif key == \"full_paper\":\n",
    "        kv_pairs = {get_clean_full_paper(record): get_clean_corpusid(record) for record in data}\n",
    "    elif key == \"paragraphs\":\n",
    "        kv_pairs = {}\n",
    "        for record in data:\n",
    "            corpusid = get_clean_corpusid(record)\n",
    "            paragraphs = get_clean_paragraphs(record)\n",
    "            for paragraph_idx, paragraph in enumerate(paragraphs):\n",
    "                kv_pairs[paragraph] = (corpusid, paragraph_idx)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid key\")\n",
    "    return kv_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd8bf03d-f148-4e72-a610-095fadb4261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_pairs = create_kv_pairs(corpus_clean_data, \"title_abstract\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e87088-38d5-4af5-bf32-a4af77041521",
   "metadata": {},
   "source": [
    "Model\n",
    "1. Scibert\n",
    "2. SciNCL\n",
    "3. SPECTER2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f45b35-990a-4a53-9f45-c9351dc2c9d7",
   "metadata": {},
   "source": [
    "Parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9cebba5-20a7-48ed-b788-b30973bcce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class TextType(Enum):\n",
    "    KEY = 1\n",
    "    QUERY = 2\n",
    "\n",
    "class Retrieval:\n",
    "    def __init__(self, index_name: str, index_type: str) -> None:\n",
    "        self.index_name = index_name\n",
    "        self.index_type = index_type\n",
    "\n",
    "        self.keys = []\n",
    "        self.values = []\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.keys)\n",
    "\n",
    "    def _get_embeddings(self, textList: List[str], type: TextType, show_progress_bar: bool = False) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _query(self, query_embedding: np.ndarray, top_k: int = 10) -> List[int]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def query(self, query_text: str, n: int, return_keys: bool = False) -> List[Any]:\n",
    "        embedding_query = self._get_embeddings([query_text], TextType.QUERY)\n",
    "        indices = self._query(embedding_query, n)\n",
    "        if return_keys:\n",
    "            results = [(self.keys[i], self.values[i]) for i in indices]\n",
    "        else:\n",
    "            results = [self.values[i] for i in indices]\n",
    "        return results\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.keys = []\n",
    "        self.encoded_keys = []\n",
    "        self.values = []\n",
    "\n",
    "    def create_index(self, key_value_pairs: Dict[str, int]) -> None:\n",
    "        if len(self.keys) > 0:\n",
    "            raise ValueError(\"Index is not empty. Please create a new index or clear the existing one.\")\n",
    "\n",
    "        for key, value in key_value_pairs.items():\n",
    "            self.keys.append(key)\n",
    "            self.values.append(value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5575624e-2819-46b1-8058-464002228875",
   "metadata": {},
   "source": [
    "SciBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d67946bc-d6e5-4f89-af80-40e6f363e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import faiss\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class SciBert(Retrieval):\n",
    "    def __init__(self, index_name: str):\n",
    "        super().__init__(index_name, 'SciBert')\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "        self._model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased').to(self.device)\n",
    "        self.index = None\n",
    "        self.faiss_index = None\n",
    "\n",
    "        def clear(self):\n",
    "            super().clear()\n",
    "            self.index = None\n",
    "            self.faiss_index = None\n",
    "\n",
    "    def _get_embeddings(self, textList: List[str], type: TextType, show_progress_bar: bool = True) -> torch.Tensor:\n",
    "        batch_size = 32\n",
    "        embeddings = []\n",
    "\n",
    "        should_show_progress = show_progress_bar and (type == TextType.KEY)\n",
    "\n",
    "        iterator = range(0, len(textList), batch_size)\n",
    "        if should_show_progress:\n",
    "            iterator = tqdm(iterator, desc=\"Processing document embeddings\")\n",
    "\n",
    "        for i in iterator:\n",
    "            batch_texts = textList[i:i+batch_size]\n",
    "            encoded = self._tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=512\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self._model(**encoded)\n",
    "\n",
    "            batch_embeddings = outputs.last_hidden_state[:,0,:].cpu()\n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "            if i % (batch_size * 10) == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        embeddings = torch.cat(embeddings, dim=0)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def create_index(self, key_value_pairs: Dict[str, Any]) -> None:\n",
    "        super().create_index(key_value_pairs)\n",
    "        self.index = self._get_embeddings(self.keys, TextType.KEY)\n",
    "\n",
    "        # FAISS 인덱스 생성\n",
    "        vector_dim = self.index.shape[1]\n",
    "        index_flat = faiss.IndexFlatIP(vector_dim)\n",
    "        index_vectors = self.index.numpy()\n",
    "        faiss.normalize_L2(index_vectors)\n",
    "\n",
    "        # 인덱스에 벡터 추가\n",
    "        index_flat.add(index_vectors)\n",
    "        self.faiss_index = index_flat\n",
    "\n",
    "    def _query(self, query_embedding: torch.Tensor, top_k: int = 10) -> List[int]:\n",
    "        if self.faiss_index is None:\n",
    "            raise ValueError(\"FAISS index has not been created yet. Call create_index first.\")\n",
    "\n",
    "        query_vector = query_embedding.numpy()\n",
    "        faiss.normalize_L2(query_vector)\n",
    "        distances, indices = self.faiss_index.search(query_vector, top_k)\n",
    "\n",
    "        return indices[0].tolist()\n",
    "\n",
    "    def query(self, query_text: str, n: int, return_keys: bool = False) -> List[Any]:\n",
    "        query_embedding = self._get_embeddings([query_text], TextType.QUERY)\n",
    "        indices = self._query(query_embedding, n)\n",
    "\n",
    "        if return_keys:\n",
    "            results = [(self.keys[i], self.values[i]) for i in indices]\n",
    "        else:\n",
    "            results = [self.values[i] for i in indices]\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaa3a03-b80e-4067-803f-22e8533c73ec",
   "metadata": {},
   "source": [
    "2. SciNCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6595932-c0fe-4373-8485-2d3462278ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scincl(Retrieval):\n",
    "    def __init__(self, index_name:str):\n",
    "        super().__init__(index_name, 'SciNCL')\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "        self._model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased').to(self.device)\n",
    "        self.index = None\n",
    "        self.faiss_index = None\n",
    "\n",
    "    def clear(self):\n",
    "        super().clear()\n",
    "        self.index = None\n",
    "        self.faiss_index = None\n",
    "\n",
    "    def _get_embeddings(self, textList: List[str], type: TextType, show_progress_bar: bool = True) -> torch.Tensor:\n",
    "        batch_size = 32\n",
    "        embeddings = []\n",
    "\n",
    "        should_show_progress = show_progress_bar and (type == TextType.KEY)\n",
    "\n",
    "        iterator = range(0, len(textList), batch_size)\n",
    "        if should_show_progress:\n",
    "            iterator = tqdm(iterator, desc=\"Processing document embeddings\")\n",
    "\n",
    "        for i in iterator:\n",
    "            batch_texts = textList[i:i+batch_size]\n",
    "            encoded = self._tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=512\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self._model(**encoded)\n",
    "\n",
    "            batch_embeddings = outputs.last_hidden_state[:,0,:].cpu()\n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "            if i % (batch_size * 10) == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        embeddings = torch.cat(embeddings, dim=0)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def create_index(self, key_value_pairs: Dict[str, Any]) -> None:\n",
    "        super().create_index(key_value_pairs)\n",
    "        self.index = self._get_embeddings(self.keys, TextType.KEY)\n",
    "\n",
    "        # FAISS 인덱스 생성\n",
    "        vector_dim = self.index.shape[1]\n",
    "        index_flat = faiss.IndexFlatIP(vector_dim)\n",
    "        index_vectors = self.index.numpy()\n",
    "        faiss.normalize_L2(index_vectors)\n",
    "\n",
    "        # 인덱스에 벡터 추가\n",
    "        index_flat.add(index_vectors)\n",
    "        self.faiss_index = index_flat\n",
    "\n",
    "    def _query(self, query_embedding: torch.Tensor, top_k: int = 10) -> List[int]:\n",
    "        if self.faiss_index is None:\n",
    "            raise ValueError(\"FAISS index has not been created yet. Call create_index first.\")\n",
    "\n",
    "        query_vector = query_embedding.numpy()\n",
    "        faiss.normalize_L2(query_vector)\n",
    "        distances, indices = self.faiss_index.search(query_vector, top_k)\n",
    "\n",
    "        return indices[0].tolist()\n",
    "\n",
    "    def query(self, query_text: str, n: int, return_keys: bool = False) -> List[Any]:\n",
    "        query_embedding = self._get_embeddings([query_text], TextType.QUERY)\n",
    "        indices = self._query(query_embedding, n)\n",
    "\n",
    "        if return_keys:\n",
    "            results = [(self.keys[i], self.values[i]) for i in indices]\n",
    "        else:\n",
    "            results = [self.values[i] for i in indices]\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ed26a-4635-4cea-a8c0-4e1e39c3b8bc",
   "metadata": {},
   "source": [
    "3. SPECTER2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "781a2e91-fb34-40c6-b0b3-807974c9b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any, Tuple, Dict, Optional\n",
    "from transformers import AutoTokenizer\n",
    "from adapters import AutoAdapterModel\n",
    "\n",
    "class Specter2(Retrieval):\n",
    "    def __init__(self, index_name: str):\n",
    "        super().__init__(index_name, 'SPECTER2')\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # SPECTER2 base model과 tokenizer 로드\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_base')\n",
    "        self._model = AutoAdapterModel.from_pretrained('allenai/specter2_base')\n",
    "\n",
    "        # Retrieval용 proximity adapter 로드 (문서 임베딩용)\n",
    "        self._model.load_adapter(\"allenai/specter2\", source=\"hf\", load_as=\"proximity\")\n",
    "        # Adhoc query adapter 로드 (쿼리 임베딩용)\n",
    "        self._model.load_adapter(\"allenai/specter2_adhoc_query\", source=\"hf\", load_as=\"adhoc_query\")\n",
    "\n",
    "\n",
    "        self._model.to(self.device)\n",
    "        self.index = None\n",
    "        self.faiss_index = None\n",
    "\n",
    "    def clear(self):\n",
    "        super().clear()\n",
    "        self.index = None\n",
    "        self.faiss_index = None\n",
    "\n",
    "    def _get_embeddings(self, textList: List[str], type: TextType, show_progress_bar: bool = True) -> torch.Tensor:\n",
    "\n",
    "        if type == TextType.KEY:\n",
    "            self._model.set_active_adapters(\"proximity\")\n",
    "        else:\n",
    "            self._model.set_active_adapters(\"adhoc_query\")\n",
    "\n",
    "        batch_size = 32\n",
    "        embeddings = []\n",
    "\n",
    "        should_show_progress = show_progress_bar and (type == TextType.KEY)\n",
    "\n",
    "        iterator = range(0, len(textList), batch_size)\n",
    "        if should_show_progress:\n",
    "            iterator = tqdm(iterator, desc=\"Processing document embeddings\")\n",
    "\n",
    "        for i in iterator:\n",
    "            batch_texts = textList[i:i+batch_size]\n",
    "            encoded = self._tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=512\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self._model(**encoded)\n",
    "\n",
    "            batch_embeddings = outputs.last_hidden_state[:,0,:].cpu()\n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "            if i % (batch_size * 10) == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        embeddings = torch.cat(embeddings, dim=0)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def create_index(self, key_value_pairs: Dict[str, Any]) -> None:\n",
    "        super().create_index(key_value_pairs)\n",
    "        self.index = self._get_embeddings(self.keys, TextType.KEY)\n",
    "\n",
    "        # FAISS 인덱스 생성\n",
    "        vector_dim = self.index.shape[1]\n",
    "        index_flat = faiss.IndexFlatIP(vector_dim)\n",
    "        index_vectors = self.index.numpy()\n",
    "        faiss.normalize_L2(index_vectors)\n",
    "\n",
    "        # 인덱스에 벡터 추가\n",
    "        index_flat.add(index_vectors)\n",
    "        self.faiss_index = index_flat\n",
    "\n",
    "    def _query(self, query_embedding: torch.Tensor, top_k: int = 10) -> List[int]:\n",
    "        if self.faiss_index is None:\n",
    "            raise ValueError(\"FAISS index has not been created yet. Call create_index first.\")\n",
    "\n",
    "        query_vector = query_embedding.numpy()\n",
    "        faiss.normalize_L2(query_vector)\n",
    "        distances, indices = self.faiss_index.search(query_vector, top_k)\n",
    "\n",
    "        return indices[0].tolist()\n",
    "\n",
    "    def query(self, query_text: str, n: int, return_keys: bool = False) -> List[Any]:\n",
    "        query_embedding = self._get_embeddings([query_text], TextType.QUERY)\n",
    "        indices = self._query(query_embedding, n)\n",
    "\n",
    "        if return_keys:\n",
    "            results = [(self.keys[i], self.values[i]) for i in indices]\n",
    "        else:\n",
    "            results = [self.values[i] for i in indices]\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf63e39-000f-438e-abd4-9d290e9d4119",
   "metadata": {},
   "source": [
    "Evaluate for litSearch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac4f934b-90ad-4f44-966f-938f1a433180",
   "metadata": {},
   "outputs": [],
   "source": [
    "scibert = SciBert(\"Title_Abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc0848c8-9c74-474e-ae6b-a75a47933e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing document embeddings: 100%|██████████| 1802/1802 [11:39<00:00,  2.58it/s]\n"
     ]
    }
   ],
   "source": [
    "scibert.create_index(kv_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3deb016-db7a-4535-a7c8-42d09344fd18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 597/597 [00:39<00:00, 15.09it/s]\n"
     ]
    }
   ],
   "source": [
    "scibert_query_set = [query for query in query_data]\n",
    "for query in tqdm(scibert_query_set):\n",
    "    query_text = query[\"query\"]\n",
    "    top_k = scibert.query(query_text, 20)\n",
    "    query[\"retrieved\"] = top_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26ba62-cbef-43f5-9e2d-ae55b230287f",
   "metadata": {},
   "source": [
    "Separate specific and broad set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cf44544b-b55e-4441-a815-74217802d9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(corpusids: list, retrieved: list, k: int):\n",
    "    top_k = retrieved[:k]\n",
    "    intersection = set(corpusids) & set(top_k)\n",
    "    return len(intersection) / len(corpusids) if corpusids and intersection else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "11f870ca-673e-4250-a6b7-df8bbd91453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "scibert_query_df = pd.DataFrame(scibert_query_set)\n",
    "scibert_broad_query_df = scibert_query_df[scibert_query_df['specificity'] == 0]\n",
    "scibert_specific_query_df = scibert_query_df[scibert_query_df['specificity'] == 1]\n",
    "\n",
    "scibert_broad_inline_query_df = scibert_broad_query_df[scibert_broad_query_df['query_set'].str.contains('inline')]\n",
    "scibert_broad_manual_query_df = scibert_broad_query_df[scibert_broad_query_df['query_set'].str.contains('manual')]\n",
    "\n",
    "scibert_specific_inline_query_df = scibert_specific_query_df[scibert_specific_query_df['query_set'].str.contains('inline')]\n",
    "scibert_specific_manual_query_df = scibert_specific_query_df[scibert_specific_query_df['query_set'].str.contains('manual')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e4d00dd5-66e8-4730-9f8e-b21b8454ed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_scibert_broad_inline = []\n",
    "recall_scibert_broad_manual = []\n",
    "recall_scibert_specific_inline = []\n",
    "recall_scibert_specific_manual = []\n",
    "\n",
    "for _, query in scibert_broad_inline_query_df.iterrows():\n",
    "    result = calculate_recall(query['corpusids'], query['retrieved'], 20)\n",
    "    recall_scibert_broad_inline.append(result)\n",
    "\n",
    "for _, query in scibert_broad_manual_query_df.iterrows():\n",
    "    result = calculate_recall(query['corpusids'], query['retrieved'], 20)\n",
    "    recall_scibert_broad_manual.append(result)\n",
    "\n",
    "for _, query in scibert_specific_inline_query_df.iterrows():\n",
    "    result = calculate_recall(query['corpusids'], query['retrieved'], 20)\n",
    "    recall_scibert_specific_inline.append(result)\n",
    "\n",
    "for _, query in scibert_specific_manual_query_df.iterrows():\n",
    "    result = calculate_recall(query['corpusids'], query['retrieved'], 20)\n",
    "    recall_scibert_specific_manual.append(result)\n",
    "\n",
    "result_scibert_broad_inline = np.mean(recall_scibert_broad_inline)\n",
    "result_scibert_broad_manual = np.mean(recall_scibert_broad_manual)\n",
    "result_scibert_specific_inline = np.mean(recall_scibert_specific_inline)\n",
    "result_scibert_specific_manual = np.mean(recall_scibert_specific_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a0400271-05cc-4d17-a2e0-0cea99f5d0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "SciBERT Model Evaluation Results\n",
      "==================================================\n",
      "Query Type           Inline          Manual         \n",
      "--------------------------------------------------\n",
      "Broad Queries        0.0000          0.0000\n",
      "Specific Queries     0.0000          0.0000\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"SciBERT Model Evaluation Results\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Query Type':<20} {'Inline':<15} {'Manual':<15}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Broad Queries':<20} {result_scibert_broad_inline:.4f}{' '*9} {result_scibert_broad_manual:.4f}\")\n",
    "print(f\"{'Specific Queries':<20} {result_scibert_specific_inline:.4f}{' '*9} {result_scibert_specific_manual:.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "debfb646-e1ff-4c7d-8bfc-4cf28c314f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [1, 5, 10, 20]\n",
    "recalls = {\n",
    "    'scibert_broad_inline': {k: [] for k in k_values},\n",
    "    'scibert_broad_manual': {k: [] for k in k_values},\n",
    "    'scibert_specific_inline': {k: [] for k in k_values},\n",
    "    'scibert_specific_manual': {k: [] for k in k_values}\n",
    "}\n",
    "\n",
    "for _, query in scibert_broad_inline_query_df.iterrows():\n",
    "    for k in k_values:\n",
    "        result = calculate_recall(query['corpusids'], query['retrieved'], k)\n",
    "        recalls['scibert_broad_inline'][k].append(result)\n",
    "\n",
    "for _, query in scibert_broad_manual_query_df.iterrows():\n",
    "    for k in k_values:\n",
    "        result = calculate_recall(query['corpusids'], query['retrieved'], k)\n",
    "        recalls['scibert_broad_manual'][k].append(result)\n",
    "\n",
    "for _, query in scibert_specific_inline_query_df.iterrows():\n",
    "    for k in k_values:\n",
    "        result = calculate_recall(query['corpusids'], query['retrieved'], k)\n",
    "        recalls['scibert_specific_inline'][k].append(result)\n",
    "\n",
    "for _, query in scibert_specific_manual_query_df.iterrows():\n",
    "    for k in k_values:\n",
    "        result = calculate_recall(query['corpusids'], query['retrieved'], k)\n",
    "        recalls['scibert_specific_manual'][k].append(result)\n",
    "\n",
    "results = {\n",
    "    'scibert_broad_inline': {k: np.mean(recalls['scibert_broad_inline'][k]) for k in k_values},\n",
    "    'scibert_broad_manual': {k: np.mean(recalls['scibert_broad_manual'][k]) for k in k_values},\n",
    "    'scibert_specific_inline': {k: np.mean(recalls['scibert_specific_inline'][k]) for k in k_values},\n",
    "    'scibert_specific_manual': {k: np.mean(recalls['scibert_specific_manual'][k]) for k in k_values}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9eddc6e2-fb41-4c05-b158-0d016513e6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SciBERT Model Evaluation Results\n",
      "================================================================================\n",
      "Query Type           k=1             k=5             k=10            k=20           \n",
      "--------------------------------------------------------------------------------\n",
      "Broad Inline         0.0000          0.0000          0.0000          0.0000\n",
      "Broad Manual         0.0000          0.0000          0.0000          0.0000\n",
      "Specific Inline      0.0000          0.0000          0.0000          0.0000\n",
      "Specific Manual      0.0000          0.0000          0.0000          0.0000\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SciBERT Model Evaluation Results\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Query Type':<20} {'k=1':<15} {'k=5':<15} {'k=10':<15} {'k=20':<15}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Broad Inline':<20} {results['scibert_broad_inline'][1]:.4f}{' '*9} {results['scibert_broad_inline'][5]:.4f}{' '*9} {results['scibert_broad_inline'][10]:.4f}{' '*9} {results['scibert_broad_inline'][20]:.4f}\")\n",
    "print(f\"{'Broad Manual':<20} {results['scibert_broad_manual'][1]:.4f}{' '*9} {results['scibert_broad_manual'][5]:.4f}{' '*9} {results['scibert_broad_manual'][10]:.4f}{' '*9} {results['scibert_broad_manual'][20]:.4f}\")\n",
    "print(f\"{'Specific Inline':<20} {results['scibert_specific_inline'][1]:.4f}{' '*9} {results['scibert_specific_inline'][5]:.4f}{' '*9} {results['scibert_specific_inline'][10]:.4f}{' '*9} {results['scibert_specific_inline'][20]:.4f}\")\n",
    "print(f\"{'Specific Manual':<20} {results['scibert_specific_manual'][1]:.4f}{' '*9} {results['scibert_specific_manual'][5]:.4f}{' '*9} {results['scibert_specific_manual'][10]:.4f}{' '*9} {results['scibert_specific_manual'][20]:.4f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033e72a9-47eb-4c61-9e08-0ed09c51fd8f",
   "metadata": {},
   "source": [
    "2. SciNCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8173563d-48da-4091-829a-7ffd4c0c9824",
   "metadata": {},
   "outputs": [],
   "source": [
    "scincl = Scincl(\"Title_Abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9c06a24e-7c9d-402d-8188-a8342c659f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing document embeddings: 100%|██████████| 1802/1802 [11:39<00:00,  2.58it/s]\n"
     ]
    }
   ],
   "source": [
    "scincl.create_index(kv_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3ed9f8f1-0637-4fab-a073-4a38d54e01bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 597/597 [00:39<00:00, 15.15it/s]\n"
     ]
    }
   ],
   "source": [
    "scincl_query_set = [query for query in query_data]\n",
    "for query in tqdm(scincl_query_set):\n",
    "    query_text = query[\"query\"]\n",
    "    top_k = scincl.query(query_text, 20)\n",
    "    query[\"retrieved\"] = top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3c289c4a-2ad3-4c94-b4fb-4f482ae8c4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scincl_query_df = pd.DataFrame(scincl_query_set)\n",
    "scincl_broad_query_df = scincl_query_df[scincl_query_df['specificity'] == 0]\n",
    "scincl_specific_query_df = scincl_query_df[scincl_query_df['specificity'] == 1]\n",
    "\n",
    "scincl_broad_inline_query_df = scincl_broad_query_df[scincl_broad_query_df['query_set'].str.contains('inline')]\n",
    "scincl_broad_manual_query_df = scincl_broad_query_df[scincl_broad_query_df['query_set'].str.contains('manual')]\n",
    "\n",
    "scincl_specific_inline_query_df = scincl_specific_query_df[scincl_specific_query_df['query_set'].str.contains('inline')]\n",
    "scincl_specific_manual_query_df = scincl_specific_query_df[scincl_specific_query_df['query_set'].str.contains('manual')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "789cf4f1-da3f-4a9d-8079-48a2c9780983",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [1, 5, 10, 20]\n",
    "recalls = {\n",
    "    'scincl_broad_inline': {k: [] for k in k_values},\n",
    "    'scincl_broad_manual': {k: [] for k in k_values},\n",
    "    'scincl_specific_inline': {k: [] for k in k_values},\n",
    "    'scincl_specific_manual': {k: [] for k in k_values}\n",
    "}\n",
    "\n",
    "for _, query in scincl_broad_inline_query_df.iterrows():\n",
    "    for k in k_values:\n",
    "        result = calculate_recall(query['corpusids'], query['retrieved'], k)\n",
    "        recalls['scincl_broad_inline'][k].append(result)\n",
    "\n",
    "for _, query in scincl_broad_manual_query_df.iterrows():\n",
    "    for k in k_values:\n",
    "        result = calculate_recall(query['corpusids'], query['retrieved'], k)\n",
    "        recalls['scincl_broad_manual'][k].append(result)\n",
    "\n",
    "for _, query in scincl_specific_inline_query_df.iterrows():\n",
    "    for k in k_values:\n",
    "        result = calculate_recall(query['corpusids'], query['retrieved'], k)\n",
    "        recalls['scincl_specific_inline'][k].append(result)\n",
    "\n",
    "for _, query in scincl_specific_manual_query_df.iterrows():\n",
    "    for k in k_values:\n",
    "        result = calculate_recall(query['corpusids'], query['retrieved'], k)\n",
    "        recalls['scincl_specific_manual'][k].append(result)\n",
    "\n",
    "results = {\n",
    "    'scincl_broad_inline': {k: np.mean(recalls['scincl_broad_inline'][k]) for k in k_values},\n",
    "    'scincl_broad_manual': {k: np.mean(recalls['scincl_broad_manual'][k]) for k in k_values},\n",
    "    'scincl_specific_inline': {k: np.mean(recalls['scincl_specific_inline'][k]) for k in k_values},\n",
    "    'scincl_specific_manual': {k: np.mean(recalls['scincl_specific_manual'][k]) for k in k_values}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f96e50eb-2e67-4564-a1c6-8f0c6dc78198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SciNCL Model Evaluation Results\n",
      "================================================================================\n",
      "Query Type           k=1             k=5             k=10            k=20           \n",
      "--------------------------------------------------------------------------------\n",
      "Broad Inline         0.0000          0.0000          0.0000          0.0000\n",
      "Broad Manual         0.0000          0.0000          0.0000          0.0000\n",
      "Specific Inline      0.0000          0.0000          0.0000          0.0000\n",
      "Specific Manual      0.0000          0.0000          0.0000          0.0000\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SciNCL Model Evaluation Results\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Query Type':<20} {'k=1':<15} {'k=5':<15} {'k=10':<15} {'k=20':<15}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Broad Inline':<20} {results['scincl_broad_inline'][1]:.4f}{' '*9} {results['scincl_broad_inline'][5]:.4f}{' '*9} {results['scincl_broad_inline'][10]:.4f}{' '*9} {results['scincl_broad_inline'][20]:.4f}\")\n",
    "print(f\"{'Broad Manual':<20} {results['scincl_broad_manual'][1]:.4f}{' '*9} {results['scincl_broad_manual'][5]:.4f}{' '*9} {results['scincl_broad_manual'][10]:.4f}{' '*9} {results['scincl_broad_manual'][20]:.4f}\")\n",
    "print(f\"{'Specific Inline':<20} {results['scincl_specific_inline'][1]:.4f}{' '*9} {results['scincl_specific_inline'][5]:.4f}{' '*9} {results['scincl_specific_inline'][10]:.4f}{' '*9} {results['scincl_specific_inline'][20]:.4f}\")\n",
    "print(f\"{'Specific Manual':<20} {results['scincl_specific_manual'][1]:.4f}{' '*9} {results['scincl_specific_manual'][5]:.4f}{' '*9} {results['scincl_specific_manual'][10]:.4f}{' '*9} {results['scincl_specific_manual'][20]:.4f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827aa95d-8aa5-4b01-838f-9735a409453c",
   "metadata": {},
   "source": [
    "3. SPECTER2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d5f8cf1e-b092-4027-8d68-93a4f9e8bf38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ad3586ca724ddd870aeed05d15b25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4818a9630c4dca95793b84e1c0b528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "specter2 = Specter2(\"Title_Abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6b103c47-07b8-4671-884a-10297cfca405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_title_abstract_for_specter2(item: dict) -> str:\n",
    "    title = get_clean_title(item)\n",
    "    abstract = get_clean_abstract(item)\n",
    "    return f\"{title} + {specter2._tokenizer.sep_token} + {abstract}\"\n",
    "\n",
    "def create_kv_pairs(data: List[dict], key: str) -> dict:\n",
    "    if key == \"title_abstract\":\n",
    "        kv_pairs = {get_clean_title_abstract_for_specter2(record): get_clean_corpusid(record) for record in data}\n",
    "    elif key == \"full_paper\":\n",
    "        kv_pairs = {get_clean_full_paper(record): get_clean_corpusid(record) for record in data}\n",
    "    elif key == \"paragraphs\":\n",
    "        kv_pairs = {}\n",
    "        for record in data:\n",
    "            corpusid = get_clean_corpusid(record)\n",
    "            paragraphs = get_clean_paragraphs(record)\n",
    "            for paragraph_idx, paragraph in enumerate(paragraphs):\n",
    "                kv_pairs[paragraph] = (corpusid, paragraph_idx)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid key\")\n",
    "    return kv_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c82333a7-f78f-48f8-8903-ebd507b2b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_pairs_for_specter2 = create_kv_pairs(corpus_clean_data, \"title_abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e8625844-8fec-4ed9-910c-d79fff04ce06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing document embeddings: 100%|██████████| 1802/1802 [12:14<00:00,  2.45it/s]\n"
     ]
    }
   ],
   "source": [
    "specter2.create_index(kv_pairs_for_specter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "91aa9c51-173b-45ee-9c35-792d7369441f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 597/597 [00:44<00:00, 13.54it/s]\n"
     ]
    }
   ],
   "source": [
    "specter2_query_set = [query for query in query_data]\n",
    "for query in tqdm(specter2_query_set):\n",
    "    query_text = query[\"query\"]\n",
    "    top_k = specter2.query(query_text, 20)\n",
    "    query[\"retrieved\"] = top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "78ed0198-b774-4e42-8bca-752c34f8f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "specter2_query_df = pd.DataFrame(specter2_query_set)\n",
    "specter2_broad_query_df = specter2_query_df[specter2_query_df['specificity'] == 0]\n",
    "specter2_specific_query_df = specter2_query_df[specter2_query_df['specificity'] == 1]\n",
    "\n",
    "specter2_broad_inline_query_df = specter2_broad_query_df[specter2_broad_query_df['query_set'].str.contains('inline')]\n",
    "specter2_broad_manual_query_df = specter2_broad_query_df[specter2_broad_query_df['query_set'].str.contains('manual')]\n",
    "\n",
    "specter2_specific_inline_query_df = specter2_specific_query_df[specter2_specific_query_df['query_set'].str.contains('inline')]\n",
    "specter2_specific_manual_query_df = specter2_specific_query_df[specter2_specific_query_df['query_set'].str.contains('manual')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5e963ab5-9f39-4916-8e22-33fe9ae1440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [1, 5, 10, 20]\n",
    "recalls = {\n",
    "    'specter2_broad_inline': {k: [] for k in k_values},\n",
    "    'specter2_broad_manual': {k: [] for k in k_values},\n",
    "    'specter2_specific_inline': {k: [] for k in k_values},\n",
    "    'specter2_specific_manual': {k: [] for k in k_values}\n",
    "}\n",
    "\n",
    "for _, query in specter2_broad_inline_query_df.iterrows():\n",
    "    for k in k_values:\n",
    "        result = calculate_recall(query['corpusids'], query['retrieved'], k)\n",
    "        recalls['specter2_broad_inline'][k].append(result)\n",
    "\n",
    "for _, query in specter2_broad_manual_query_df.iterrows():\n",
    "    for k in k_values:\n",
    "        result = calculate_recall(query['corpusids'], query['retrieved'], k)\n",
    "        recalls['specter2_broad_manual'][k].append(result)\n",
    "\n",
    "for _, query in specter2_specific_inline_query_df.iterrows():\n",
    "    for k in k_values:\n",
    "        result = calculate_recall(query['corpusids'], query['retrieved'], k)\n",
    "        recalls['specter2_specific_inline'][k].append(result)\n",
    "\n",
    "for _, query in specter2_specific_manual_query_df.iterrows():\n",
    "    for k in k_values:\n",
    "        result = calculate_recall(query['corpusids'], query['retrieved'], k)\n",
    "        recalls['specter2_specific_manual'][k].append(result)\n",
    "\n",
    "results = {\n",
    "    'specter2_broad_inline': {k: np.mean(recalls['specter2_broad_inline'][k]) for k in k_values},\n",
    "    'specter2_broad_manual': {k: np.mean(recalls['specter2_broad_manual'][k]) for k in k_values},\n",
    "    'specter2_specific_inline': {k: np.mean(recalls['specter2_specific_inline'][k]) for k in k_values},\n",
    "    'specter2_specific_manual': {k: np.mean(recalls['specter2_specific_manual'][k]) for k in k_values}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7b3b78a9-feb2-45cd-bb96-01d141f7991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results = {}\n",
    "# Broad 카테고리 - Inline과 Manual 합침\n",
    "for k in k_values:\n",
    "    # 원본 데이터 리스트 합치기\n",
    "    combined_broad_data = recalls['specter2_broad_inline'][k] + recalls['specter2_broad_manual'][k]\n",
    "    # 합친 데이터의 평균 계산\n",
    "    combined_results[f'specter2_broad_combined_{k}'] = np.mean(combined_broad_data) if combined_broad_data else 0.0\n",
    "\n",
    "# Specific 카테고리 - Inline과 Manual 합침\n",
    "for k in k_values:\n",
    "    # 원본 데이터 리스트 합치기\n",
    "    combined_specific_data = recalls['specter2_specific_inline'][k] + recalls['specter2_specific_manual'][k]\n",
    "    # 합친 데이터의 평균 계산\n",
    "    combined_results[f'specter2_specific_combined_{k}'] = np.mean(combined_specific_data) if combined_specific_data else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "227bf9fc-dd97-4034-ace8-c83c988149cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SPECTER2 Model Evaluation Results\n",
      "================================================================================\n",
      "Query Type           k=1             k=5             k=10            k=20           \n",
      "--------------------------------------------------------------------------------\n",
      "Broad Inline         0.1208          0.2267          0.3033          0.3814\n",
      "Broad Manual         0.0857          0.1286          0.1714          0.2000\n",
      "Specific Inline      0.1710          0.3377          0.4069          0.4502\n",
      "Specific Manual      0.1754          0.3460          0.4265          0.5118\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SPECTER2 Model Evaluation Results\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Query Type':<20} {'k=1':<15} {'k=5':<15} {'k=10':<15} {'k=20':<15}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Broad Inline':<20} {results['specter2_broad_inline'][1]:.4f}{' '*9} {results['specter2_broad_inline'][5]:.4f}{' '*9} {results['specter2_broad_inline'][10]:.4f}{' '*9} {results['specter2_broad_inline'][20]:.4f}\")\n",
    "print(f\"{'Broad Manual':<20} {results['specter2_broad_manual'][1]:.4f}{' '*9} {results['specter2_broad_manual'][5]:.4f}{' '*9} {results['specter2_broad_manual'][10]:.4f}{' '*9} {results['specter2_broad_manual'][20]:.4f}\")\n",
    "print(f\"{'Specific Inline':<20} {results['specter2_specific_inline'][1]:.4f}{' '*9} {results['specter2_specific_inline'][5]:.4f}{' '*9} {results['specter2_specific_inline'][10]:.4f}{' '*9} {results['specter2_specific_inline'][20]:.4f}\")\n",
    "print(f\"{'Specific Manual':<20} {results['specter2_specific_manual'][1]:.4f}{' '*9} {results['specter2_specific_manual'][5]:.4f}{' '*9} {results['specter2_specific_manual'][10]:.4f}{' '*9} {results['specter2_specific_manual'][20]:.4f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "09982a70-63fb-4ad6-ad80-fadd0bfbb0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Results (Inline + Manual):\n",
      "--------------------------------------------------------------------------------\n",
      "Query Type           k=1             k=5             k=10            k=20           \n",
      "--------------------------------------------------------------------------------\n",
      "Broad Combined       0.1129          0.2045          0.2735          0.3404\n",
      "Specific Combined    0.1731          0.3416          0.4163          0.4796\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCombined Results (Inline + Manual):\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Query Type':<20} {'k=1':<15} {'k=5':<15} {'k=10':<15} {'k=20':<15}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Broad Combined':<20} {combined_results['specter2_broad_combined_1']:.4f}{' '*9} {combined_results['specter2_broad_combined_5']:.4f}{' '*9} {combined_results['specter2_broad_combined_10']:.4f}{' '*9} {combined_results['specter2_broad_combined_20']:.4f}\")\n",
    "print(f\"{'Specific Combined':<20} {combined_results['specter2_specific_combined_1']:.4f}{' '*9} {combined_results['specter2_specific_combined_5']:.4f}{' '*9} {combined_results['specter2_specific_combined_10']:.4f}{' '*9} {combined_results['specter2_specific_combined_20']:.4f}\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
